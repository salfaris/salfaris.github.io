{
  "hash": "5fb191d71a25c50e2553f60ec0b0f8ae",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Root-hunting algorithm: Newton's method\"\ncategories: [numerical analysis, algorithms]\nauthor: \"Salman Faris\"\ndate: \"05 04 2021\"\ndate-modified: \"19 02 2023\"\nimage: \"newton-method.png\"\njupyter: python3\nformat:\n  html:\n    toc: true\n    html-math-method: katex\n---\n\n\n**Problem:** Given a real-valued function $f(x)$ in one real variable, what are the values $x_0 \\in \\mathbb{R}$ such that $f(x_0) = 0$?\n\nIf the function $f(x)$ is linear, then the problem is trivial. Explicitly, if $f(x) = ax + b$ for some $a, b \\in \\mathbb{R}$, then $x_0 = -b/a$ gives a solution as long as $a \\neq 0$. However, when the function is nonlinear, the problem can get complicated very fast. For example, try solving when the function is $f(x) = \\sin(e^{x}) + \\cos(\\log x)$.\n\n---\n\n### Newton's idea (an overview)\n\nOne way of solving this problem is to _linearize_ the function $f(x)$ around a certain point $x_0$ of our choice so that we can easily solve the resulting linear equation. Say we get $x_1$ as a solution, then we repeat linearizing $f(x)$ around $x_1$; so on and so forth. The initial point $x_0$ is chosen such that it is close to our hoped solution, say, $x^*$. The idea is that if $x_0$ is suitably chosen, then the solutions $x_1, x_2, x_3, \\ldots$ to each linear approximation of $f(x)$ approximates $x^*$ better and better, and in the limit converges to $x^*$. This whole process is known as the **Newton's method**. \n\nA nice picture of the Newton's method can be seen in @fig-newton. Here, Newton's method is applied to the function $f(x) = x^2$ over $n = 10$ iterations, starting at $x_0 = 1$. We see from @fig-newton that the $x_i$ values converges to $x^* = 0$ which is expected since $x^2 = 0$ if and only if $x = 0$.\n\n---\n\n### Newton's idea (the algebra)\n\nLet us make our discussion above more precise. Linearizing $f(x)$ around $x_0$ simply means Taylor expanding $f$ around $x_0$ and neglecting $\\mathcal{O}(h^2)$ terms. Of course... this is assuming that we can actually perform Taylor expansion in the first place. With that disclaimer out of the way, the Taylor expansion of $f$ around $x_0$ yields\n\n$$f(x) = f(x_0) + f'(x_0) (x - x_0) + \\mathcal{O}(h^2) \\approx f(x_0) + f'(x_0) (x - x_0). $$\n\nSo if we neglect $\\mathcal{O}(h^2)$ terms, we get the linear equation\n\n$$f(x) = f(x_0) + f'(x_0) (x - x_0).$$\n\nSo a solution $x_1$ that satisfy $f(x_1) = 0$ is given by\n\n$$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$$\n\nWe then repeat the process by linearizing $f$ around $x_1$. In this case we have\n\n$$f(x) = f(x_1) + f'(x_1) (x - x_1) \\implies x_2 = x_1 - \\frac{f(x_1)}{f'(x_1)}, $$\n\nwith $x_2$ being a solution. Doing this iteratively yields a general formula\n\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)},$$\n\nknown as **Newton's formula**. Here is the Newton's method in one statement.\n\n---\n\n::: {#thm-line}\n\n## Newton's method\n\nLet $x^* \\in \\mathbb{R}$ be a solution to $f(x) = 0$. If $x_n$ is an approximation of $x^*$ and $f'(x_n) \\neq 0$, then the next approximation to $x^*$ is given by\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)},$$\nwith initial condition, a suitably chosen $x_0 \\in \\mathbb{R}$.\n\n:::\n\n---\n\n### Code implementation\n\nAn iterative implementation of the Newton's method in Python is given below:\n\n::: {#50d29bb8 .cell execution_count=1}\n``` {.python .cell-code}\ndef iterative_newton(f, df, x0, n):\n    \"\"\"Solves f(x) = 0 using the Newton's method.\n\n    Args:\n        f: A callable, the function f(x) of interest.\n        df: A callable, the derivative of f(x).\n        x0: Initial good point to start linearizing.\n        n (Optional): Number of recursion steps to make.\n    \"\"\"\n    xs = [x0] # Sequence of xn.\n\n    # Get latest x value in sequence and\n    # apply the Newton recurrence formula.\n    for _ in range(n):\n        last = xs[-1]\n        res = last - f(last)/df(last)\n        xs.append(res)\n\n    return xs\n```\n:::\n\n\nUsing the same parameters as above, we can also implement a one-liner recursive implementation:\n\n::: {#e99a4cb6 .cell execution_count=2}\n``` {.python .cell-code}\ndef recursive_newton(f, df, x0, n):\n    return x0 if n <= 0 else recursive_newton(f, df, x0 - f(x0)/df(x0), n-1)\n```\n:::\n\n\nObserve that both algorithms have $\\mathcal{O}(n)$ space complexity where $n$ is the number of iterations or depth of the recursion. The time complexity for the iterative implementation is also $\\mathcal{O}(n)$, but for the recursive implementation, it is a bit tricky to compute (so we leave it as an exercise!).\n\nNote that there is a small caveat to the Newton's method which we have implicitly highlight in this post, can you spot it?\n\n### Example usage: finding root of $f(x) = x^2$\n\n::: {#5e3700d8 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Library imports\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\nWe first need a helper function to differentiate a function using finite difference approximation.\n\n::: {#9b45754d .cell execution_count=4}\n``` {.python .cell-code}\ndef finite_diff(f):\n    \"\"\" Returns the derivative of f(x) based on the\n    finite difference approximation.\n    \"\"\"\n    h = 10**(-8)\n    def df(x):\n        return (f(x+h)-f(x-h)) / (2*h)\n    return df\n```\n:::\n\n\nWe then define the function $f(x) = x^2$, compute its derivative and apply Newton's method over $n = 10$ iterations, starting at $x_0 = 1$.\n\n::: {#7bcf99de .cell execution_count=5}\n``` {.python .cell-code}\nf = lambda x: x**2\ndf = finite_diff(f)  # Differentiate f(x).\nres = iterative_newton(f, df, 1, 10)\nres = np.array(res)  # To utilize numpy broadcasting later.\n```\n:::\n\n\nFinally, we plot the function.\n\n::: {#cell-fig-newton .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting code\"}\nplt.style.use('bmh')\n\n# Bounds on the x-axis.\nlo = -0.1\nhi = 1.1\nmesh = abs(hi) + abs(lo)\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Points of the function f(x).\nxs = np.arange(start=lo, stop=hi, step=0.01)\nys = f(xs)\n\ndef tangent_line(f, x0, a, b):\n    \"\"\" Generates the tangent line to f(x) at x0 over\n    the interval [a, b]. Helps visualize the Newton's method.\n    \"\"\"\n    df = finite_diff(f)\n    x = np.linspace(a, b, 300)\n    ytan = (x - x0)*df(x0) + f(x0)\n\n    return x, ytan\n\n# Tangent lines to f(x) at the approximations.\nxtan0, ytan0 = tangent_line(f, res[0], 0.35*mesh, hi)\nxtan1, ytan1 = tangent_line(f, res[1], 0.1*mesh, hi)\nxtan2, ytan2 = tangent_line(f, res[2], lo, 0.7*mesh)\n\nax.plot(xs, ys, label=\"$f(x) = x^2$\", linewidth=3)\nax.plot(xtan0, ytan0, label=\"Linearization 1\", alpha=0.8)\nax.plot(xtan1, ytan1, label=\"Linearization 2\", alpha=0.8)\nax.plot(xtan2, ytan2, label=\"Linearization 3\", alpha=0.8)\nax.plot(res, f(res), color='darkmagenta',\n        label=\"Newton's method\\napproximations\",\n        marker='o', linestyle='None', markersize=6)\n\n# Labels for occurring approximations.\nfor i in range(0, 4):\n    ax.plot(res[i], 0, marker='+', color='k')\n    ax.vlines(res[i], ymin=0, ymax=f(res[i]),\n              linestyles='dotted', color='k', alpha=0.3)\n    plt.annotate(f\"$x_{i}$\",\n                 (res[i], 0),\n                 textcoords=\"offset points\",\n                 xytext=(0, -20),\n                 ha='center',\n                 fontsize=16)\n\n# Grid and xy-axis.\nax.grid(True, which='both')\nax.axvline(x = 0, color='k')\nax.axhline(y = 0, color='k')\n\n# Labels and legend.\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"Newton's method applied to $f(x) = x^2$\")\nplt.legend(fontsize=12, loc=9)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Newton's method applied to $f(x) = x^2$, starting at $x_0 = 1$.](index_files/figure-html/fig-newton-output-1.png){#fig-newton width=821 height=527}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}