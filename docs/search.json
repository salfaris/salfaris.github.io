[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "thoughts.",
    "section": "",
    "text": "How to do aws s3 ls s3://bucket/ using boto3 in python?\n\n\n\n\n\n\n\ncloud\n\n\nAWS\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2023\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClash Series: Checking Number is Prime\n\n\n\n\n\n\n\nclash of code\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2022\n\n\nSalman Faris\n\n\n\n\n\n\n  \n\n\n\n\nPreparing Image Dataset for Neural Networks in PyTorch\n\n\n\n\n\n\n\nmachine learning\n\n\ncomputer vision\n\n\npytorch\n\n\ndata preprocessing\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Coding Kata\n\n\n\n\n\n\n\nsoftware engineering\n\n\nclean coding\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n  \n\n\n\n\nPlotly.py main theme in Plotly.js\n\n\n\n\n\n\n\njavascript\n\n\nplotly\n\n\npython\n\n\nweb dev\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n  \n\n\n\n\nUnit Testing with pytest\n\n\n\n\n\n\n\npython\n\n\ntesting\n\n\nsoftware engineering\n\n\n\n\nWhy do we do unit testing and how to do unit testing in python using pytest.\n\n\n\n\n\n\nMay 3, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n  \n\n\n\n\nRelational Databases in Python (part I)\n\n\n\n\n\n\n\ndata science\n\n\npython\n\n\nsql\n\n\nsqlalchemy\n\n\ndatabases\n\n\ndata handling\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n  \n\n\n\n\nMaking pastel-colored boxes using tcolorbox in LaTeX\n\n\n\n\n\n\n\nlatex\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n  \n\n\n\n\nRoot-hunting algorithm: Newton’s method\n\n\n\n\n\n\n\nnumerical analysis\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging again\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\nSalman Faris\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-05-03-unit-testing-with-pytest/index.html",
    "href": "posts/2021-05-03-unit-testing-with-pytest/index.html",
    "title": "Unit Testing with pytest",
    "section": "",
    "text": "It is a common scene that we want to test our functions after we’ve done writing them. In Python, we can usually do this by printing to the console. For example, suppose we are interested in testing the following function:\nwhich returns the sum of the two integers x and y if the output is strictly positive, or else return None. Since we are adding two positive integers, we expect that the add_positive function returns a positive integer as well. So we can try a few pairs, say, add_positive(2, 3) and add_positive(1, 1), and we expect these to return 5 and 2 respectively; both greater than 0. If we try add_positive(-1, -5), then this should return None as it yields -6 instead which is less than 0. If we want to test our function with multiple inputs, we can do something like a for loop. So maybe something like:\nwhich should yield\nand then we eyeball each of the output to see if it as expected.\nThis is of course a correct way to do it as we can evaluate whether the output on the console is as expected; but is it efficient?"
  },
  {
    "objectID": "posts/2021-05-03-unit-testing-with-pytest/index.html#why-unit-testing-is-a-no-brainer",
    "href": "posts/2021-05-03-unit-testing-with-pytest/index.html#why-unit-testing-is-a-no-brainer",
    "title": "Unit Testing with pytest",
    "section": "🧠 Why unit testing is a no-brainer",
    "text": "🧠 Why unit testing is a no-brainer\nIn a typical life cycle of a function, there are three situations where we would want to test our functions:\n\nThe first time it is implemented.\nWhen a test fails, we fix the bug, and then we have to redo testing.\nWhen implementing a new feature or refactoring code.\n\nImagine how many times we would have to manually test this function. If it would take 3 minutes per (manual) test, and if we would have to do it 100 times over the function’s whole life cycle, then we would have effectively spent 3 x 100 = 300 minutes, which is roughly 5 hours of testing! Now if we have 10 functions to test, it would take us 50 hours or roughly 2 days worth of time to test these functions! I don’t know about you, but that sounds like a lot of time to me for only 10 functions. This is why we would want to automate the testing process by writing unit tests, which, together with the planning phase, requires about an hour to write in perpetuity (in theory).\n\nManually testing a function throughout its whole life cycle may take you 5 hours; whereas writing a properly planned unit test may take you only an hour, and this is one-fifth of the former.\n\nThere are a variety of Python libraries to do unit testing, some of which are:\n\npytest\nunittest\nnosetests\ndoctest"
  },
  {
    "objectID": "posts/2021-05-03-unit-testing-with-pytest/index.html#basic-unit-testing-procedure",
    "href": "posts/2021-05-03-unit-testing-with-pytest/index.html#basic-unit-testing-procedure",
    "title": "Unit Testing with pytest",
    "section": "🧪 Basic unit testing procedure",
    "text": "🧪 Basic unit testing procedure\nIn this post, we will look at pytest because it is simply the most popular (hence, a lot of support, say, on StackOverflow) and easiest to use. We start by installing pytest if it’s not already installed:\npip install pytest\n\nStep 1: Creating the test module\nAssume that the add_positive() function lies in a module called add_positive.py. We begin the unit testing process by creating a file called test_add_positive.py in the same directory as add_positive.py. Such a file will contain the unit tests of functions in add_positive.py, and is called a test module. The test_ in front of test_add_positive.py is important as it lets pytest knows that this is a file containing unit tests.\n_Remark: The test module for the module add_positive.py does not have to be named test_add_positive.py. You can name it test_x.py or test_covid.py or whatever as long as it is prepended with test_. But it is good practice to follow the naming convention of test*module_name to trace which module is this test module testing.*\n\n\nStep 2: Importing inside the test module\nThe next step is to (mainly) import two things:\n\nThe pytest module;\nand the module (or function) we want to test.\n\nSo at the top of our test module test_add_positive.py, we would have:\nimport pytest\nfrom add_positive import add_positive\n\n\nStep 3: Begin writing unit tests\nA unit test is basically a Python function, no more and no less. The only thing special about a unit test is that it is prepended by test_ in its name. This is just to tell pytest to use it as part of the testing procedure. Here is an example of a unit test declaration:\ndef test_for_positive_pairs():\n    ...\nInside the body of a unit test are assertions, and this is the actual testing process. We want to test if our add_positive() function returns correctly if positive pairs (x, y) are passed into the function. So we assert the following:\ndef test_for_positive_pairs():\n    assert add_positive(3, 5) == 8\n    assert add_positive(99999, 99999) == 199998\nHow about if we want to add a second unit test which tests when a zero pair i.e. (0, 0) is passed into the function? Well we just write another function right below it. We expect add_positive(0, 0) to return None, so we write exactly that:\ndef test_for_positive_pairs():\n    assert add_positive(3, 5) == 8\n    assert add_positive(99999, 99999) == 199998\n\ndef test_for_zero_pair():\n    assert add_positive(0, 0) is None\nI would like to add two more tests which test the add_positive function when negative and mixed pairs are fed into the function. The whole test module should now look something like this:\nimport pytest\nfrom add_positive import add_positive\n\ndef test_for_positive_pairs():\n    assert add_positive(3, 5) == 8\n    assert add_positive(99999, 99999) == 199998\n\ndef test_for_negative_pairs():\n    assert add_positive(-1, -6) is None\n    assert add_positive(-99, -99) is None\n\ndef test_for_zero_pair():\n    assert add_positive(0, 0) is None\n\ndef test_for_mixed_pairs():\n    assert add_positive(-1, 2) == 1\n    assert add_positive(-9, 100) > 0\n    assert add_positive(-900, 2) is None\nRemark: Note that you can get as creative as you’d like with the values you’re testing. It is good practice to always consider edge cases in the unit test, but this is a topic of a later post.\n\n\nStep 4: Running pytest and reading the test report\nOnce you’re done with writing the test module, testing is as easy as opening your terminal, cd-ing into the directory containing the test module, and executing:\npytest test_add_positive.py\nIf the implementation of add_positive() is correct (with respect to our tests), you should see a test report like this.\n\n\n\nPass test\n\n\nJust ignore everything above “collected 4 items” as it is not too important. The breakdown of this test report is the following:\n\n“Collected 4 items” means that we will be executing 4 unit tests.\nThe 4 green dots indicates that we have pass all 4 of them, and they are sequential (see what happens when 1 fail below).\nThe obvious “4 passed in 0.02s” message is the summary."
  },
  {
    "objectID": "posts/2021-05-03-unit-testing-with-pytest/index.html#why-unit-tests-are-important-in-prod",
    "href": "posts/2021-05-03-unit-testing-with-pytest/index.html#why-unit-tests-are-important-in-prod",
    "title": "Unit Testing with pytest",
    "section": "📌 Why unit tests are important in prod",
    "text": "📌 Why unit tests are important in prod\nThree months after the add_positive.py code has been deployed, your colleague Gilfoyle modified the add_positive() function returning only the sum:\ndef add_positive(x, y):\n    \"\"\"Add two positive integers x and y. If the sum\n     is negative or zero, return None.\"\"\"\n    return x + y\nApparently, he didn’t read the short description of the function; he thought that you’re adding an unnecesary check. However, the function is supposed to work like that – only returning if the sum is positive (think of another module which relies on the correctness of this function).\nIf there were no unit tests written to check this modification, this seemingly trivial edit by Gilfoyle could have been merged into the main branch and could possibly crash the whole system! Thankfully, we implemented continuous integration and upon making a pull request, Gilfoyle was bombarded with the following test report:\n\n\n\nFail test\n\n\nHere is a breakdown of the report:\n\n“Collected 4 items” means the same as before.\nThe first green dot means we passed the first unit test, but the subsequent F’s means we failed the second, third and fourth unit test;\nthis is reflected in the FAILURES section below it, showing which exact lines contribute to these failures.\nThe “3 failed, 1 passed in 0.19s” message is the summary.\n\nSometimes, we just want to run the test up until the first failure. This can be done by adding a -x flag like so:\npytest -x test_add_positive.py\nNow you might be wondering, what to do if we have multiple functions to test in the same module given that this one single add_positive function alone requires 4 unit tests. Things can start getting really messy right? To solve this issue, we can create a class containing these unit tests for each function we want to test. This will be a topic of a future post on testing."
  },
  {
    "objectID": "posts/2021-04-05-newton-method/index.html",
    "href": "posts/2021-04-05-newton-method/index.html",
    "title": "Root-hunting algorithm: Newton’s method",
    "section": "",
    "text": "Problem: Given a real-valued function f(x) in one real variable, what are the values x_0 \\in \\mathbb{R} such that f(x_0) = 0?\nIf the function f(x) is linear, then the problem is trivial. Explicitly, if f(x) = ax + b for some a, b \\in \\mathbb{R}, then x_0 = -b/a gives a solution as long as a \\neq 0. However, when the function is nonlinear, the problem can get complicated very fast. For example, try solving when the function is f(x) = \\sin(e^{x}) + \\cos(\\log x).\n\n\nNewton’s idea (an overview)\nOne way of solving this problem is to linearize the function f(x) around a certain point x_0 of our choice so that we can easily solve the resulting linear equation. Say we get x_1 as a solution, then we repeat linearizing f(x) around x_1; so on and so forth. The initial point x_0 is chosen such that it is close to our hoped solution, say, x^*. The idea is that if x_0 is suitably chosen, then the solutions x_1, x_2, x_3, \\ldots to each linear approximation of f(x) approximates x^* better and better, and in the limit converges to x^*. This whole process is known as the Newton’s method.\nA nice picture of the Newton’s method can be seen in Figure 1. Here, Newton’s method is applied to the function f(x) = x^2 over n = 10 iterations, starting at x_0 = 1. We see from Figure 1 that the x_i values converges to x^* = 0 which is expected since x^2 = 0 if and only if x = 0.\n\n\n\nNewton’s idea (the algebra)\nLet us make our discussion above more precise. Linearizing f(x) around x_0 simply means Taylor expanding f around x_0 and neglecting \\mathcal{O}(h^2) terms. Of course… this is assuming that we can actually perform Taylor expansion in the first place. With that disclaimer out of the way, the Taylor expansion of f around x_0 yields\nf(x) = f(x_0) + f'(x_0) (x - x_0) + \\mathcal{O}(h^2) \\approx f(x_0) + f'(x_0) (x - x_0). \nSo if we neglect \\mathcal{O}(h^2) terms, we get the linear equation\nf(x) = f(x_0) + f'(x_0) (x - x_0).\nSo a solution x_1 that satisfy f(x_1) = 0 is given by\nx_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}\nWe then repeat the process by linearizing f around x_1. In this case we have\nf(x) = f(x_1) + f'(x_1) (x - x_1) \\implies x_2 = x_1 - \\frac{f(x_1)}{f'(x_1)}, \nwith x_2 being a solution. Doing this iteratively yields a general formula\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)},\nknown as Newton’s formula. Here is the Newton’s method in one statement.\n\n\nTheorem 1 (Newton’s method) Let x^* \\in \\mathbb{R} be a solution to f(x) = 0. If x_n is an approximation of x^* and f'(x_n) \\neq 0, then the next approximation to x^* is given by x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}, with initial condition, a suitably chosen x_0 \\in \\mathbb{R}.\n\n\n\n\nCode implementation\nAn iterative implementation of the Newton’s method in Python is given below:\n\ndef iterative_newton(f, df, x0, n):\n    \"\"\"Solves f(x) = 0 using the Newton's method.\n\n    Args:\n        f: A callable, the function f(x) of interest.\n        df: A callable, the derivative of f(x).\n        x0: Initial good point to start linearizing.\n        n (Optional): Number of recursion steps to make.\n    \"\"\"\n    xs = [x0] # Sequence of xn.\n\n    # Get latest x value in sequence and\n    # apply the Newton recurrence formula.\n    for _ in range(n):\n        last = xs[-1]\n        res = last - f(last)/df(last)\n        xs.append(res)\n\n    return xs\n\nUsing the same parameters as above, we can also implement a one-liner recursive implementation:\n\ndef recursive_newton(f, df, x0, n):\n    return x0 if n <= 0 else recursive_newton(f, df, x0 - f(x0)/df(x0), n-1)\n\nObserve that both algorithms have \\mathcal{O}(n) space complexity where n is the number of iterations or depth of the recursion. The time complexity for the iterative implementation is also \\mathcal{O}(n), but for the recursive implementation, it is a bit tricky to compute (so we leave it as an exercise!).\nNote that there is a small caveat to the Newton’s method which we have implicitly highlight in this post, can you spot it?\n\n\nExample usage: finding root of f(x) = x^2\n\n\nLibrary imports\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nWe first need a helper function to differentiate a function using finite difference approximation.\n\ndef finite_diff(f):\n    \"\"\" Returns the derivative of f(x) based on the\n    finite difference approximation.\n    \"\"\"\n    h = 10**(-8)\n    def df(x):\n        return (f(x+h)-f(x-h)) / (2*h)\n    return df\n\nWe then define the function f(x) = x^2, compute its derivative and apply Newton’s method over n = 10 iterations, starting at x_0 = 1.\n\nf = lambda x: x**2\ndf = finite_diff(f)  # Differentiate f(x).\nres = iterative_newton(f, df, 1, 10)\nres = np.array(res)  # To utilize numpy broadcasting later.\n\nFinally, we plot the function.\n\n\nPlotting code\nplt.style.use('bmh')\n\n# Bounds on the x-axis.\nlo = -0.1\nhi = 1.1\nmesh = abs(hi) + abs(lo)\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Points of the function f(x).\nxs = np.arange(start=lo, stop=hi, step=0.01)\nys = f(xs)\n\ndef tangent_line(f, x0, a, b):\n    \"\"\" Generates the tangent line to f(x) at x0 over\n    the interval [a, b]. Helps visualize the Newton's method.\n    \"\"\"\n    df = finite_diff(f)\n    x = np.linspace(a, b, 300)\n    ytan = (x - x0)*df(x0) + f(x0)\n\n    return x, ytan\n\n# Tangent lines to f(x) at the approximations.\nxtan0, ytan0 = tangent_line(f, res[0], 0.35*mesh, hi)\nxtan1, ytan1 = tangent_line(f, res[1], 0.1*mesh, hi)\nxtan2, ytan2 = tangent_line(f, res[2], lo, 0.7*mesh)\n\nax.plot(xs, ys, label=\"$f(x) = x^2$\", linewidth=3)\nax.plot(xtan0, ytan0, label=\"Linearization 1\", alpha=0.8)\nax.plot(xtan1, ytan1, label=\"Linearization 2\", alpha=0.8)\nax.plot(xtan2, ytan2, label=\"Linearization 3\", alpha=0.8)\nax.plot(res, f(res), color='darkmagenta',\n        label=\"Newton's method\\napproximations\",\n        marker='o', linestyle='None', markersize=6)\n\n# Labels for occurring approximations.\nfor i in range(0, 4):\n    ax.plot(res[i], 0, marker='+', color='k')\n    ax.vlines(res[i], ymin=0, ymax=f(res[i]),\n              linestyles='dotted', color='k', alpha=0.3)\n    plt.annotate(f\"$x_{i}$\",\n                 (res[i], 0),\n                 textcoords=\"offset points\",\n                 xytext=(0, -20),\n                 ha='center',\n                 fontsize=16)\n\n# Grid and xy-axis.\nax.grid(True, which='both')\nax.axvline(x = 0, color='k')\nax.axhline(y = 0, color='k')\n\n# Labels and legend.\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"Newton's method applied to $f(x) = x^2$\")\nplt.legend(fontsize=12, loc=9)\n\nplt.show()\n\n\n\n\n\nFigure 1: Newton’s method applied to f(x) = x^2, starting at x_0 = 1."
  },
  {
    "objectID": "posts/2021-08-18-coding-kata/index.html",
    "href": "posts/2021-08-18-coding-kata/index.html",
    "title": "The Coding Kata",
    "section": "",
    "text": "A coding kata is an exercise to sharpen your programming skills and muscle memory via repetition. I learnt this concept when I was reading The Clean Coder (highly recommended!) and have since adopted it as part of my programming routine. It helps my hands “know” what to type when I need to type.\nI have also adapted the concept of kata as a means to learn/revise programming languages effectively. Instead of writing a random script doing god knows what after learning the syntax, I would implement the simple linear regression algorithm from scratch in this language. Why this is a good kata you ask? Here is why among others:\n\nYou can practice using OOP in the new lang\nYou will deal with ints, doubles, static and dynamic arrays.\nYou will do some basic math operations.\nYou will implement at least one function.\nYou will implement at least a for loop.\nYou might use an import.\n\nTo put simply, it ensures you use a lot of the functionalities in the language and actually spend time doing it.\nI recently wanted to refresh my mind on the C++ lang and I also want to reinforce my JavaScript knowledge. So here’s my linear regression implementation in these two languages (although the way data is expected is a bit different in the JS implementation compared to the C++ implementation).\n\nC++JavaScript\n\n\n#include <vector>\nusing namespace std;\n\nclass LinearRegression {\n    private:\n        int m_epoch;\n        double m_learningRate;\n        vector<double> weights;\n\n    public:\n        LinearRegression(int epoch, double learningRate) {\n            this->m_epoch = epoch;\n            this->m_learningRate = learningRate;\n        }\n\n        void fit(vector<double> x, vector<double> y) {\n            vector<double> weights = {0, 0};\n            int dataLength = x.size();\n\n            int epoch = this->m_epoch;\n            for (int e = 0; e < epoch; e++) {\n                for (int i = 0; i < dataLength; i++) {\n                    double estimate = weights[0] + x[i]*weights[1];\n                    double error = y[i] - estimate;\n\n                    weights[0] += this->m_learningRate * error;\n                    weights[1] += this->m_learningRate * error * x[i];\n                }\n            }\n\n            this->weights = weights;\n        }\n\n        vector<double> predict(vector<double> x) {\n            int dataLength = x.size();\n            vector<double> yPred;\n            yPred.reserve(dataLength);  // Preallocate length of yPred based on size of x.\n\n            vector<double> weights = this->weights;\n            for (int i = 0; i < dataLength; i++) {\n                yPred.push_back(weights[0] + x[i]*weights[1]);\n            }\n\n            return yPred;\n        }\n};\n\n\nclass LinearRegression {\n  constructor(params = {}) {\n    this.weights = params.weights || [];\n    this.learningRate = params.learningRate || 0.01;\n    this.data = [];\n    this.fittedValues = [];\n  }\n\n  estimator(x, weights) {\n    const [w0, w1] = weights;\n    return w0 + x * w1;\n  }\n\n  fit(data) {\n    this.data = data;\n    if (this.weights === undefined || this.weights.length === 0) {\n      this.weights = [0, 0];\n    }\n\n    for (let i = 0; i < this.data.length; i++) {\n      const { x, y } = this.data[i];\n\n      const estimate = this.estimator(x, this.weights);\n      const error = y - estimate;\n\n      let [w0, w1] = this.weights;\n\n      w0 += this.learningRate * error;\n      w1 += this.learningRate * error * x;\n\n      this.weights = [w0, w1];\n    }\n\n    this.fittedValues = this.getFittedValues();\n  }\n\n  getFittedValues() {\n    return this.data.map(({ x, y }) => {\n      return { x: x, y: this.estimator(x, this.weights) };\n    });\n  }\n}\n\n\n\nI leave the Python implementation to you ;)"
  },
  {
    "objectID": "posts/2021-06-13-plotly-theme-in-react-plotly-js/index.html",
    "href": "posts/2021-06-13-plotly-theme-in-react-plotly-js/index.html",
    "title": "Plotly.py main theme in Plotly.js",
    "section": "",
    "text": "If you’re like me who is used to using Plotly.py (i.e. Plotly Python) and then suddenly decided to use Plotly.js directly, you might immediately realize that there are some significant differences in terms of the plot design.\n\n\n\nPlotly Python Plot\n\n\nFor one, your default plot in Plotly.js has a white background and not the steel blue colour as you would expect from a default Plotly.py plot. The reason is that the plots produced in the python version of Plotly come prepackaged with nice out-of-the-box themes. For example, the plot image above is a Plotly.py scatter plot with the default plotly template. So how do we reproduce these themes inside of Plotly.js?\nDiving deeper into the Plotly documentation, I discovered that you can actually get the “theme settings” for the Plotly.py templates with just 4 lines of Python code.\nimport plotly.io as pio\ntemplate = \"plotly\"\nplotly_template = pio.templates[template]\nprint(plotly_template.layout)\nThe Python code above prints the “theme settings” of the plot or in proper Plotly lingo, the plot layout settings. In our particular case above where we set template = \"plotly\", we get the default plotly theme layout settings:\nLayout({\n    'annotationdefaults': {'arrowcolor': '#2a3f5f', 'arrowhead': 0, 'arrowwidth': 1},\n    'autotypenumbers': 'strict',\n    'coloraxis': {'colorbar': {'outlinewidth': 0, 'ticks': ''}},\n    ...\n})\nAs you can see, it is in serialized JSON format. The idea now then is to deserialize this layout settings into a JavaScript object literal and pass it into the layout of your plot in Plotly.js. I did the deserialize process manually because I cannot find a way to automate this process (I would love to hear if you are able to do it!). For the default plotly theme, the JavaScript object literal is given below:\n\n\nNote: Stating the obvious but since the layout setting is extracted from the Plotly documentation, it is the hard work of the people behind Plotly. Thank you for the awesome theme!\n\nThe final step is to pass this layout into the layout parameter of the Plotly plot, and you get your favorite default Plotly.py theme again but now directly in Plotly.js!"
  },
  {
    "objectID": "posts/2021-04-05-first-post/index.html",
    "href": "posts/2021-04-05-first-post/index.html",
    "title": "Blogging again",
    "section": "",
    "text": "I’ve been an active blogger since 2011 up until 2016 (although most of the earlier contents are now hidden due to cringiness) where I talked about various things – life events, some tech stuff and my scholarship interview experiences. I stopped blogging when I started my International Baccalaureate (IB) diploma program. Throughout my undergraduate years, I channelled most of my writing energy answering IB questions on Quora and writing my notes using LaTeX. Today, I’ve decided to start blogging again, focusing on math, statistics and ML.\nDo note that in the early stages of this blog, the contents might be inconsistent or quite random. This is because I am still finding the right pace and momentum in writing and generating content. Thanks for understanding!"
  },
  {
    "objectID": "posts/2021-04-07-tcolorbox/index.html",
    "href": "posts/2021-04-07-tcolorbox/index.html",
    "title": "Making pastel-colored boxes using tcolorbox in LaTeX",
    "section": "",
    "text": "One of the many questions I get when people see my \\LaTeX-ed math notes is how do you create these nice pastel-colored boxes?\n \n\nTwo examples of pastel-colored boxes from my math notes\n\n\nI feel like this is a very relevant and important question, because I asked the exact same thing when I was new to \\LaTeX.\n\nSimple box using fbox and minipage\nMy first exposure to \\LaTeX box-like environments was my mentor Ghaleo’s genius tutorial notes. He used them for two main reasons:\n\nto highlight important theorems, lemmas, etc;\nan empty space for students to follow along and fill in the blanks live in class.\n\nI tried to reproduce his box environment when I was writing my very first math notes on \\LaTeX – my Linear Algebra & Geometry II notes.\n\n\nA snippet of my Linear Algebra & Geometry II notes\n\nIt was achievable using the fbox environment alone, but for it to generalize well, I have to throw in a minipage usage as well. The code implementation is quite simple and requires no package imports. Just use the following code directly in your document:\n% Simple box environment.\n\\fbox{ %\n\\begin{minipage}[t]{0.9\\textwidth}\n    % Your text here.\n\\end{minipage}\n}\nTo make the box look cleaner, use the center environment as well. Here is a concrete example of using this box. The code:\n\\begin{document}\n\n\\begin{center}\n\\fbox{ %\n\\begin{minipage}[t]{0.9\\textwidth}\n    This is a plain small box.\n\\end{minipage}\n}\n\\end{center}\n\n\\end{document}\nwould yield the box:\n\nNow the problem with this simple fbox is that… it’s just plain black and white. I want something more vibrant and colourful, so I decided to start learning the tcolorbox package.\n\n\n\nTransition to tcolorbox\nAs usual, the best way to learn is to straight dive in and do. After (very) little reading on the package’s documentation, I started to recreate pretty boxes that other people have made from scratch. These boxes are usually from notes I found online like:\n\nEvan Chen’s The Napkin project;\nTony Zhang’s notes snippets from his Quora post;\nor even the tcolorbox documentation itself.\n\nWith my then minimal knowledge, I started writing my Real Analysis notes. I was quite happy with the end product but the overall design felt a bit odd (as you can see in the figure below). Nevertheless, my initial goal was to just make prettier boxes and I think I’ve achieved just that.\n\n\nA snippet of my Real Analysis notes\n\nTo reproduce the (proof) box above with a Cerulean colour, use the following code in your document’s preamble:\n% In preamble.\n\\usepackage[dvipsnames]{xcolor}\n\\usepackage[many]{tcolorbox}\n\n\\newtcolorbox{myAwesomeBox}{\n    enhanced,\n    sharp corners,\n    breakable,  % Allows page break.\n    borderline west={2pt}{0pt}{Cerulean},\n    colback=Cerulean!10,  % Background color.\n    colframe=Cerulean!10  % Frame (border) color.\n}\nTo see this box in action, we instantiate this environment in our document:\n\\begin{document}\n\n\\begin{myAwesomeBox}\n    This nice blue pastel box!\n\\end{myAwesomeBox}\n\n\\end{document}\nThis code should then yield:\n\nI want to note two things regarding the imports:\n\nThe dvipsnames option for xcolor is simply a choice of the set of colors I want from xcolor. It does affect the name of colors you can pass as arguments, for example, here I used Cerulean. For a full set of possible colors in xcolor, see here.\nThe many option for tcolorbox loads additional libraries which allows us to use more features from tcolorbox. In particular for our use case, we want to be able to use enhanced. See the documentation for other options.\n\nHere are some tweaks that you might want to consider:\n\nIf you want to add a nice smooth border-like effect, do this: instead of using colframe=Cerulean!10, remove that line and put boxrule=0pt instead. In general, boxrule adjusts the border stroke.\nIf you want to remove padding inside the box, add boxsep=0pt. In general, boxsep controls the inside padding.\n\nThere are many more parameters that you can control and these can be found in the tcolorbox documentation.\nNowadays, I write my notes using the NotesTex package which coincidentally uses tcolorbox with a similar setting as mine. I guess this is one of the reasons why it was a no brainer for me to use it in the first place. The nice thing about the package is that it provides a complete framework to write notes in \\LaTeX. In particular, you don’t have to reinvent the wheel for everything (e.g. theorem environment, sidenotes). And for features that you feel are missing, you can simply add it on your own with ease. I’ve added plenty of features already and they integrate quite well. I’d highly recommend checking NotesTex out!"
  },
  {
    "objectID": "posts/2021-09-07-preparing-image-dataset-in-pytorch/index.html",
    "href": "posts/2021-09-07-preparing-image-dataset-in-pytorch/index.html",
    "title": "Preparing Image Dataset for Neural Networks in PyTorch",
    "section": "",
    "text": "Preparing and handling data is a core step of any machine learning pipeline. Today, we will look at handling data when the data is an image (or image-like) in PyTorch.\n\nPyTorch and Torchvision\nPyTorch provides us with the amazing torchvision package for dealing with common image transformations such as normalizing, scaling, random flipping and converting arrays to PyTorch tensors. It also provides us with common computer vision datasets such as MNIST, Fashion MNIST and CIFAR-10. In this post, we will focus on preparing the Fashion MNIST dataset.\nTo begin, we start by importing torch and torchvision.\nimport torch\nfrom torchvision import datasets, transforms\n\nNote that we will refer to the submodule datasets and transforms directly from now on (i.e. we will not emphasize that it’s part of torchvision).\n\n\n\n👜 Fashion MNIST dataset and composing transformations\nThe Fashion MNIST dataset by Zalando Research is a famous benchmark dataset in computer vision, perhaps second only to MNIST. It is a dataset containing 60,000 training examples and 10,000 test examples where each example is a 28 x 28 grayscale image. Since the images are in grayscale, they only have a single channel. If the image is in RGB format instead (e.g. if we are dealing with CIFAR-10), then it has 3 channels one for each red, green and blue.\nAs mentioned before, the Fashion MNIST dataset is already part of PyTorch. However, this does not mean that the dataset is already in perfect shape to pass into a PyTorch neural net. We would need to apply some (image) transformations to the dataset upon fetching. For brevity, we will apply only two simple transformations:\n\nConverting the images to a PyTorch tensor – by using transforms.ToTensor().\nNormalize the channel of the resulting tensor – by using transforms.Normalize().\n\nWhy do we do these transformations?\n\nSince we will be working with neural nets in PyTorch, it is only natural that we want the image to be a PyTorch tensor. This enables the PyTorch API to interact properly with our dataset.\nNormalization is important to ensure that our neural nets learn better. For an idea of how normalization works, check out this discussion.\n\nWe can then compose these transformations using transforms.Compose() as below.\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5), std=(0.5)),\n])\n\nNote that the mean and standard deviation value of 0.5 should be calculated from the training set in advance. Here, we just assume that mean = std = 0.5 for simplicity.\n\n\n\n💾 From dataset to DataLoader\nThe next step is to finally fetch the dataset, passing our transform above as an argument. The FashionMNIST dataset can be accessed via datasets.FashionMNIST, no surprise there. We can then fetch the 60,000 training examples using the following code:\ntrainset = datasets.FashionMNIST(root='./data',\n                                 download=True,\n                                 train=True,\n                                 transform=transform)\nLet us break down what each argument means.\n\nroot specifies the location of the dataset. Here, we specify that it should be in the directory './data'.\ndownload is a boolean flag which determines if we want to download the dataset if the data is not already in root.\ntrain is another boolean flag which determines if we want the training set. Getting the test set is as simple as passing train=False.\ntransform is the transformations we would like to apply to the dataset upon fetching.\n\nOnce we have our transformed train set, we can now start training neural nets on this data using PyTorch. However, let us take a second to think about the following:\n\nWhat if we want to work with minibatches of this dataset instead of single examples? This is definitely a need when the dataset is too large like ours to be trained entirely.\nWe would also want to reshuffle this dataset on each epoch so that our neural net generalizes better.\nIf the data is big, we might even want to load the data in parallel using multiprocessing workers to retrieve our data faster.\n\nThis is where PyTorch’s so-called DataLoader comes in. It is an iterable that provides all the above features out of the box on top of providing a smooth API for working with data!\nTo use the DataLoader object on our train set, we simply access torch.utils.data.DataLoader and feed trainset into it.\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n                                          shuffle=True, num_workers=0)\nHere, we have decided to use a batch_size of 64 images, which are sampled randomly on each epoch due to shuffle=True. We also put num_workers=0 meaning we are not loading the data in parallel.\nWe can fetch the Fashion MNIST test dataset in a similar fashion. The only difference is that we now have train=False.\ntestset = datasets.FashionMNIST(root='./data',\n                                download=True,\n                                train=False,\n                                transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64,\n                                         shuffle=True, num_workers=0)\n\n\n🕵️ Inspecting the dataset in DataLoader form\nOnce we have the dataset in DataLoader form, we can start inspecting our dataset. For example, we can get the shapes of our trainset.\nprint(\"Train shape:\", trainloader.dataset.data.shape)\nprint(\"Test shape:\", testloader.dataset.data.shape)\nTrain shape: torch.Size([60000, 28, 28])\nTest shape: torch.Size([10000, 28, 28])\nWe can also get the minibatch size as specified when initializing the DataLoader.\nprint(\"Train batch size:\", trainloader.batch_size)\nprint(\"Test batch size:\", testloader.batch_size)\nTrain batch size: 64\nTest batch size: 64\nFor a more advanced inspection, we can even look at the sampler and the collate function used in the DataLoader. The sampler determines how the data is shuffled and the collate function specifies how the data is batched.\nprint(\"Sampler:\", trainloader.sampler)\nprint(\"Collate function:\", trainloader.collate_fn)\nSampler: <torch.utils.data.sampler.RandomSampler object at 0x7fcc02b23b90>\nCollate function: <function default_collate at 0x7fcc05c9a710>\nSince we did not pass anything during initialization, we get the default RandomSampler object for the sampler and the default default_collate collate function as expected.\nAs we are dealing with an image dataset, it is a shame if we are not plotting anything during inspection. Let’s plot the first image from the first batch in trainloader.\nimages, labels = next(iter(trainloader))  # Gets a batch of 64 images in the training set\nfirst_image = images[0]  # Get the first image out of the 64 images.\n\nimport matplotlib.pyplot as plt\nplt.imshow(first_image.numpy().squeeze(), cmap='Greys_r')\nplt.show()\nHere, we get a t-shirt which is expected since we are dealing with a fashion dataset after all. If you run the exact code, you might get a different output since the dataset is shuffled and I did not specify a seed.\n\n\n\nPlot of image from Fashion MNIST\n\n\nFor the simplified version of this post in jupyter notebook format: notebook version."
  },
  {
    "objectID": "posts/2021-04-30-relational-databases-in-python-part-I/index.html",
    "href": "posts/2021-04-30-relational-databases-in-python-part-I/index.html",
    "title": "Relational Databases in Python (part I)",
    "section": "",
    "text": "Post header\nWe usually hear the word databases being thrown around especially when talking about data-related things. So what is it, and what is the more precise term relational databases?\nA relational database is like an Excel file. It is made up of tables (note that this is plural) which holds data in the form of columns and rows. In the Excel analogy, tables are basically sheets. Moreover, tables can be related to each other but they need a column to act as a bridge linking them. Such a column is usually called a key (either primary key or foreign key). This feature of being related explains the relational term in relational databases. Relational databases is part of the relational model which is a much more general framework of structuring and handling data management.\nFrom now on, we shall simply refer to relational databases as just databases."
  },
  {
    "objectID": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#introduction-to-sqlalchemy",
    "href": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#introduction-to-sqlalchemy",
    "title": "Relational Databases in Python (part I)",
    "section": "Introduction to SQLAlchemy",
    "text": "Introduction to SQLAlchemy\npip install sqlalchemy\nGreat, we now know what databases are, but how do we work with them? If this is an Excel file, we just open it and the rest is obvious (at least we think it’s obvious because we’re used to working with it). How do we open and interact with a database? There are many ways to do this and there’s no one right way. For example, you can work directly with SQLite or MySQL in the command line but things can really get messy if you do it this way.\nIf you’re using Python, then enter SQLAlchemy! SQLAlchemy gives you the power of interacting with databases using SQL queries straight from Python. Plus, it helps us abstract away complex queries and the difference in databases (remember that there are a lot of popular databases e.g. MySQL, PostgreSQL and Oracle with subtle differences among them). So querying databases becomes cleaner (and more exciting?) via SQLAlchemy. To install SQLAlchemy is as easy as executing the line of code you see below this section’s title in your favorite terminal. If you’re using Anaconda, then it is already shipped and ready to use!\nAdditional notes:\n\nThis is not an excuse to not learn writing raw SQL queries because understanding SQL is still important when working with tools like SQLAlchemy!\nThose of you who have built web applications with Flask or Django might realize that we usually handle data using an object-oriented approach – using so-called data models. This is the Object Relational Model (ORM) approach, which is one of the two main components of SQLAlchemy. The other main component is called the “core part” of SQLAlchemy which is really centred around the relational model of the database. The latter is the one that we will be focusing on in this post today."
  },
  {
    "objectID": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#talking-to-a-database-first-steps",
    "href": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#talking-to-a-database-first-steps",
    "title": "Relational Databases in Python (part I)",
    "section": "💬 Talking to a database, first steps",
    "text": "💬 Talking to a database, first steps\n\nStep 1: Create an engine\nTo write something on a paper, you would need a pencil. To turn the lights on, you would need to switch the toggle. To interact with a database, you would need a so-called engine.\nIn theory, a database relies on this engine just like how a car would rely on its (car) engine. But I found that thinking of this engine as a mediator rather than a literal engine is much easier to digest.\nTo create an engine, you first have to import the create_engine function from sqlalchemy. Then, you need to pass in a connection string which in its simplest form specifies two things: (i) the database you want to talk to, and (ii) the path to the database. For example, if I want to connect to race_data.db which is a SQLite database in my current directory, the connection string would be \"sqlite:///race_data.db\". So together, they would look something like this:\nengine = create_engine(\"sqlite:///race_data.db\")\n\nKey terms\n\nThe engine is a mediator for SQLAlchemy to interact with a database.\nA connection string specifies the details of the database we want to interact with.\n\n\n\n\nStep 2: Establish a connection\nIn the previous step, we have simply created an engine but have yet to connect to it! To establish a connection, you can simply invoke another one-liner:\nconnection = engine.connect()\nOne thing worth pointing out is that SQLAlchemy is clever enough to not actually make a connection until we pass in some queries for it to execute.\n\n\nStep 3: Checking table names\nRecall that tables are to databases just like sheets are to Excel files. So you’d want to know what tables (not columns of a table, yet!) are available before making queries. To do this, you can simply execute\nprint(engine.table_names())\nto get a list of available tables to work with. For me, this returns\n>>> ['race_table']\nwhich means that I have only one table named 'race_table'. In practice, you would usually have a few tables."
  },
  {
    "objectID": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#querying-the-database-using-sqlalchemy",
    "href": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#querying-the-database-using-sqlalchemy",
    "title": "Relational Databases in Python (part I)",
    "section": "📝 Querying the database using SQLAlchemy",
    "text": "📝 Querying the database using SQLAlchemy\nFrom now on, we assume that we have instantiated an engine and connection object exactly like what we did previously.\n\nRaw SQL queries, ResultProxy and ResultSet\nRecall that we use the SQL language to make CRUD operations – create, read, update and delete. If you are not familiar with this, I highly recommend that you learn a bit of SQL after reading this - my recommendation is Mike Dane’s{:target=“_blank”} free and complete SQL course, from installing MySQL to joining tables. However, for now, it is sufficient to know the “Hello World” of SQL – which is SELECT * FROM this_table where this_table is some table in the database. The query SELECT * FROM this_table does exactly what you expect it to do, it selects every possible row (symbolized by the asterisk *) in the table this_table and returns it to the user. In my case, with my race_data.db database, I would want to execute SELECT * FROM race_table. So how would I do this with SQLAlchemy? This is where connection from Step 2 comes in.\nThe connection object has a method .execute() which can execute raw SQL queries like SELECT * FROM race_table. This will then return a ResultProxy object which can be utilized in various ways to get the data we want (based on our query). Here are some examples of how we would want our data:\n\nSometimes, we know our query will return a unique result (e.g. because we query based on a unique ID), so it is trivial that we want the first and only result;\nSometimes, we want the whole result;\nSometimes, we want only the top 10.\n\nImagine processing 500k rows of data just because we want the top 10, not so efficient right? This is why we would want a two-layer process before getting our actual data, the first layer being the ResultProxy object. Getting the actual data from the ResultProxy object is simply a matter of invoking a method. For example, if we would want to get the whole result, we use the .fetchall() method. If we want the top 10 result, we use the .fetchmany() method with size argument set to 10. Invoking these methods returns a ResultSet object, which basically contains the data we want. Here is an example of a complete implementation:\nq = \"SELECT * FROM race_table\"\nresult_proxy = connection.execute(q)  # ResultProxy\nresults = result_proxy.fetchmany(size=10)  # ResultSet\n\nQuerying data in Python is a two-layer process:\n\nA ResultProxy gives us flexibility to access the data that we queried – do you want 1, 10 or 500k?.\nA ResultSet contains the actual data that we queried, retrieved via a ResultProxy method.\n\n\n\n\nWorking with ResultSet\nThe ResultSet results is a list of tuples whose entries corresponds to columns in the table. Let’s get the first row in results. Since it is a list, we do this by accessing the zeroth element in the list via row = results[0]. We can print the row to get the actual data, a tuple with entries:\n>>> (1, 88, 0.95, 1, 5, 436, '2013-11-03 13:19:25')\nTo get the column names that correspond to each entry, we can invoke row.keys() to get:\n>>> ['Race #', 'WPM', 'Accuracy', 'Rank', '# Racers', 'Text ID', 'Date/Time (UTC)']\nIf we already know which column of interest we want to look at for a particular row, we can access the attribute of the row just like how we would normally do to access the value of a key in a dictionary. For example, if I’m interested in knowing the Accuracy of this row, I would just execute either row.Accuracy or row[\"Accuracy\"] which returns 1 as expected.\n\n\nSQLAlchemy queries\nI know I promised that SQLAlchemy can help abstract away complex SQL queries and database differences, and using raw SQL queries like we did so far doesn’t seem to agree with this promise. Enter SQLAlchemy’s neat and Pythonic way of querying using table reflection.\nA table reflection or just reflection is basically a process which reads the desired database, looks for your desired table and copies it into a Table object (see below) as if you wrote a whole raw SQL query to create this table. Personally, the term reflection is quite misleading for me and I would more prefer the term autocopy – because that is literally what the process does, and my philosophy is that explicit is better than nano-misleadings.\nTo make a reflection, you would need to import two classes from the sqlalchemy library: MetaData and Table. It is worth understanding a basic idea of what these things do:\n\nThe MetaData class can be thought of a folder or catalog that keeps information about database stuffs such as tables. In this way, we don’t have to keep looking up table information because we have “organized” things nicely.\nThe Table class does exactly what you expect it to do. It stores and handles the reflected (i.e. autocopied) version of your desired table so that SQLAlchemy can interact with it.\n\nNow we know the required ingredients, let’s see how to actually do a table reflection. Recall that we have instantiated an engine and connection object from the previous section. We now instantiate a MetaData object via metadata = MetaData(); and then instantiate a Table object passing the arguments:\n\nOur desired table; in my case, \"race_table\" (this is a string, cf. Step 3 of checking table names).\nThe instantiated metadata.\nSet autoload to True, and put autoload_with our engine.\n\nOverall, it should look like this:\nfrom sqlalchemy import MetaData, Table\nmetadata = MetaData()\n\n# Table reflection\nrace_table = Table(\"race_table\", metadata, autoload=True, autoload_with=engine)\nThe first thing you might want to do then is to use the repr function on race_table to learn our table’s details such as the column names together with their types such as REAL, INTEGER or TEXT. Mine returns this:\n>>> Table('race_table', MetaData(bind=None), Column('Race #', INTEGER(), table=<race_table>), Column('WPM', INTEGER(), table=<race_table>), Column('Accuracy', REAL(), table=<race_table>), Column('Rank', INTEGER(), table=<race_table>), Column('# Racers', INTEGER(), table=<race_table>), Column('Text ID', INTEGER(), table=<race_table>), Column('Date/Time (UTC)', TEXT(), table=<race_table>), schema=None)\nThe true power of the SQLAlchemy way of querying comes now. To replicate our raw SQL query of SELECT * FROM race_table, we can import and use the select function from the sqlalchemy library. The select function can take a (list of) Table object to select all the rows in that table. The equivalent query to SELECT * FROM race_table is then select([race_table]). More completely, we execute the following code:\nq = select([race_table])\nresult_proxy = connection.execute(q)  # ResultProxy\nresults = result_proxy.fetchmany(size=10)  # ResultSet\nObserve that the last two lines are identical as to when we were writing raw SQL queries, the only difference being the first line. Hence, the way we access results is exactly the same as we’ve discussed earlier. For the SELECT query, it might be trivial to use the select function but you can imagine that when the queries gets more complex, this will be really nice and clean. Note that you can get the raw SQL query of q by simply printing it to the console.\nReferences\n\n[1] Introduction to Databases in Python, DataCamp\n[2] Sam Hartman’s Answer to Understanding MetaData() from SQLAlchemy in Python, Stack Overflow"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "Problem statement: What characterises high fatal accidents? Can we find areas of focus to reduce such accidents?\n\nIn this project, I have identified 9 improvement opportunities to reduce high fatal accidents and raised key questions to guide the exploration of such opportunities.\nThe project involves a lot of fun viz techniques such as plotting a dendrogram based on Ward’s clustering method to visualise correlated features, and plotting a choropleth map of the UK which get your hands dirty with GeoJSON files.\nTo identify 11 key features of high fatal accidents, I applied feature selection using Chi-squared test, mutual information gain, and tree-based MDI. I then take a deeper look at these selected features to see which criteria that separates high fatal accidents and verify these criteria using a 2 proportion Z-test.\nSince the dataset was extremely imbalanced with 200 high fatal accidents in a pool of 91000+ accidents (roughly 0.002 ratio), I implemented SMOTE to oversample the minority label since uniform downsampling is not suitable since location is a key feature.\n\nThis project is hosted on DataCamp Workspace.\n\n\n\n\nProblem statement: How can we determine if the change in observed conversion rates after website redesign is not due to random chance? Moreover, can we get an exact probability of how likely a test group is to perform better/worse than the control group?\n\nIn this mini project, I analysed the conversion rates of four different user groups (one of which is the control group) using pivot tables and bar plots to get a first big picture.\nThen, using PyMC3, I applied Bayesian A/B testing based on a Metropolis sampler to measure which group is most likely to have a higher conversion rate than the control group. The Bayesian way yields a solid probability unlike the frequentist approach. For example, in this project, I was able to deduce that “there is a 99% probability that user group B has a higher conversion rate than the control group”.\nThis exact probability gives us the confidence required to make important business decisions. In this case, we can be really confident in our decision of switching to the design version exposed to group B since there is only a 1% chance of B being worse than the control. This is one of the many awesome reasons why I favor the Bayesian approach for data tasks; this also helps us answer the second problem.\n\nThis project is hosted on DataCamp Workspace.\n\n\n\n\nProblem statement: To what extent can we use a lightweight, explainable model to predict cycle hires with lowest RMSE?\nThe idea of the project is to understand trends in the usage of TfL Santander Cycles (aka Boris bikes) and then predict cycle hires using linear models which are scalable and lightweight. In this project, I have\n\nInvestigated suitability of fitting a linear model by checking existence of trends in the residuals vs fitted plot.\nVerified normality of cycle hires after Yeo-Johnson normalisation using a density, histogram and QQ plot.\nApplied feature engineering methods such as forward search model selection, one-hot encoding, moving average, and Yeo-Johnson normalisation.\nImplemented a linear model with 99% lower RMSE (935.5 to 3.5) and 155% higher R 2 score (0.27 to 0.69) relative to the baseline naive linear model.\n\nThis project was hosted on Deepnote and then hosted on DataCamp Workspace.\n\n\n\n\nThe Covid-19 pandemic made me want to look back at the history of handwashing when it was originally introduced. In this data exploration, we are able to answer questions like: (i) “How significant was the change between pre-handwashing era and post-handwashing era?” and (ii) “What is the 90% confidence interval of the mean of difference between these eras?”.\nThis project was hosted on Deepnote and then hosted on DataCamp Workspace."
  },
  {
    "objectID": "projects.html#web-app-projects",
    "href": "projects.html#web-app-projects",
    "title": "projects",
    "section": "💻 web app projects",
    "text": "💻 web app projects\n\n⁍ TradeLogger\nThis project is written in Python using the Flask framework. One of the problems with stock traders is that they do not keep the big picture of where they are in their trading journey. For example. if they are losing, do they realise that they are losing 5 trades consecutively? This tool is designed for traders to trade better. A machine learning pipeline is also added to learn the time series pattern of whether the trader has a good chance on the next trade given the outcomes of the previous trades.\n\n\n\n⁍ allpow\nThis project is written in Python using Streamlit. It is a simple calculator to estimate one’s monthly budget (in London by default). I originally made it to help my fellow Malaysian students who are coming to London for the first time organise their finances better. The first problem of coming to a new country far away from home (and also most probably first time renting a property in their life) is to estimate the prices of groceries, transport, rent and the like. This calculator solves that problem.\nLink to the deployed calculator: click here"
  },
  {
    "objectID": "projects.html#utilities",
    "href": "projects.html#utilities",
    "title": "projects",
    "section": "🔧 utilities",
    "text": "🔧 utilities\n\n⁍ EasyPS\nA simple, out of the box personal statement LaTeX framework. It helps handling duplicated tex files when writing personal statements for multiple universities, scholarships or jobs.\n\n\n⁍ PyEasyPS\nPyEasyPS is EasyPS (see above) ported to Python. It is even easier to use since the manual processes required in EasyPS (e.g. error-handling, targeted-updating of personal statements) are done for you. Essentially, it lets you focus on what’s most important – your personal statement content."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I am currently studying for an MSc in Advanced Computer Science at the University of Oxford. I recently graduated from King’s College London with a BSc in Mathematics.\nPreviously, I was an Undergraduate Research Fellow at the Department of Mathematics, King’s College London, under the supervision of Dr. Izaak Neri. There, I worked on exploring a new method to visualise large food webs, and in more generality, large complex networks.\nI am very passionate about data science and machine learning. I like to view data science as detective work. It starts with a mystery, and you have to think hard of asking the right questions to find potential culprits. You’d then have to collect evidence to verify whether these culprits are guilty or not.\nIt’s the same with data – you start with a problem, and you have to ask the right questions in hope of moving towards the potential solution. You’d then have to verify whether these are actually solutions, probably by using some statistical technique.\nWhen I’m not writing code, you can find me playing table tennis or poker (and recently, Avalon) in the MCR.\nMy tech stack includes:\n\nProgramming languages: Python, R, C/C++ (prior experience)\nData analysis: SQL, numpy, pandas, matplotlib/seaborn, Plotly\nML & stats modelling: RStan, PyMC3, scikit-learn, statsmodels, scipy, PyTorch\nWeb & dashboard: HTML/CSS/JavaScript, ReactJS, Flask, Streamlit\nOthers: Git, LaTeX, NetworkX"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Undergraduate stuff\n\nMy dissertation – Diophantine Equations, supervised by Prof. Payman Kassaei.\n\nThroughout my undergrad math life at King’s College London, I have written my notes in \\(\\LaTeX\\) to help me understand better. Some of these notes can be accessed via this repository. Notable ones include:\n\n6CCM350A Rings and Modules – based on lectures by Prof. Nicholas Shepherd-Barron, Fall 2020.\n6CCM359A Numerical and Computational Methods – based on lectures by Prof. Benjamin Doyon, Fall 2020.\n5CCM224A Introduction to Number Theory – based on lectures by Dr. James Newton, Fall 2019.\n5CCM226A Metric Spaces and Topology – based on lectures by Dr. Jerry Buckley, Fall 2019."
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html",
    "href": "posts/2023-03-05-clash-is-prime/index.html",
    "title": "Clash Series: Checking Number is Prime",
    "section": "",
    "text": "This is the first of hopefully many more posts on Clash of Code tricks I learn along the way. I call this the Clash Series and in today’s series, we look at how to write an efficient script to check for prime numbers, and how to write it fast python for those fastest mode clashes.\nEquivalently, we say p is prime if p > 1 and whenever it decomposes into p = ab, then either a=1, b=p or a=p, b=1. Thus, if p is not prime, then there is a decomposition of p into a product of positive integers a, b that are not necessarily 1 or p; hence, the name composite.\nThe first few prime numbers are 2, 3, 5, 7, 11 and so on, while the first few composite numbers are 1, 4, 6, 8, 9 and so on.\nNow here is what we are interested in."
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html#the-problem",
    "href": "posts/2023-03-05-clash-is-prime/index.html#the-problem",
    "title": "Clash Series: Checking Number is Prime",
    "section": "The problem",
    "text": "The problem\n\nproblem statement\n\nGiven an integer n \\in \\mathbb{Z}, how to enumerate all prime numbers less than n?\n\n\nA naive enumeration algorithm can be achieved in the following way: For any given integer n,\n\nIf n < 2, then n cannot be prime.\nIf n = 2, then n is prime.\nLoop over all integers k \\in [3, n). If there exists k such that k \\equiv 0 \\bmod n, then n is not prime. Otherwise n is prime.\n\nA python code for this naive implementation is as follows.\n\ndef is_prime_naive(n: int) -> bool:\n    # 1 and any negative integer are not prime.\n    if n < 2:\n        return False\n    # 2 is prime.\n    if n == 2:\n        return True\n    for k in range(2, n):\n        # If n is divisible by k for any k < n, n is not prime.\n        if n % k == 0:\n            return False\n    return True\n\nThe time complexity for this algorithm is \\mathcal{O}(n) where n is the given integer. This is already good in most cases, but can we do better?\nWell any integer n > 2 is always divisible by 2 if it is even, and therefore cannot be prime. So we can add a step to check if n is even or not before doing the looping step. As a result, our algorithm running time is cut in half as we now only have to loop over only odd numbers less than n.\n\ndef is_prime_better(n: int) -> bool:\n    # 1 and any negative integer are not prime.\n    if n < 2:\n        return False\n    # 2 is prime.\n    if n == 2:\n        return True\n    # NEW: If n is even and n is not 2, then it is not prime.\n    if n % 2 == 0:\n        return False\n    for k in range(3, n, 2):\n        # NEW: If n is divisible by odd k for any k < n, n is not prime.\n        if n % k == 0:\n            return False\n    return True\n\nThe problem, however, is that the theoretical time complexity of this algorithm is still \\mathcal{O}(n/2) = \\mathcal{O}(n). So good for small n but still problematic for super large n."
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html#efficient-algorithm",
    "href": "posts/2023-03-05-clash-is-prime/index.html#efficient-algorithm",
    "title": "Clash Series: Checking Number is Prime",
    "section": "Efficient algorithm",
    "text": "Efficient algorithm\nSo what can we do to make this code faster? Enter… the square root trick.\nFirst, without loss of generality, let’s assume that n is a positive integer since if n < 2, then it’s taken care by the n < 2 check. We now make the following claim.\n\nLemma 1 (Sqrt Decomposition) If n = ab is a composite such that 0 < a < b, then exactly one of the following is true:\n\na < \\sqrt{n} and b > \\sqrt{n},\na = b = \\sqrt{n}.\n\n\n\nProof. Suppose a < \\sqrt{n} but b < \\sqrt{n}, then ab < n which is a contradiction. Similarly, if both a and b are > \\sqrt{n}, then ab > n, also a contradiction. So we know that a \\leq \\sqrt{n} and b \\geq \\sqrt{n}, where equality holds only if n is a perfect square. \\blacksquare\n\nSo what’s the point of Lemma 1? Well the main takeaway is the following: if the number n is composite, then we will at least find one of its divisors before or equal its integer square root \\lfloor \\sqrt{n} \\rfloor. This speeds up our algorithm tremendously, especially for large n. In fact, even for small n like n=1080, its integer square root is 32 which is already two order of magnitudes lower.\nWe now add the sqrt trick to our python implementation.\n\ndef is_prime_sqrt_trick(n: int) -> bool:\n    # 1 and any negative integer are not prime.\n    if n < 2:\n        return False\n    # 2 is prime.\n    if n == 2:\n        return True\n    # If n is even and n is not 2, then it is not prime.\n    if n % 2 == 0:\n        return False\n    # NEW: loop until int(sqrt(n)) + 1.\n    # The + 1 is to handle if n is perfect square.\n    for k in range(3, int(n**.5)+1, 2):\n        # If n is divisible by odd k for any k < n, n is not prime.\n        if n % k == 0:\n            return False\n    return True\n\nThe sqrt trick improves time complexity from \\mathcal{O}(n) to \\mathcal{O}(\\sqrt{n}) which is massive. To give you an idea of this speedup, we will run the naive and sqrt algorithms to check if n = 2,147,462,143 is prime. Note that this is a number on the order of 2^{31}.\n\n\nspeedup comparison\nimport time\n\nn = 2_147_462_143\n\ndef comp(f: callable) -> None:\n    start = time.time()\n    f(n)\n    print(f\"Took total of {(time.time()-start):.10f} seconds using {f.__name__}\")\ncomp(is_prime_naive)\ncomp(is_prime_sqrt_trick)\n\n\nTook total of 118.2879400253 seconds using is_prime_naive\nTook total of 0.0010151863 seconds using is_prime_sqrt_trick"
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html#efficient-writing",
    "href": "posts/2023-03-05-clash-is-prime/index.html#efficient-writing",
    "title": "Clash Series: Checking Number is Prime",
    "section": "Efficient writing",
    "text": "Efficient writing\nSo the impementation is super efficient \\mathcal{O}(\\sqrt{n}), but how do we make writing the code efficient for something like Clash of Code?\nWell first note that when writing in a clash, you don’t care about the 80 char PEP format or readability. So it’s finally time to write one-liner 100 chars fugly code.\nThe trick I use is to use all to replace the for-loop and replace the False checks with True statement by “bundling” their evaluation using and. This gives the following one-liner.\n\ndef is_prime_sqrt_short(n: int) -> bool:\n    return n == 2 or (n > 2 and n % 2 != 0 and all(n % k != 0 for k in range(3, int(n**.5)+1, 2)))\n\nIn fact, since the modulo operator % in python returns a positive integer, we can save writing one character for each != by writing > instead.\n\ndef is_prime_sqrt_super_short(n: int) -> bool:\n    return n == 2 or (n > 2 and n % 2 > 0 and all(n % k > 0 for k in range(3, int(n**.5)+1, 2)))"
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html#efficient-algorithm-sqrt-trick",
    "href": "posts/2023-03-05-clash-is-prime/index.html#efficient-algorithm-sqrt-trick",
    "title": "Clash Series: Checking Number is Prime",
    "section": "Efficient algorithm (sqrt trick)",
    "text": "Efficient algorithm (sqrt trick)\nSo what can we do to make this code faster? Enter… the square root (sqrt) trick.\nFirst, without loss of generality, let’s assume that n is a positive integer since if n < 2, then it’s taken care by the n < 2 check. We now make the following claim.\n\nLemma 1 (Sqrt Decomposition) Let n > 1 be a positive integer. If n = ab is a composite integer such that 0 < a \\leqslant b, then a \\leqslant \\sqrt{n} \\leqslant b.\n\n\nProof. First observe that n = ab is a perfect square if and only if a = b = \\sqrt{n}, in which case we are done.\nSo suppose n = ab is not a perfect square. Then a and b cannot be \\sqrt{n} and moreover a < b. We now claim that a < \\sqrt{n} < b. Suppose a < \\sqrt{n} but also b < \\sqrt{n}, then ab < n which is a contradiction. Similarly, if both a > \\sqrt{n} and b > \\sqrt{n}, then ab > n, also a contradiction. Thus, since a < b, it has to be that a < \\sqrt{n} < b. \\blacksquare\n\nSo what’s the point of Lemma 1? Well the main takeaway is the following: if the number n is composite, then we will at least find one of its divisors before or equal its integer square root \\lfloor \\sqrt{n} \\rfloor.\n\nTheorem 1 Let n be a composite positive integer. Then there exists a prime divisor p of n such that p \\leqslant \\lfloor \\sqrt{n} \\rfloor.\n\n\nProof. By Lemma Lemma 1, we know that if n = kb is a composite integer with 0 < k \\leqslant b, then k \\leqslant \\sqrt{n}, with equality iff n is a perfect square. Equivalently, this means that k \\leqslant \\lfloor \\sqrt{n} \\rfloor. If k is prime, then we are done. Otherwise, there exists a prime p that divides k (hence, divides n) that would also satisfy p \\leqslant \\lfloor \\sqrt{n} \\rfloor as desired. \\blacksquare\n\n\nNote: for a linear traversal check (like we are doing, starting from 3, 4, 5, …), we know that we will encounter any prime divisor p of any composite divisor k of n first. So the last part of the proof is unnecessary for our need, but we put it there for completeness.\n\nSo the idea of using sqrt decomposition speeds up our algorithm tremendously, especially for large n. In fact, even for small n like n=1080, its integer square root is 32 which is already two order of magnitudes lower.\nWe now add the sqrt trick to our python implementation.\n\ndef is_prime_sqrt_trick(n: int) -> bool:\n    # 1 and any negative integer are not prime.\n    if n < 2:\n        return False\n    # 2 is prime.\n    if n == 2:\n        return True\n    # If n is even and n is not 2, then it is not prime.\n    if n % 2 == 0:\n        return False\n    # NEW: loop until int(sqrt(n)) + 1.\n    # The + 1 is to handle if n is perfect square.\n    for k in range(3, int(n**.5)+1, 2):\n        # If n is divisible by odd k for any k < n, n is not prime.\n        if n % k == 0:\n            return False\n    return True\n\nThe sqrt trick improves time complexity from \\mathcal{O}(n) to \\mathcal{O}(\\sqrt{n}) which is massive! To give you an idea of this speedup, we will run the naive and sqrt algorithms to check if n = 2{,}147{,}462{,}143 is prime. Note that this is a prime number on the order of 2^{31}.\n\nThe sqrt trick improves time complexity from \\mathcal{O}(n) to \\mathcal{O}(\\sqrt{n}) which is massive!\n\n\n\nspeedup comparison ⚡️\n\nimport time\n\nn = 2_147_462_143\n\nfor f in [is_prime_naive, is_prime_sqrt_trick]:\n    start = time.time()\n    f(n)\n    print(f\"Took total of {(time.time()-start):.10f} seconds using {f.__name__}\")\n\nTook total of 118.3165051937 seconds using is_prime_naive\nTook total of 0.0010521412 seconds using is_prime_sqrt_trick"
  },
  {
    "objectID": "posts/draft_2023-03-06-ls-aws-in-python/index.html",
    "href": "posts/draft_2023-03-06-ls-aws-in-python/index.html",
    "title": "How to do aws s3 ls s3://bucket/ using boto3 in python?",
    "section": "",
    "text": "Today, we look at how can we perform the following terminal command in python."
  },
  {
    "objectID": "posts/draft_2023-03-06-ls-aws-in-python/index.html#why-do-we-even-care",
    "href": "posts/draft_2023-03-06-ls-aws-in-python/index.html#why-do-we-even-care",
    "title": "How to do aws s3 ls s3://bucket/ using boto3 in python?",
    "section": "Why do we even care?",
    "text": "Why do we even care?\nA lot of times, you just want to list all the existing subobjects in a given object without getting its content. A typical use case is to list all existing objects in the bucket, where here, the bucket is viewed as an object – the root object. This list action can be achieved using the simple aws s3 ls command in the terminal. But what if we want to do it natively as part of a module in python? That will be our goal for today.\nTo ensure clarity, let’s kick off with the definition of a path, a bucket and an object prefix.\n\ndefinition\n\nA path is a string that consists of an S3 tag, a bucket and an object prefix. For example, s3://bucket/object/subobject/... is a generic path where s3:// is the S3 tag, bucket is the bucket and object/subobject/... is the object prefix. Note the position of / carefully."
  },
  {
    "objectID": "posts/draft_2023-03-06-ls-aws-in-python/index.html#aws-s3-ls-in-python",
    "href": "posts/draft_2023-03-06-ls-aws-in-python/index.html#aws-s3-ls-in-python",
    "title": "How to do aws s3 ls s3://bucket/ using boto3 in python?",
    "section": "aws s3 ls in python",
    "text": "aws s3 ls in python\nI assume that you’ve done the standard AWS credentials step, storing it at ~/.aws/credentials for example. We can then initialize an S3 client in Python using boto3.session.Session, I hope this step is familiar to you.\nimport boto3\n\nsession = boto3.session.Session()\nclient = session.client(\"s3\")\nNow that we have our S3 client, define our bucket and object prefix of interest.\nbucket = \"bucket_name\"\nobj_prefix = \"obj_in_bucket/\"\nThen we can simply list all the existing subobjects of our given object prefix using the following function:\ndef aws_s3_ls(bucket: str, obj_prefix: str):\n    params = dict(Bucket=bucket, Prefix=obj_prefix, Delimiter=\"/\")\n\n    paginator = client.get_paginator(\"list_objects_v2\")\n    for page in paginator.paginate(**params):\n        for obj in page.get(\"CommonPrefix\", []):\n            print(\"PRE\", obj[\"Key\"])\n\naws_s3_ls(bucket, obj_prefix)\nPRE subobj_1/\nPRE subobj_2/\nPRE subobj_3/\n...\nAnd that’s it! That was a short one and I hope to cover more AWS things in the future."
  },
  {
    "objectID": "posts/2023-03-06-ls-aws-in-python/index.html",
    "href": "posts/2023-03-06-ls-aws-in-python/index.html",
    "title": "How to do aws s3 ls s3://bucket/ using boto3 in python?",
    "section": "",
    "text": "Today, we look at how can we perform the following terminal command in python."
  },
  {
    "objectID": "posts/2023-03-06-ls-aws-in-python/index.html#why-do-we-even-care",
    "href": "posts/2023-03-06-ls-aws-in-python/index.html#why-do-we-even-care",
    "title": "How to do aws s3 ls s3://bucket/ using boto3 in python?",
    "section": "Why do we even care?",
    "text": "Why do we even care?\nA lot of times, you just want to list all the existing subobjects in a given object without getting its content. A typical use case is to list all existing objects in the bucket, where here, the bucket is viewed as an object – the root object. This list action can be achieved using the simple aws s3 ls command in the terminal.\nBut what if we want to do it natively as part of a module in python? For example, if we want to trigger a callback on the server whenever a new subobject is found in our bucket. A fast way is to just perform an equivalent aws s3 ls in our python module, and this is our goal today.\nTo ensure clarity, let’s kick off with the definition of a path, a bucket and an object prefix.\n\ndefinition\n\nA path is a string that consists of an S3 tag, a bucket and an object prefix. For example, s3://bucket/object/subobject/... is a generic path where s3:// is the S3 tag, bucket is the bucket and object/subobject/... is the object prefix. Note the position of / carefully."
  },
  {
    "objectID": "posts/2023-03-06-ls-aws-in-python/index.html#aws-s3-ls-in-python",
    "href": "posts/2023-03-06-ls-aws-in-python/index.html#aws-s3-ls-in-python",
    "title": "How to do aws s3 ls s3://bucket/ using boto3 in python?",
    "section": "aws s3 ls in python",
    "text": "aws s3 ls in python\nI assume that you’ve done the standard AWS credentials step, storing it at ~/.aws/credentials for example. We can then initialize an S3 client in Python using boto3.session.Session, I hope this step is familiar to you.\nimport boto3\n\nsession = boto3.session.Session()\nclient = session.client(\"s3\")\nNow that we have our S3 client, define our bucket and object prefix of interest.\nbucket = \"bucket_name\"\nobj_prefix = \"obj_in_bucket/\"\nThen we can simply list all the existing subobjects of our given object prefix using the following function:\ndef aws_s3_ls(bucket: str, obj_prefix: str):\n    params = dict(Bucket=bucket, Prefix=obj_prefix, Delimiter=\"/\")\n\n    paginator = client.get_paginator(\"list_objects_v2\")\n    for page in paginator.paginate(**params):\n        for obj in page.get(\"CommonPrefix\", []):\n            print(\"PRE\", obj[\"Key\"])\n\naws_s3_ls(bucket, obj_prefix)\nPRE subobj_1/\nPRE subobj_2/\nPRE subobj_3/\n...\nAnd that’s it! That was a short one and I hope to cover more AWS things in the future."
  }
]