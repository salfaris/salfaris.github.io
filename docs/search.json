[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Undergraduate stuff @ KCL\n\nMy dissertation ‚Äì Diophantine Equations, supervised by Prof.¬†Payman Kassaei.\n\nI wrote my undergrad math notes in \\(\\LaTeX\\) to help me understand better. These notes can replace lecture notes and are around 100 pages long each. They can be accessed via this repository. Notable ones include:\n\n6CCM350A Rings and Modules ‚Äì based on lectures by Prof.¬†Nicholas Shepherd-Barron, Fall 2020.\n6CCM359A Numerical and Computational Methods ‚Äì based on lectures by Prof.¬†Benjamin Doyon, Fall 2020.\n5CCM224A Introduction to Number Theory ‚Äì based on lectures by Dr.¬†James Newton, Fall 2019.\n5CCM226A Metric Spaces and Topology ‚Äì based on lectures by Dr.¬†Jerry Buckley, Fall 2019.\n\n\n\nPostgraduate stuff @ Oxford\n\nMy dissertation ‚Äì ‚ÄúEnhancing VAE-learning on spatial priors using graph convolutional networks‚Äù, supervised by Seth Flaxman, Swapnil Mishra and Elizaveta Semenova.\nImplemented ‚ÄúVariational Graph Auto-Encoders‚Äù (Kipf and Welling, 2016) in JAX as part of my dissertation. Source code can be found in this repo.\nReproduced and extended ‚ÄúInference Suboptimality in Variational Autoencoders‚Äù (Cremer et. al, 2018) using JAX. Built together with Basim Khajwal, Snehal Raj and Vasileios Ntogram. Source code can be found in this repo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "random technical thoughts.",
    "section": "",
    "text": "A benchmark for testing quantum computers with Clifford+T hardware\n\n\n\n\n\n\nalgorithms\n\n\nquantum-computing\n\n\nzx-calculus\n\n\nbenchmarking\n\n\n\nWe provide a quantum algorithm that can be used to benchmark quantum computers with Clifford+T hardware and provide a proof of the algorithm using ZX-calculus.\n\n\n\n\n\nAug 4, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of payments fraud detection\n\n\n\n\n\n\nfraud-detection\n\n\nmachine-learning\n\n\n\nHow can machine learning help make payments fraud detection efficient?\n\n\n\n\n\nJun 2, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nA mathematical take on when to take a loan such as ASB Financing\n\n\n\n\n\n\nquant\n\n\ninvestment\n\n\nstrategies\n\n\n\nWe look at formalizing the parity between investing with a disposable capital versus with a financed leveraged capital in ASB.\n\n\n\n\n\nApr 22, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nRevisit: My Quora Post on ‚ÄúWhat is an example of an IB math HL 20/20 exploration?‚Äù\n\n\n\n\n\n\nIB\n\n\nmathematics\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do aws s3 ls s3://bucket/ using boto3 in python?\n\n\n\n\n\n\ncloud\n\n\nAWS\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 12, 2023\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nClash Series: Checking Number is Prime\n\n\n\n\n\n\nclash of code\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\nMar 5, 2022\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nPreparing Image Dataset for Neural Networks in PyTorch\n\n\n\n\n\n\nmachine learning\n\n\ncomputer vision\n\n\npytorch\n\n\ndata preprocessing\n\n\n\n\n\n\n\n\n\nSep 7, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nThe Coding Kata\n\n\n\n\n\n\nsoftware engineering\n\n\nclean coding\n\n\n\n\n\n\n\n\n\nAug 18, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nPlotly.py main theme in Plotly.js\n\n\n\n\n\n\njavascript\n\n\nplotly\n\n\npython\n\n\nweb dev\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nUnit Testing with pytest\n\n\n\n\n\n\npython\n\n\ntesting\n\n\nsoftware engineering\n\n\n\nWhy do we do unit testing and how to do unit testing in python using pytest.\n\n\n\n\n\nMay 3, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nRelational Databases in Python (part I)\n\n\n\n\n\n\ndata science\n\n\npython\n\n\nsql\n\n\nsqlalchemy\n\n\ndatabases\n\n\ndata handling\n\n\n\n\n\n\n\n\n\nApr 30, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nMaking pastel-colored boxes using tcolorbox in LaTeX\n\n\n\n\n\n\nlatex\n\n\nwriting\n\n\n\n\n\n\n\n\n\nApr 7, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nRoot-hunting algorithm: Newton‚Äôs method\n\n\n\n\n\n\nnumerical analysis\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging again\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\nSalman Faris\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "This project covers strategy research, experimentation (with reporting) and deployment via Google Cloud.\n\n\nResearched and verified the class weight strategy in ‚ÄúFraud Detection using Machine Learning‚Äù (Oza, 2018); and further extended the paper‚Äôs analysis with new models and metrics. I wrote a full experiment report on this strategy verification which can be found here: Replication & extension of ‚ÄúFraud Detection using Machine Learning‚Äù (Oza, 2018).\nTrained logistic regression, SVMs and decision trees to perform financial transactions fraud detection in an imbalanced dataset of 6+ million rows and 0.1% sample fraud transactions (Python, scikit-learn).\nBuilt a web app to visualise streaming transactions data and perform live fraud detection using trained models. (Python, Streamlit, Docker, Google Artifact Registry, Google Cloud Run, Google Cloud Pub/Sub, Google Vertex AI).\n\n\n\n\n\nProblem statement: How can we determine if the change in observed conversion rates after website redesign is not due to random chance? Moreover, can we get an exact probability of how likely a test group is to perform better/worse than the control group?\n\n\nIn this mini project, I analysed the conversion rates of four different user groups (one of which is the control group) using pivot tables and bar plots to get a first big picture.\nThen, using PyMC3, I applied Bayesian A/B testing based on a Metropolis sampler to measure which group is most likely to have a higher conversion rate than the control group. The Bayesian way yields a solid probability unlike the frequentist approach. For example, in this project, I was able to deduce that ‚Äúthere is a 99% probability that user group B has a higher conversion rate than the control group‚Äù.\nThis exact probability gives us the confidence required to make important business decisions. In this case, we can be really confident in our decision of switching to the design version exposed to group B since there is only a 1% chance of B being worse than the control. This is one of the many awesome reasons why I favor the Bayesian approach for data tasks; this also helps us answer the second problem.\n\nThis project is hosted on DataCamp Workspace.\n\n\n\n\nProblem statement: To what extent can we use a lightweight, explainable model to predict cycle hires with lowest RMSE?\nThe idea of the project is to understand trends in the usage of TfL Santander Cycles (aka Boris bikes) and then predict cycle hires using linear models which are scalable and lightweight. In this project, I have\n\nInvestigated suitability of fitting a linear model by checking existence of trends in the residuals vs fitted plot.\nVerified normality of cycle hires after Yeo-Johnson normalisation using a density, histogram and QQ plot.\nApplied feature engineering methods such as forward search model selection, one-hot encoding, moving average, and Yeo-Johnson normalisation.\nImplemented a linear model with 99% lower RMSE (935.5 to 3.5) and 155% higher R 2 score (0.27 to 0.69) relative to the baseline naive linear model.\n\nThis project was hosted on Deepnote and then hosted on DataCamp Workspace."
  },
  {
    "objectID": "projects.html#data-science",
    "href": "projects.html#data-science",
    "title": "projects",
    "section": "",
    "text": "This project covers strategy research, experimentation (with reporting) and deployment via Google Cloud.\n\n\nResearched and verified the class weight strategy in ‚ÄúFraud Detection using Machine Learning‚Äù (Oza, 2018); and further extended the paper‚Äôs analysis with new models and metrics. I wrote a full experiment report on this strategy verification which can be found here: Replication & extension of ‚ÄúFraud Detection using Machine Learning‚Äù (Oza, 2018).\nTrained logistic regression, SVMs and decision trees to perform financial transactions fraud detection in an imbalanced dataset of 6+ million rows and 0.1% sample fraud transactions (Python, scikit-learn).\nBuilt a web app to visualise streaming transactions data and perform live fraud detection using trained models. (Python, Streamlit, Docker, Google Artifact Registry, Google Cloud Run, Google Cloud Pub/Sub, Google Vertex AI).\n\n\n\n\n\nProblem statement: How can we determine if the change in observed conversion rates after website redesign is not due to random chance? Moreover, can we get an exact probability of how likely a test group is to perform better/worse than the control group?\n\n\nIn this mini project, I analysed the conversion rates of four different user groups (one of which is the control group) using pivot tables and bar plots to get a first big picture.\nThen, using PyMC3, I applied Bayesian A/B testing based on a Metropolis sampler to measure which group is most likely to have a higher conversion rate than the control group. The Bayesian way yields a solid probability unlike the frequentist approach. For example, in this project, I was able to deduce that ‚Äúthere is a 99% probability that user group B has a higher conversion rate than the control group‚Äù.\nThis exact probability gives us the confidence required to make important business decisions. In this case, we can be really confident in our decision of switching to the design version exposed to group B since there is only a 1% chance of B being worse than the control. This is one of the many awesome reasons why I favor the Bayesian approach for data tasks; this also helps us answer the second problem.\n\nThis project is hosted on DataCamp Workspace.\n\n\n\n\nProblem statement: To what extent can we use a lightweight, explainable model to predict cycle hires with lowest RMSE?\nThe idea of the project is to understand trends in the usage of TfL Santander Cycles (aka Boris bikes) and then predict cycle hires using linear models which are scalable and lightweight. In this project, I have\n\nInvestigated suitability of fitting a linear model by checking existence of trends in the residuals vs fitted plot.\nVerified normality of cycle hires after Yeo-Johnson normalisation using a density, histogram and QQ plot.\nApplied feature engineering methods such as forward search model selection, one-hot encoding, moving average, and Yeo-Johnson normalisation.\nImplemented a linear model with 99% lower RMSE (935.5 to 3.5) and 155% higher R 2 score (0.27 to 0.69) relative to the baseline naive linear model.\n\nThis project was hosted on Deepnote and then hosted on DataCamp Workspace."
  },
  {
    "objectID": "projects.html#research-ml",
    "href": "projects.html#research-ml",
    "title": "projects",
    "section": "üî¨ research ML",
    "text": "üî¨ research ML\n\nMSc thesis. ‚ÄúEnhancing VAE-learning on spatial priors using GCN‚Äù\nMy Oxford MSc thesis supervised by Seth Flaxman, Swapnil Mishra and Elizaveta Semenova.\n\nI made the observation that the PriorVAE framework in ‚ÄúPriorVAE: encoding spatial priors with variational autoencoders for small-area estimation‚Äù (Semenova et. al, 2022) does not make efficient use of the data‚Äôs local neighbourhood structure when learning spatial priors for MCMC sampling.\nSo I proposed a new framework, called PriorVGAE, which exploits the underlying local neighbourhood structure and propagates this information efficiently via the use of graph convolutional networks (GCN). Under this new framework, we were able to perform MCMC inference with far superior performance as compared to PriorVAE while enjoying better sampling speeds. We achieved lower mean squared error (MSE) for our sample reconstructions across all our experiments and attained higher effective sample sizes with lower MCMC sampling time.\nThe breakthrough. Naively injecting a GCN into a VAE framework would not make the architecture work. The key observation was to realize that spatial priors inherently admits global information. To this end, I introduced a local-to-global scheme that consolidates all the local information into a global representation, relying on the universal approximator theorem of MLPs, to efficiently learn the spatial priors.\nSome bonus advantages of this framework as compared to PriorVAE include getting computationally cheaper training. We now have fewer learnable parameters since we replace MLPs with GCNs and GCN weights are shared across the support. Out-of-sample predictions are also now tractable as restarting compute training is relatively inexpensive.\n\n\n\nReproduction of ‚ÄúInference Suboptimality in Variational Autoencoders‚Äù (Cremer et. al, 2018) in JAX\nWe reproduced some of the key interesting experiments in the paper ‚ÄúInference Suboptimality in Variational Autoencoders‚Äù (Cremer et. al, 2018) using JAX and extended the result to the K-MNIST dataset. This work was done jointly with Basim Khajwal, Snehal Raj and Vasileios Ntogram.\n\n\n\n\nReproduction of ‚ÄúVariational Graph Auto-Encoders‚Äù (Kipf and Welling, 2016) in JAX\nI reproduced the variational graph autoencoder (VGAE) deep learning model using JAX to be used as a component in my MSc thesis."
  },
  {
    "objectID": "projects.html#web-app",
    "href": "projects.html#web-app",
    "title": "projects",
    "section": "üíª web app",
    "text": "üíª web app\n\nTradeLogger\nThis project is written in Python using the Flask framework. One of the problems with stock traders is that they do not keep the big picture of where they are in their trading journey. For example. if they are losing, do they realise that they are losing 5 trades consecutively? This tool is designed for traders to trade better. A machine learning pipeline is also added to learn the time series pattern of whether the trader has a good chance on the next trade given the outcomes of the previous trades.\n\n\n\n\nallpow\nThis project is written in Python using Streamlit. It is a simple calculator to estimate one‚Äôs monthly budget (in London by default). I originally made it to help my fellow Malaysian students who are coming to London for the first time organise their finances better. The first problem of coming to a new country far away from home (and also most probably first time renting a property in their life) is to estimate the prices of groceries, transport, rent and the like. This calculator solves that problem.\nLink to the deployed calculator: click here"
  },
  {
    "objectID": "projects.html#utilities",
    "href": "projects.html#utilities",
    "title": "projects",
    "section": "üîß utilities",
    "text": "üîß utilities\n\nEasyPS\nA simple, out of the box personal statement LaTeX framework. It helps handling duplicated tex files when writing personal statements for multiple universities, scholarships or jobs.\n\n\n\n\nPyEasyPS\nPyEasyPS is EasyPS (see above) ported to Python. It is even easier to use since the manual processes required in EasyPS (e.g.¬†error-handling, targeted-updating of personal statements) are done for you. Essentially, it lets you focus on what‚Äôs most important ‚Äì your personal statement content."
  },
  {
    "objectID": "projects.html#open-source",
    "href": "projects.html#open-source",
    "title": "projects",
    "section": "üì≠ open source",
    "text": "üì≠ open source\n\nJraph\nI am a key contributor of Jraph which is Google DeepMind‚Äôs open source library for graph neural networks using JAX."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I am currently a Senior Data Scientist at Dyson, helping decision making at the highest level in Dyson‚Äôs new Home Business Unit. I spend my 9-5 dealing with Python, SQL, PySpark, GCP (BigQuery, DataProc, Pub/Sub, App Engine, Vertex AI, among others), on top of mentoring and leading the data science side in my team.\nPrior to work, I studied Mathematics at King‚Äôs College London and got my BSc, then went on to do my MSc in Advanced Computer Science at the University of Oxford.\nWhen I‚Äôm not writing code, I like to explore new hobbies. Most recently, I picked up cycling and running. I enjoy good acoustic music and always down for a game of table tennis or poker.\nMy tech stack has changed over the years, with some technologies I haven‚Äôt used for quite a while now. This is a good overview of what I still have in my mind:\n\nProgramming languages: Python, R, (prior experience), C/C++ (prior experience)\nData analysis: SQL, PySpark, PyData stack, Plotly, NetworkX, Google BigQuery, AWS Redshift (yes, I‚Äôve used both).\nML & stats modelling: scikit-learn, statsmodels, scipy, PyMC3, PyTorch, Jax\nWeb & dashboard: HTML/CSS/JavaScript, Tableau, ReactJS, Flask, Streamlit\nDeployment: Git, Docker, Google Cloud Platform (GCP) stack, AWS Redshift\nOthers: LaTeX"
  },
  {
    "objectID": "posts/2021-08-18-coding-kata/index.html",
    "href": "posts/2021-08-18-coding-kata/index.html",
    "title": "The Coding Kata",
    "section": "",
    "text": "A coding kata is an exercise to sharpen your programming skills and muscle memory via repetition. I learnt this concept when I was reading The Clean Coder (highly recommended!) and have since adopted it as part of my programming routine. It helps my hands ‚Äúknow‚Äù what to type when I need to type.\nI have also adapted the concept of kata as a means to learn/revise programming languages effectively. Instead of writing a random script doing god knows what after learning the syntax, I would implement the simple linear regression algorithm from scratch in this language. Why this is a good kata you ask? Here is why among others:\n\nYou can practice using OOP in the new lang\nYou will deal with ints, doubles, static and dynamic arrays.\nYou will do some basic math operations.\nYou will implement at least one function.\nYou will implement at least a for loop.\nYou might use an import.\n\nTo put simply, it ensures you use a lot of the functionalities in the language and actually spend time doing it.\nI recently wanted to refresh my mind on the C++ lang and I also want to reinforce my JavaScript knowledge. So here‚Äôs my linear regression implementation in these two languages (although the way data is expected is a bit different in the JS implementation compared to the C++ implementation).\n\nC++JavaScript\n\n\n#include &lt;vector&gt;\nusing namespace std;\n\nclass LinearRegression {\n    private:\n        int m_epoch;\n        double m_learningRate;\n        vector&lt;double&gt; weights;\n\n    public:\n        LinearRegression(int epoch, double learningRate) {\n            this-&gt;m_epoch = epoch;\n            this-&gt;m_learningRate = learningRate;\n        }\n\n        void fit(vector&lt;double&gt; x, vector&lt;double&gt; y) {\n            vector&lt;double&gt; weights = {0, 0};\n            int dataLength = x.size();\n\n            int epoch = this-&gt;m_epoch;\n            for (int e = 0; e &lt; epoch; e++) {\n                for (int i = 0; i &lt; dataLength; i++) {\n                    double estimate = weights[0] + x[i]*weights[1];\n                    double error = y[i] - estimate;\n\n                    weights[0] += this-&gt;m_learningRate * error;\n                    weights[1] += this-&gt;m_learningRate * error * x[i];\n                }\n            }\n\n            this-&gt;weights = weights;\n        }\n\n        vector&lt;double&gt; predict(vector&lt;double&gt; x) {\n            int dataLength = x.size();\n            vector&lt;double&gt; yPred;\n            yPred.reserve(dataLength);  // Preallocate length of yPred based on size of x.\n\n            vector&lt;double&gt; weights = this-&gt;weights;\n            for (int i = 0; i &lt; dataLength; i++) {\n                yPred.push_back(weights[0] + x[i]*weights[1]);\n            }\n\n            return yPred;\n        }\n};\n\n\nclass LinearRegression {\n  constructor(params = {}) {\n    this.weights = params.weights || [];\n    this.learningRate = params.learningRate || 0.01;\n    this.data = [];\n    this.fittedValues = [];\n  }\n\n  estimator(x, weights) {\n    const [w0, w1] = weights;\n    return w0 + x * w1;\n  }\n\n  fit(data) {\n    this.data = data;\n    if (this.weights === undefined || this.weights.length === 0) {\n      this.weights = [0, 0];\n    }\n\n    for (let i = 0; i &lt; this.data.length; i++) {\n      const { x, y } = this.data[i];\n\n      const estimate = this.estimator(x, this.weights);\n      const error = y - estimate;\n\n      let [w0, w1] = this.weights;\n\n      w0 += this.learningRate * error;\n      w1 += this.learningRate * error * x;\n\n      this.weights = [w0, w1];\n    }\n\n    this.fittedValues = this.getFittedValues();\n  }\n\n  getFittedValues() {\n    return this.data.map(({ x, y }) =&gt; {\n      return { x: x, y: this.estimator(x, this.weights) };\n    });\n  }\n}\n\n\n\nI leave the Python implementation to you ;)"
  },
  {
    "objectID": "posts/2024-08-04-quantum-fibonacci/index.html",
    "href": "posts/2024-08-04-quantum-fibonacci/index.html",
    "title": "A benchmark for testing quantum computers with Clifford+T hardware",
    "section": "",
    "text": "I was revisiting some of my MSc stuff from two years ago and I came across one of my original work in Quantum Processes and Computations (QPC). It was one of the funner courses I took ‚Äì I had a genuine interest in the hype behind quantum computing at the time and the prerequisite for the course was just simple linear algebra. Aleks Kissinger taught the course and he‚Äôs incredibly passionate about the subject and is also a good explainer. He was also the one who interviewed me to get into the program where he randomly asked me about my favourite theorem in Galois Theory, really peeking my interest to the point of ‚Äúwhat the heck are they cooking in the quantum computing group‚Äù but that‚Äôs another story.\nIn QPC, a part of the take-home ‚Äúexam‚Äù was to produce anything interesting using the ZX-calculus. Bonus points if it‚Äôs original, has a strong motivation and/or has a rigorous proof. ‚ÄúOriginal‚Äù here was a bit vague in the sense that various parts can be original ‚Äì the idea, implementation, proof, etc. I remember just spending several days trying to figure out what to do ‚Äî really went deep into theorems in number theory, for example, and I manage to land on something original and provided a working proof, but not so much on good motivation I confess. Ultimately, I decided to keep it simple and build on a Fibonacci heuristic proposed in (Gilliam, Pistoia, and Gonciulea 2020), where I modified the routine they wrote, implemented it in the ZX-calculus and proved that the algorithm is correct. The routine relies solely on Clifford+T gates and because of the simplicity of the Fibonacci algorithm (as we shall see), it ought to serve as a good benchmarking tool for quantum computers with hardware that accepts only Clifford+T gates.\nBefore I go any further, I have to warn you that this is neither an introduction to quantum computing nor ZX-calculus. What I can tell you is that ZX-calculus gives you superpowers for reasoning in quantum computing. If you want a primer on both at the same time, check out (Coecke and Kissinger 2017) which is where I learnt all the basics from. Another good resource is John‚Äôs ZX-calculus primer ZX-calculus for the working quantum computer scientist. John van de Wetering is also one of the guys who taught QPC alongside Aleks. I realize that nowadays, there‚Äôs also less dense intros to the subject as well like this one at PennyLane."
  },
  {
    "objectID": "posts/2024-08-04-quantum-fibonacci/index.html#what-does-combinatorics-tells-us-about-fibonacci-numbers",
    "href": "posts/2024-08-04-quantum-fibonacci/index.html#what-does-combinatorics-tells-us-about-fibonacci-numbers",
    "title": "A benchmark for testing quantum computers with Clifford+T hardware",
    "section": "What does combinatorics tells us about Fibonacci numbers?",
    "text": "What does combinatorics tells us about Fibonacci numbers?\nDefine a recurrence relation F_n = F_{n-1} + F_{n-2} with initial conditions F_0=1 and F_1 = 2 where n &gt; 1. The positive integers F_n generated by this recurrence relation is universally known today as the Fibonacci sequence 1, 2, 3, 5, 8, 13, ... and so on. At this point, it is not straightforward how we can encode this relation into a quantum circuit. So how do we encode this information to perform quantum computation? If I learn anything from my short stint of constructing quantum algorithms, you can always end up with a working algorithm by converting the problem into a counting problem. If you have a counting problem, you can perform what is called heuristic encoding where you can design a quantum circuit that ‚Äúgenerates‚Äù the things you want to count, run the circuit repeatedly for a sufficiently long period, and then count the number of measurement outcomes, possibly with some required classical post-processing.\nFor the Fibonacci numbers, combinatorics tells us that there is an equivalence between the Fibonacci numbers and the number of length n binary sequences with no consecutive ones where this equivalence is given as F_n = \\left|B^n \\right| where\nB^n = \\left\\{ x \\in \\{0,1\\}^n : x \\text{ has no consecutive ones} \\right\\}.\nWith this formulation of the Fibonacci numbers, we now have a counting problem rather than a recurrence problem where we can perform heuristic encoding! So let‚Äôs now turn our attention to constructing a quantum routine that generates elements of B^n."
  },
  {
    "objectID": "posts/2024-08-04-quantum-fibonacci/index.html#building-a-quantum-routine-for-generating-elements-of-bn",
    "href": "posts/2024-08-04-quantum-fibonacci/index.html#building-a-quantum-routine-for-generating-elements-of-bn",
    "title": "A benchmark for testing quantum computers with Clifford+T hardware",
    "section": "Building a quantum routine for generating elements of B^n",
    "text": "Building a quantum routine for generating elements of B^n\nLet‚Äôs think about this from the ground up and consider n=2. How can we generate elements of B^2? Enumerating B^2 gives us the set\nB^2 = \\left\\{ 00, 01, 10 \\right\\},\nwhere it‚Äôs missing the sole binary sequence 11 \\in \\{ 0, 1\\}^2. One way to think about this is to build a quantum routine that generates the outcomes \\ket {00}, \\ket {01}, \\ket {10} but not \\ket {10}. To achieve this, you want a 2-qubit system such that\n\nqubit 1 admits equal superposition when measured; but\nqubit 2 to be in equal superposition only if the first qubit is in the state \\ket {0}.\n\nAn easy way to do this is to use two X_{\\pi/2} rotation gates together with a single CX_{\\pi/2} controlled rotation gate where X_{\\pi/2} is a 2x2 matrix given by\nX_{\\pi/2} = \\begin{pmatrix} \\cos{\\pi/4} & -i \\sin{\\pi/4} \\\\ -i \\sin{\\pi/4} & \\cos{\\pi/4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} \\end{pmatrix},\nand CX_{\\pi/2} is a 4x4 matrix given by CX_{\\pi/2} = \\begin{pmatrix} I_2 & 0 \\\\ 0 & X_{\\pi/2} \\end{pmatrix}.\nHere I_2 is the 2x2 identity matrix and 0 is a block 2x2 matrix of zeros. For a sanity check, you can evaluate the product\n (CX_{\\pi/2})  (X_{\\pi/2} \\otimes X_{\\pi/2})  (\\ket {0} \\otimes \\ket {0})\nto see that we indeed have a quantum state that yields binary sequences in B^2. Here, \\otimes is the Kronecker product induced by the tensor product of linear maps. If you are lazy, you can skip the sanity check as we are going to prove this using ZX-calculus next anyways which will be much simpler and more elegant, especially for the general case.\nNow we go to n=3 and enumerate B^3. Here we have the set\nB^3 = \\left\\{ 000, 001, 010, 100, 101 \\right\\},\nwhere it‚Äôs missing the binary sequences 110 and 111 that has two and three consecutive ones respectively. As before, think about state outcomes. It is slightly complicated as you now have 2^3 potential outcomes. However, if you think inductively, there is a pattern here. In this case, we now want\n\nqubit 1 to be in equal superposition;\nqubit 2 to be in equal superposition only if qubit 1 is in the state \\ket {0}; and\nqubit 3 to be in equal superposition only if qubit 2 is in the state \\ket {0}.\n\nIn this case, we end up with outcomes exactly as in B^3. Observe that the control on qubit 3 is dependent only on the measurement of qubit 2. This means that we can reuse the same principle we had in the case of n=2. In fact, we can simply extend the previous routine by introducing an X_{\\pi/2} rotation gate on qubit 3 and a single CX_{\\pi/2} controlled rotation gate which conditions on qubit 2.\nFurther observe that qubit 2 is dependent only on the measurement of qubit 1. Thus going from the n=2 case to n=3 is simply extending qubit 3 with 2 new rotation gates. It should be clear that we can reuse this same principle to go from n=3 to n=4 and so on; so we conjecture the following:\n\nConjecture: For n &gt; 1, B^n can be generated by an n-bit quantum routine that performs X_{\\pi/2} rotation gates to all qubits n and then applying n-1 controlled rotation gates CX_{\\pi/2} such that the jth controlled rotation gate conditions on qubit j-1 for j &gt; 2.\n\nSo how do we go about proving this conjecture? Well, it‚Äôs time to turn to ZX-calculus."
  },
  {
    "objectID": "posts/2024-08-04-quantum-fibonacci/index.html#fibonacci-in-the-zx-calculus",
    "href": "posts/2024-08-04-quantum-fibonacci/index.html#fibonacci-in-the-zx-calculus",
    "title": "A benchmark for testing quantum computers with Clifford+T hardware",
    "section": "Fibonacci in the ZX-calculus",
    "text": "Fibonacci in the ZX-calculus\nA short note before we proceed. It has been incredibly hard to embed tikz files (which I originally wrote all my ZX diagrams in) into a HTML format, so I will be taking screenshots from my original solution for my sanity ‚Äì I‚Äôve already wasted 3 hours trying, please believe me. I hate low quality screenshots as well so if you have an elegant solution on how to do this, please let me know.\nI claim that the following ZXFibo algorithm is a working n-bit quantum routine that generates B^n for n &gt; 1, and hence the Fibonacci sequence F_n.\n\nAlgorithm. ZXFibo\n\nGiven. An integer n &gt; 1, for the Fibonacci number F_n to be computed.\nPerform.\n\n\n\n\n\nwhere the process \\widehat{f_n} is given by\n\n\n\n\n\nwhere\n\n\n\n\n\nis called the y-box (since it topologically looks like a Y, sorry I‚Äôm clearly not creative with my naming).\nMeasuring this circuit yields an outcome of 0 up to a number for Z-measurement outcomes \\ket {k_1 \\ldots k_n} with consecutive ones and not 0 (up to a number) otherwise.\n\nIt shouldn‚Äôt be too difficult to see that the process \\widehat{f_n} given above is indeed equivalent to spamming X_{\\pi/2} rotations and putting a control CX_{\\pi/2} gate on each qubit. But if you‚Äôre not convinced or your ZX-calculus is rusty, I‚Äôll probably do it as an appendix at some point. What‚Äôs important, however, is to recognize that the process only uses Clifford+T gates.\nNow, let‚Äôs prove that the ZXFibo algorithm is correct. It‚Äôs a counting algorithm, so naturally we prove by induction on n. But before we start, let‚Äôs recall some of the spider rules in ZX-calculus. Core to this proof are the copy rule (c), commute rule (\\pi) and the eliminate rule (e) given below:\n\n\n\n\n\nThe commute rule is especially used when k = 0. We can also combine the commute rule, the eliminate rule and phase spider fusion to arrive at what I like to call the (\\alpha \\pi) rule:\n\n\n\n\n\nThe (\\alpha \\pi) rule is an important one and we will be using it several times in our correctness proof of ZXFibo. With these rules refreshed, we are ready to prove the base case.\nBase case. For n=2, just using definitions of \\widehat{f_2} and the y-box \\widehat{y}, and using phase spider fusion, we get\n\n\n\n\n\nWe can then look at individual measurement outcomes. For k, \\ell \\in \\{0, 1\\} we have\n\n\n\n\n\nThen using spider fusion, the colour change rule and the copy rule, we end up with\n\n\n\n\n\nWe can now apply the (\\alpha \\pi) rule to end up with\n\n\n\n\n\nFrom here, we can evaluate each pair (k, \\ell) separately. Recall what these spiders actually mean ‚Äì the representation of an undoubled phase state \\alpha \\in \\mathbb{C}^2 is given by the matrix\n\n\n\n\n\nwhere the phase effect version is simply attained by taking the adjoint of the phase state matrix. This implies that\n\n\n\n\n\nConsequently, the undoubled number\n\n\n\n\n\ncan be tabulated into the following table of numbers and their corresponding complex amplitudes\n\n\n\n\n\nIn particular the measurement outcome pair (k, \\ell) = (1, 1) attains a zero complex amplitude. Moreover, where k and \\ell are not both 1, the amplitude is nonzero. This is exactly what we want and so completes the base case. In fact, you can actually get the actual probabilities pretty easily from here, but we leave it as an exercise for the reader.\n\nExercise. Show that the actual probabilities for each measurement outcome are tabulated as follows:  Hint: look at the complex amplitudes of the doubled version.\n\nInductive step. Now suppose that the ZXFibo algorithm is correct for all 2 \\leq n &lt; N + 1. Then for n = N+1, we can look at individual measurement outcomes k_1, \\ldots, k_{N+1} \\in \\{0, 1\\} to have\n\n\n\n\n\nWe can apply the copy rule on the N-th measurement so that we have\n\n\n\n\n\nThe inductive hypothesis takes care of the evaluated \\widehat{f_N}, so we know that it will not be 0 up to a number for Z-measurement outcomes \\ket {k_1 \\ldots k_N} with no consecutive ones. So now we focus on the rightmost y-box. To complete the inductive step, we just need to prove that if k_N = k_{N+1} = 1, then the y-box evaluates to 0, and not 0 otherwise.\nUsing the y-box definition, applying phase spider fusion and using the colour change rule, we have\n\n\n\n\n\nWe can then apply the eliminate rule and the (\\alpha \\pi) rule to further obtain\n\n\n\n\n\nThis number is exactly what we had before with \\ell = k_{N+1} and k = k_N. Thus by using the same logic as we did before, we can conclude that the pair (k_N, k_{N+1}) = (1, 1) which gives the only remaining measurement outcome with consecutive ones will evaluate the whole diagram to 0 (up to a number) and not 0 otherwise. This completes the inductive step and the correctness proof of the ZXFibo algorithm. As a bonus, we have also implicitly proven that the algorithm terminates for all qubits n &gt; 1. \\blacksquare"
  },
  {
    "objectID": "posts/2024-08-04-quantum-fibonacci/index.html#zxfibo-on-an-ibm-quantum-computer",
    "href": "posts/2024-08-04-quantum-fibonacci/index.html#zxfibo-on-an-ibm-quantum-computer",
    "title": "A benchmark for testing quantum computers with Clifford+T hardware",
    "section": "ZXFibo on an IBM quantum computer",
    "text": "ZXFibo on an IBM quantum computer\nNow that we have proven the correctness of the ZXFibo algorithm, let‚Äôs see how it performs on an IBM quantum computer simulator. We will implement ZXFibo using the IBM Qiskit library (Javadi-Abhari et al. 2024) and the PyZX library (Kissinger and Wetering 2020) which we import below.\n\n\nPyZX and IBM Qiskit base imports\nimport math\nfrom fractions import Fraction\n\nimport pyzx as zx\nfrom pyzx import print_matrix\nfrom pyzx.basicrules import *\nzx.settings.drawing_backend = 'matplotlib'\n\nimport qiskit\nfrom qiskit.test.mock import FakeAthens\nfrom qiskit import QuantumCircuit, Aer, IBMQ, execute\nfrom qiskit.compiler import assemble\nfrom qiskit.tools.monitor import job_monitor\n\nimport matplotlib.pyplot as plt\n\n\nWe can then draw ZXFibo using PyZX as follows.\n\ndef add_cx_alpha_gate(\n    circuit: zx.Circuit, alpha: Fraction | int, control: int, target: int\n) -&gt; zx.Circuit:\n    circuit.add_gate(\"HAD\", target)\n    circuit.add_gate(\"ZPhase\", control, phase=alpha * Fraction(1, 2))\n    circuit.add_gate(\"ZPhase\", target, phase=alpha * Fraction(1, 2))\n    circuit.add_gate(\"CNOT\", control, target)\n    circuit.add_gate(\"ZPhase\", target, phase=alpha * Fraction(-1, 2))\n    circuit.add_gate(\"CNOT\", control, target)\n    circuit.add_gate(\"HAD\", target)\n    return circuit\n\n\ndef zxfibo(n: int, graph: bool = False) -&gt; zx.Circuit:\n    circ = zx.Circuit(n)\n    # For each qubit, add an X(\\pi/2) gate.\n    for i in range(n):\n        circ.add_gate(\"XPhase\", i, Fraction(1, 2))\n    # For each qubit n &gt; 1, add a controlled X(\\pi/2) gate.\n    for i in range(n - 1):\n        add_cx_alpha_gate(circ, Fraction(-1, 2), i, i + 1)\n    if graph:\n        return circ.to_graph()\n    return circ\n\nGiven a PyZX circuit, we can convert it into a Qiskit circuit using the following function (I believe this was Aleks‚Äôs code).\n\ndef pyzx_to_qiskit(circ: zx.Circuit) -&gt; qiskit.QuantumCircuit:\n    # converts all gates to CNOT, CZ, HAD, ZPhase, and XPhase\n    circ = circ.to_basic_gates()\n    q = circ.qubits\n    ibm_circ = QuantumCircuit(q, q)\n    for g in circ.gates:\n        if isinstance(g, zx.gates.CNOT): ibm_circ.cnot(g.control, g.target)\n        elif isinstance(g, zx.gates.CZ): ibm_circ.cz(g.control, g.target)\n        elif isinstance(g, zx.gates.HAD): ibm_circ.h(g.target)\n        elif isinstance(g, zx.gates.ZPhase): ibm_circ.rz(math.pi * g.phase, g.target)\n        elif isinstance(g, zx.gates.XPhase): ibm_circ.rx(math.pi * g.phase, g.target)\n    \n    # measure everything\n    ibm_circ.measure(range(q), range(q))\n    return ibm_circ\n\nFor the base case n=2 of ZXFibo, we can then obtain its ZX-diagram and Qiskit quantum circuit by running:\nzxfibo2_pyzx = zxfibo(2)  # PyZX\nzxfibo2_ibm_circ = pyzx_to_qiskit(zxfibo2_pyzx)  # Qiskit\n\n\n\n\n\n\n\n\n\n\n\n(a) PyZX\n\n\n\n\n\n\n\n\n\n\n\n(b) Qiskit\n\n\n\n\n\n\n\nFigure¬†1: ZXFibo for n=2 in its PyZX and Qiskit circuit representation.\n\n\n\nUsing the Qiskit API, we can then run ZXFibo using an IBM backend. To not spend any money, I will be running the algorithm on a non-noisy simulator where we can get ideal counts (suppressing probability 0 events) and on a noisy simulator. For the noisy simulator, I will be using the the FakeAthens backend, which is a 5 qubit fake backend that mimics the IBM Athens device.\n\ntry:\n    IBMQ.load_account()\n    provider = IBMQ.get_provider(hub=\"ibm-q\", group=\"open\", project=\"main\")\n    backend = provider.get_backend('ibmq_qasm_simulator')\nexcept:\n    print(\"Error:\", sys.exc_info()[1])\n    print(\"Setting backend to qasm_simulator\")\n    backend = FakeAthens()\n\nWe then run the ZXFibo algorithm for 1000 shots.\n\ndef run(\n    circ: qiskit.QuantumCircuit, backend: qiskit.providers.Backend, shots: int\n) -&gt; dict[str, int]:\n    job = execute(circ, backend, shots=shots)\n    result = job.result()\n    counts = result.get_counts()\n    return counts\n\ncounts = run(zxfibo2_ibm_circ, backend=backend, shots=1000)\nqiskit.visualization.plot_histogram(counts)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Ideal counts\n\n\n\n\n\n\n\n\n\n\n\n(b) Counts on a noisy simulator\n\n\n\n\n\n\n\nFigure¬†2: Simulation results for ZXFibo with n=2 using the IBM backend with 1000 shots.\n\n\n\nFrom the simulation results for n=2, we can see that the algorithm works as expected. In fact, it works too well. On the non-noisy simulator, the histogram depicts exactly the probabilities of 0.25, 0.25, 0.5 for the pairs (0, 0), (0, 1) and (1, 0) respectively. On the noisy simulator otoh, we attain a similar distribution but with ~0.5% noise in the (1, 1) measurement outcome which is tiny relative to the minimum probability for the measurement outcomes with non-consecutive ones of 25%. Setting a 5% threshold in post-processing discards the 0.5% noise and gives us the correct distribution and yields B^2. We then obtain F_2 as |B^2|.\nWe repeat the experiment by running 1000 shots of the ZXFibo algorithm but now with n=3.\n\n\n\n\n\n\n\n\n\n\n\n(a) Ideal counts\n\n\n\n\n\n\n\n\n\n\n\n(b) Counts on a noisy simulator\n\n\n\n\n\n\n\nFigure¬†3: Simulation results for ZXFibo with n=3 using the IBM backend with 1000 shots.\n\n\n\nWe see again that the ZXFibo algorithm works extremely well! The noisy measurement outcomes (i.e.¬†with consecutive ones) accounts for only about 1% of the total probabilities with a single sequence attaining a maximum of only 0.7%. This number is significantly tiny compared to the minimum probability for elements in B^3 of 12.5%. We can again set a 5% threshold in post-processing to get the correct probability distribution and obtain F_3.\nLet‚Äôs now fire 1000 shots of ZXFibo with n=5 to compute the Fibonacci number F_5.\n\n\n\n\n\n\n\n\n\n\n\n(a) Ideal counts\n\n\n\n\n\n\n\n\n\n\n\n(b) Counts on a noisy simulator\n\n\n\n\n\n\n\nFigure¬†4: Simulation results for ZXFibo with n=5 using the IBM backend with 1000 shots.\n\n\n\nAgain, ZXFibo works really well where the noisy measurement outcomes attains less than 1% in probability on average. But we can now start to see an impending doom‚Ä¶ the minimum probability for elements in B^5 is now 3.9%. This is still significantly larger than the maximum attained by the noisy measurement outcome of 0.7% but we can no longer set a 5% threshold in post-processing. Rather, we need to choose a safer threshold, say, at about 2.5% to discard the noisy outcomes and obtain F_5 with confidence. This phenomenon can seem problematic as we increase n so let‚Äôs discuss a bit more about it.\n\nAn impending doom\nBecause this is a heuristic approach, we have no reason to conclude that the maximum attained by the noisy measurement outcomes will exceed beyond 1% as we have seen this is not the case even for increasing n = 2, 3, 5. The only way to verify this is to run more experiments with higher number of qubits. However, the doom pattern that we can clearly see is that as n increases, the minimum probability for elements in B^n decreases. This means that the threshold for discarding the noisy measurement outcomes will have to be set lower and lower, and might eventually reach a point where the threshold is lower than the maximum attained by the noisy measurement outcomes. What is this breaking point though?\nFix n &gt; 1 and define p_{\\mathrm{min}} be the expected probability for the element B^n with lowest probability (relative to other elements in B^n). Assume an error of \\varepsilon &gt; 0 on p_{\\mathrm{min}}. Suppose the error arising from the noisy measurement outcomes is \\delta \\approx 0. Then a reasonable post-processing probability threshold \\tau for discarding noisy outcomes should satisfy \\delta &lt; \\tau &lt; p_{\\mathrm{min}} - \\varepsilon. The problem is that the Fibonacci sequence F_n behaves like an exponential function, which equivalently means that the number of elements in B^n increases exponentially with n. This implies that p_{\\mathrm{min}} will decrease exponentially fast as n increases! Obviously, if \\delta &lt; p_{\\mathrm{min}} - \\varepsilon, then we can always take the midpoint\n \\tau = \\frac{p_{\\mathrm{min}} - \\varepsilon + \\delta}{2} \nto be the count threshold. However, the impending doom is when p_{\\mathrm{min}} - \\varepsilon \\approx \\delta for some sufficiently large N. In this case, there does not exist a stable count threshold \\tau for any n &gt; N + 1 as for n = N, we have\n\\tau = \\frac{p_{\\mathrm{min}} - \\varepsilon + \\delta}{2} \\approx \\frac{2 \\delta}{2} = \\delta.\nSo if we are serious in counting large Fibonacci numbers, we ought to think of a better algorithm and this opens room for further research. For our purpose, however, we just want a benchmark for running Clifford+T hardware. This heuristic is excellent in doing exactly that, especially as per our discussion, for benchmarking how noisy a quantum computer is by looking at the noisy measurement outcomes. In fact, one can use the probability threshold \\tau above as a metric to measure the noise levels."
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html",
    "href": "posts/2023-03-05-clash-is-prime/index.html",
    "title": "Clash Series: Checking Number is Prime",
    "section": "",
    "text": "This is the first of hopefully many more posts on Clash of Code tricks I learn along the way. I call this the Clash Series and in today‚Äôs series, we look at how to write an efficient script to check for prime numbers, and how to write it fast python for those fastest mode clashes.\nEquivalently, we say p is prime if p &gt; 1 and whenever it decomposes into p = ab, then either a=1, b=p or a=p, b=1. Thus, if p is not prime, then there is a decomposition of p into a product of positive integers a, b that are not necessarily 1 or p; hence, the name composite.\nThe first few prime numbers are 2, 3, 5, 7, 11 and so on, while the first few composite numbers are 1, 4, 6, 8, 9 and so on.\nNow here is what we are interested in."
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html#the-problem",
    "href": "posts/2023-03-05-clash-is-prime/index.html#the-problem",
    "title": "Clash Series: Checking Number is Prime",
    "section": "The problem",
    "text": "The problem\n\nproblem statement\n\nGiven an integer n \\in \\mathbb{Z}, how to enumerate all prime numbers less than n?\n\n\nA naive enumeration algorithm can be achieved in the following way: For any given integer n,\n\nIf n &lt; 2, then n cannot be prime.\nIf n = 2, then n is prime.\nLoop over all integers k \\in [3, n). If there exists k such that k \\equiv 0 \\bmod n, then n is not prime. Otherwise n is prime.\n\nA python code for this naive implementation is as follows.\n\ndef is_prime_naive(n: int) -&gt; bool:\n    # 1 and any negative integer are not prime.\n    if n &lt; 2:\n        return False\n    # 2 is prime.\n    if n == 2:\n        return True\n    for k in range(2, n):\n        # If n is divisible by k for any k &lt; n, n is not prime.\n        if n % k == 0:\n            return False\n    return True\n\nThe time complexity for this algorithm is \\mathcal{O}(n) where n is the given integer. This is already good in most cases, but can we do better?\nWell any integer n &gt; 2 is always divisible by 2 if it is even, and therefore cannot be prime. So we can add a step to check if n is even or not before doing the looping step. As a result, our algorithm running time is cut in half as we now only have to loop over only odd numbers less than n.\n\ndef is_prime_better(n: int) -&gt; bool:\n    # 1 and any negative integer are not prime.\n    if n &lt; 2:\n        return False\n    # 2 is prime.\n    if n == 2:\n        return True\n    # NEW: If n is even and n is not 2, then it is not prime.\n    if n % 2 == 0:\n        return False\n    for k in range(3, n, 2):\n        # NEW: If n is divisible by odd k for any k &lt; n, n is not prime.\n        if n % k == 0:\n            return False\n    return True\n\nThe problem, however, is that the theoretical time complexity of this algorithm is still \\mathcal{O}(n/2) = \\mathcal{O}(n). So good for small n but still problematic for super large n."
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html#efficient-algorithm-sqrt-trick",
    "href": "posts/2023-03-05-clash-is-prime/index.html#efficient-algorithm-sqrt-trick",
    "title": "Clash Series: Checking Number is Prime",
    "section": "Efficient algorithm (sqrt trick)",
    "text": "Efficient algorithm (sqrt trick)\nSo what can we do to make this code faster? Enter‚Ä¶ the square root (sqrt) trick.\nFirst, without loss of generality, let‚Äôs assume that n is a positive integer since if n &lt; 2, then it‚Äôs taken care by the n &lt; 2 check. We now make the following claim.\n\nLemma 1 (Sqrt Decomposition) Let n &gt; 1 be a positive integer. If n = ab is a composite integer such that 0 &lt; a \\leqslant b, then a \\leqslant \\sqrt{n} \\leqslant b.\n\n\nProof. First observe that n = ab is a perfect square if and only if a = b = \\sqrt{n}, in which case we are done.\nSo suppose n = ab is not a perfect square. Then a and b cannot be \\sqrt{n} and moreover a &lt; b. We now claim that a &lt; \\sqrt{n} &lt; b. Suppose a &lt; \\sqrt{n} but also b &lt; \\sqrt{n}, then ab &lt; n which is a contradiction. Similarly, if both a &gt; \\sqrt{n} and b &gt; \\sqrt{n}, then ab &gt; n, also a contradiction. Thus, since a &lt; b, it has to be that a &lt; \\sqrt{n} &lt; b. \\blacksquare\n\nSo what‚Äôs the point of Lemma¬†1? Well the main takeaway is the following: if the number n is composite, then we will at least find one of its divisors before or equal its integer square root \\lfloor \\sqrt{n} \\rfloor.\n\nTheorem 1 Let n be a composite positive integer. Then there exists a prime divisor p of n such that p \\leqslant \\lfloor \\sqrt{n} \\rfloor.\n\n\nProof. By Lemma Lemma¬†1, we know that if n = kb is a composite integer with 0 &lt; k \\leqslant b, then k \\leqslant \\sqrt{n}, with equality iff n is a perfect square. Equivalently, this means that k \\leqslant \\lfloor \\sqrt{n} \\rfloor. If k is prime, then we are done. Otherwise, there exists a prime p that divides k (hence, divides n) that would also satisfy p \\leqslant \\lfloor \\sqrt{n} \\rfloor as desired. \\blacksquare\n\n\nNote: for a linear traversal check (like we are doing, starting from 3, 4, 5, ‚Ä¶), we know that we will encounter any prime divisor p of any composite divisor k of n first. So the last part of the proof is unnecessary for our need, but we put it there for completeness.\n\nSo the idea of using sqrt decomposition speeds up our algorithm tremendously, especially for large n. In fact, even for small n like n=1080, its integer square root is 32 which is already two order of magnitudes lower.\nWe now add the sqrt trick to our python implementation.\n\ndef is_prime_sqrt_trick(n: int) -&gt; bool:\n    # 1 and any negative integer are not prime.\n    if n &lt; 2:\n        return False\n    # 2 is prime.\n    if n == 2:\n        return True\n    # If n is even and n is not 2, then it is not prime.\n    if n % 2 == 0:\n        return False\n    # NEW: loop until int(sqrt(n)) + 1.\n    # The + 1 is to handle if n is perfect square.\n    for k in range(3, int(n**.5)+1, 2):\n        # If n is divisible by odd k for any k &lt; n, n is not prime.\n        if n % k == 0:\n            return False\n    return True\n\nThe sqrt trick improves time complexity from \\mathcal{O}(n) to \\mathcal{O}(\\sqrt{n}) which is massive! To give you an idea of this speedup, we will run the naive and sqrt algorithms to check if n = 2{,}147{,}462{,}143 is prime. Note that this is a prime number on the order of 2^{31}.\n\nThe sqrt trick improves time complexity from \\mathcal{O}(n) to \\mathcal{O}(\\sqrt{n}) which is massive!\n\n\n\nspeedup comparison ‚ö°Ô∏è\n\nimport time\n\nn = 2_147_462_143\n\nfor f in [is_prime_naive, is_prime_sqrt_trick]:\n    start = time.time()\n    f(n)\n    print(f\"Took total of {(time.time()-start):.10f} seconds using {f.__name__}\")\n\nTook total of 118.3165051937 seconds using is_prime_naive\nTook total of 0.0010521412 seconds using is_prime_sqrt_trick"
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html#efficient-writing",
    "href": "posts/2023-03-05-clash-is-prime/index.html#efficient-writing",
    "title": "Clash Series: Checking Number is Prime",
    "section": "Efficient writing",
    "text": "Efficient writing\nSo the impementation is super efficient \\mathcal{O}(\\sqrt{n}), but how do we make writing the code efficient for something like Clash of Code?\nWell first note that when writing in a clash, you don‚Äôt care about the 80 char PEP format or readability. So it‚Äôs finally time to write one-liner 100 chars fugly code.\nThe trick I use is to use all to replace the for-loop and replace the False checks with True statement by ‚Äúbundling‚Äù their evaluation using and. This gives the following one-liner.\n\ndef is_prime_sqrt_short(n: int) -&gt; bool:\n    return n == 2 or (n &gt; 2 and n % 2 != 0 and all(n % k != 0 for k in range(3, int(n**.5)+1, 2)))\n\nIn fact, since the modulo operator % in python returns a positive integer, we can save writing one character for each != by writing &gt; instead.\n\ndef is_prime_sqrt_super_short(n: int) -&gt; bool:\n    return n == 2 or (n &gt; 2 and n % 2 &gt; 0 and all(n % k &gt; 0 for k in range(3, int(n**.5)+1, 2)))"
  },
  {
    "objectID": "posts/2021-09-07-preparing-image-dataset-in-pytorch/index.html",
    "href": "posts/2021-09-07-preparing-image-dataset-in-pytorch/index.html",
    "title": "Preparing Image Dataset for Neural Networks in PyTorch",
    "section": "",
    "text": "Preparing and handling data is a core step of any machine learning pipeline. Today, we will look at handling data when the data is an image (or image-like) in PyTorch.\n\nPyTorch and Torchvision\nPyTorch provides us with the amazing torchvision package for dealing with common image transformations such as normalizing, scaling, random flipping and converting arrays to PyTorch tensors. It also provides us with common computer vision datasets such as MNIST, Fashion MNIST and CIFAR-10. In this post, we will focus on preparing the Fashion MNIST dataset.\nTo begin, we start by importing torch and torchvision.\nimport torch\nfrom torchvision import datasets, transforms\n\nNote that we will refer to the submodule datasets and transforms directly from now on (i.e.¬†we will not emphasize that it‚Äôs part of torchvision).\n\n\n\nüëú Fashion MNIST dataset and composing transformations\nThe Fashion MNIST dataset by Zalando Research is a famous benchmark dataset in computer vision, perhaps second only to MNIST. It is a dataset containing 60,000 training examples and 10,000 test examples where each example is a 28 x 28 grayscale image. Since the images are in grayscale, they only have a single channel. If the image is in RGB format instead (e.g.¬†if we are dealing with CIFAR-10), then it has 3 channels one for each red, green and blue.\nAs mentioned before, the Fashion MNIST dataset is already part of PyTorch. However, this does not mean that the dataset is already in perfect shape to pass into a PyTorch neural net. We would need to apply some (image) transformations to the dataset upon fetching. For brevity, we will apply only two simple transformations:\n\nConverting the images to a PyTorch tensor ‚Äì by using transforms.ToTensor().\nNormalize the channel of the resulting tensor ‚Äì by using transforms.Normalize().\n\nWhy do we do these transformations?\n\nSince we will be working with neural nets in PyTorch, it is only natural that we want the image to be a PyTorch tensor. This enables the PyTorch API to interact properly with our dataset.\nNormalization is important to ensure that our neural nets learn better. For an idea of how normalization works, check out this discussion.\n\nWe can then compose these transformations using transforms.Compose() as below.\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5), std=(0.5)),\n])\n\nNote that the mean and standard deviation value of 0.5 should be calculated from the training set in advance. Here, we just assume that mean = std = 0.5 for simplicity.\n\n\n\nüíæ From dataset to DataLoader\nThe next step is to finally fetch the dataset, passing our transform above as an argument. The FashionMNIST dataset can be accessed via datasets.FashionMNIST, no surprise there. We can then fetch the 60,000 training examples using the following code:\ntrainset = datasets.FashionMNIST(root='./data',\n                                 download=True,\n                                 train=True,\n                                 transform=transform)\nLet us break down what each argument means.\n\nroot specifies the location of the dataset. Here, we specify that it should be in the directory './data'.\ndownload is a boolean flag which determines if we want to download the dataset if the data is not already in root.\ntrain is another boolean flag which determines if we want the training set. Getting the test set is as simple as passing train=False.\ntransform is the transformations we would like to apply to the dataset upon fetching.\n\nOnce we have our transformed train set, we can now start training neural nets on this data using PyTorch. However, let us take a second to think about the following:\n\nWhat if we want to work with minibatches of this dataset instead of single examples? This is definitely a need when the dataset is too large like ours to be trained entirely.\nWe would also want to reshuffle this dataset on each epoch so that our neural net generalizes better.\nIf the data is big, we might even want to load the data in parallel using multiprocessing workers to retrieve our data faster.\n\nThis is where PyTorch‚Äôs so-called DataLoader comes in. It is an iterable that provides all the above features out of the box on top of providing a smooth API for working with data!\nTo use the DataLoader object on our train set, we simply access torch.utils.data.DataLoader and feed trainset into it.\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n                                          shuffle=True, num_workers=0)\nHere, we have decided to use a batch_size of 64 images, which are sampled randomly on each epoch due to shuffle=True. We also put num_workers=0 meaning we are not loading the data in parallel.\nWe can fetch the Fashion MNIST test dataset in a similar fashion. The only difference is that we now have train=False.\ntestset = datasets.FashionMNIST(root='./data',\n                                download=True,\n                                train=False,\n                                transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64,\n                                         shuffle=True, num_workers=0)\n\n\nüïµÔ∏è Inspecting the dataset in DataLoader form\nOnce we have the dataset in DataLoader form, we can start inspecting our dataset. For example, we can get the shapes of our trainset.\nprint(\"Train shape:\", trainloader.dataset.data.shape)\nprint(\"Test shape:\", testloader.dataset.data.shape)\nTrain shape: torch.Size([60000, 28, 28])\nTest shape: torch.Size([10000, 28, 28])\nWe can also get the minibatch size as specified when initializing the DataLoader.\nprint(\"Train batch size:\", trainloader.batch_size)\nprint(\"Test batch size:\", testloader.batch_size)\nTrain batch size: 64\nTest batch size: 64\nFor a more advanced inspection, we can even look at the sampler and the collate function used in the DataLoader. The sampler determines how the data is shuffled and the collate function specifies how the data is batched.\nprint(\"Sampler:\", trainloader.sampler)\nprint(\"Collate function:\", trainloader.collate_fn)\nSampler: &lt;torch.utils.data.sampler.RandomSampler object at 0x7fcc02b23b90&gt;\nCollate function: &lt;function default_collate at 0x7fcc05c9a710&gt;\nSince we did not pass anything during initialization, we get the default RandomSampler object for the sampler and the default default_collate collate function as expected.\nAs we are dealing with an image dataset, it is a shame if we are not plotting anything during inspection. Let‚Äôs plot the first image from the first batch in trainloader.\nimages, labels = next(iter(trainloader))  # Gets a batch of 64 images in the training set\nfirst_image = images[0]  # Get the first image out of the 64 images.\n\nimport matplotlib.pyplot as plt\nplt.imshow(first_image.numpy().squeeze(), cmap='Greys_r')\nplt.show()\nHere, we get a t-shirt which is expected since we are dealing with a fashion dataset after all. If you run the exact code, you might get a different output since the dataset is shuffled and I did not specify a seed.\n\n\n\nPlot of image from Fashion MNIST\n\n\nFor the simplified version of this post in jupyter notebook format: notebook version."
  },
  {
    "objectID": "posts/drafts/2024-06-02-fraud-detection-primer-01/index.html",
    "href": "posts/drafts/2024-06-02-fraud-detection-primer-01/index.html",
    "title": "Basics of payments fraud detection",
    "section": "",
    "text": "Whenever I hear someone talk about fraud detection, my brain would go ‚Äúthey must be talking about credit card fraud right?‚Äù. Well, not sure why I‚Äôm wired that way, but I just learnt recently that this space covers way more than credit cards and bank transactions. So out of curiosity, I went straight to learning. This post aims to summarize what I‚Äôve learnt.\nSo here‚Äôs an important big disclaimer: do not take what I wrote here at face value because I‚Äôm writing from a complete fraud detection amateur‚Äôs point of view. Ranting moment: when I was writing my master‚Äôs thesis, I was trying to find a way to make my three-page introduction to variational autoencoders shorter. So I went to the internet and I was appalled at how many wrong articles that exist on VAEs. I do understand that writing is part of the learning journey‚Äìjust as I‚Äôm doing now‚Äìbut please, for the love of God put a disclaimer that you have minimal understanding of what you‚Äôre writing. And scary to think how LLMs today are essentially being trained on loads of incorrect information!\nWith this disclaimer out of the way, let‚Äôs get cracking."
  },
  {
    "objectID": "posts/drafts/2024-06-02-fraud-detection-primer-01/index.html#examples-of-fraud",
    "href": "posts/drafts/2024-06-02-fraud-detection-primer-01/index.html#examples-of-fraud",
    "title": "Basics of payments fraud detection",
    "section": "Examples of fraud",
    "text": "Examples of fraud\nFraud can be found almost anywhere. The most well-known example is, perhaps, payments fraud which captures cases of stolen credit cards and a variety of identity theft attacks. It also captures the case of chargeback fraud where people make a dispute on a legitimate purchase. This is very similar to insurance fraud.\nThe problem with fraud is that they‚Äôre not very common, which from a data science point of view is not great as that means less data to work with. And they‚Äôre quite hidden among the non-fraudulent cases. Some can be flagged as an anomaly sure, but figuring out whether this anomaly is a customer being irrational or is actual fraud is typically hard. This made me think about the anecdote where Shaq spent 70,000 dollars at Walmart in a single night and got called by Amex because Amex thought his card was stolen. To make things more complex, fraudulent activities are actively changing where fraudsters know they are being monitored, so they change their pattern often making it even harder to detect. And frauds are done in an organized way where you can have multiple accounts colluding into a single fraud."
  }
]