[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Undergraduate stuff @ KCL\n\nMy dissertation – Diophantine Equations, supervised by Prof. Payman Kassaei.\n\nI wrote my undergrad math notes in \\(\\LaTeX\\) to help me understand better. These notes can replace lecture notes and are around 100 pages long each. They can be accessed via this repository. Notable ones include:\n\n6CCM350A Rings and Modules – based on lectures by Prof. Nicholas Shepherd-Barron, Fall 2020.\n6CCM359A Numerical and Computational Methods – based on lectures by Prof. Benjamin Doyon, Fall 2020.\n5CCM224A Introduction to Number Theory – based on lectures by Dr. James Newton, Fall 2019.\n5CCM226A Metric Spaces and Topology – based on lectures by Dr. Jerry Buckley, Fall 2019.\n\n\n\nPostgraduate stuff @ Oxford\n\nMy dissertation – “Enhancing VAE-learning on spatial priors using graph convolutional networks”, supervised by Seth Flaxman, Swapnil Mishra and Elizaveta Semenova.\nImplemented “Variational Graph Auto-Encoders” (Kipf and Welling, 2016) in JAX as part of my dissertation. Source code can be found in this repo.\nReproduced and extended “Inference Suboptimality in Variational Autoencoders” (Cremer et. al, 2018) using JAX. Built together with Basim Khajwal, Snehal Raj and Vasileios Ntogram. Source code can be found in this repo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "random technical thoughts.",
    "section": "",
    "text": "Migrant pension mathematics\n\n\n\n\n\n\nmathematical modelling\n\n\npersonal finance\n\n\npension\n\n\n\nTo pension or not to pension as a migrant—this is a common question I get asked. So, let’s explore both scenarios at the extremes. In particular, I’ll examine the extreme case of contributing to a pension, receiving the company match, but then making an early withdrawal. This comes at a cost—a penalty—so what would that look like?\n\n\n\n\n\nFeb 22, 2025\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nIs Costco fuel cheaper overall given the membership premium?\n\n\n\n\n\n\nmathematical modelling\n\n\npersonal finance\n\n\npricing\n\n\ncar talk\n\n\n\n\n\n\n\n\n\nOct 18, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nHow to ask Amex for a retention offer?\n\n\n\n\n\n\npersonal finance\n\n\namex\n\n\n\n\n\n\n\n\n\nOct 4, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nNaivety and desperation in mathematical modelling can somewhat push you far\n\n\n\n\n\n\nIB\n\n\nhigh school math\n\n\nmathematical modelling\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nPart II: pooling makes convolutional neural net rotation invariant\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: latte art and light/dark roasts\n\n\n\n\n\n\nTIL\n\n\ncoffee\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional neural networks are not rotation invariant… since when?\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nSearching for my favorite song in a deep playlist\n\n\n\n\n\n\ncombinatorics\n\n\nspotify\n\n\nmathematical modelling\n\n\n\n\n\n\n\n\n\nAug 31, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nQuickest intro to random forest\n\n\n\n\n\n\nmachine learning\n\n\nensembling\n\n\n\n\n\n\n\n\n\nAug 13, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nA benchmark for testing quantum computers with Clifford+T hardware\n\n\n\n\n\n\nalgorithms\n\n\nquantum computing\n\n\nzx calculus\n\n\nbenchmarking\n\n\npython\n\n\n\nWe provide a quantum algorithm that can be used to benchmark quantum computers with Clifford+T hardware and provide a proof of the algorithm using ZX-calculus.\n\n\n\n\n\nAug 5, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nA mathematical take on when to take a loan such as ASB Financing\n\n\n\n\n\n\nquant\n\n\ninvestment\n\n\npersonal finance\n\n\nmathematical modelling\n\n\n\nWe look at formalizing the parity between investing with a disposable capital versus with a financed leveraged capital in ASB.\n\n\n\n\n\nApr 22, 2024\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do aws s3 ls s3://bucket/ using boto3 in python?\n\n\n\n\n\n\ncloud development\n\n\naws\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 12, 2023\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nClash Series: Checking Number is Prime\n\n\n\n\n\n\nclash of code\n\n\nalgorithms\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 5, 2022\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nPreparing Image Dataset for Neural Networks in PyTorch\n\n\n\n\n\n\nmachine learning\n\n\ncomputer vision\n\n\npytorch\n\n\ndata preprocessing\n\n\n\n\n\n\n\n\n\nSep 7, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nThe Coding Kata\n\n\n\n\n\n\nsoftware engineering\n\n\nclean coding\n\n\n\n\n\n\n\n\n\nAug 18, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nPlotly.py main theme in Plotly.js\n\n\n\n\n\n\njavascript\n\n\nplotly\n\n\npython\n\n\nweb dev\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nUnit Testing with pytest\n\n\n\n\n\n\npython\n\n\ntesting\n\n\nsoftware engineering\n\n\n\nWhy do we do unit testing and how to do unit testing in python using pytest.\n\n\n\n\n\nMay 3, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nRelational Databases in Python (part I)\n\n\n\n\n\n\ndata science\n\n\npython\n\n\nsql\n\n\nsqlalchemy\n\n\ndatabases\n\n\ndata handling\n\n\n\n\n\n\n\n\n\nApr 30, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nMaking pastel-colored boxes using tcolorbox in LaTeX\n\n\n\n\n\n\nlatex\n\n\nwriting\n\n\n\n\n\n\n\n\n\nApr 7, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nRoot-hunting algorithm: Newton’s method\n\n\n\n\n\n\nnumerical analysis\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\nSalman Faris\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging again\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\nSalman Faris\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-09-18-cnn-rotation-invariant-part-ii/index.html",
    "href": "posts/2024-09-18-cnn-rotation-invariant-part-ii/index.html",
    "title": "Part II: pooling makes convolutional neural net rotation invariant",
    "section": "",
    "text": "In part I, I’ve shown that the convolutional operator is not rotation invariant and argued that if a convolutional neural network is to be made rotation invariant, then the magic happens at the pooling layer. In this part II post, I’ll show why."
  },
  {
    "objectID": "posts/2024-09-18-cnn-rotation-invariant-part-ii/index.html#pooling-as-a-functional-operator",
    "href": "posts/2024-09-18-cnn-rotation-invariant-part-ii/index.html#pooling-as-a-functional-operator",
    "title": "Part II: pooling makes convolutional neural net rotation invariant",
    "section": "Pooling as a functional operator",
    "text": "Pooling as a functional operator\nLet’s first agree on a definition. There are hundreds of ways to define a pooling layer or operator and they converge on the same concept. For the sake of this post, here is mine. A pooling operator is basically an operator that performs statistical aggregation such as a mean or a max over the outputs (of a subset) of a function. So given a function J: \\mathcal{D} \\subseteq \\mathbb{Z}^2 \\to \\mathbb{R} and a subset \\mathcal{P} \\subseteq \\mathcal{D} called a patch, the max pooling is defined as\n\n\\mathrm{MaxPool}_{\\mathcal{P}}(J) = \\max_{x \\in \\mathcal{P}}\\{ J(x) \\}.\n\nThe mean pooling is similarly defined as\n\n\\mathrm{MeanPool}_{\\mathcal{P}}(J) = \\frac{1}{|\\mathcal{P}|} \\sum_{x \\in \\mathcal{P}} J(x).\n\nWhen the patch that we are pooling over is clear, we will drop the subscript and simply write \\mathrm{MaxPool} and \\mathrm{MeanPool}.\nThat’s it, nothing fancy. The essence is that these operators take a patch of data and reduces it to a single value. Of course one can guess that in practice, there will be a finite number of patches \\{ \\mathcal{P}_i \\} of \\mathcal{D} such that they form a partition of \\mathcal{D}; and so there will be \\left| \\{ \\mathcal{P}_i \\} \\right| reductions. In fact, we will define patches of \\mathcal{D} to be exactly that.\n\nIf your math is rusty: a countable subset \\{ A_i \\} of \\mathcal{D} forms a partition of \\mathcal{D} if A_i \\cap A_j = \\varnothing for all i \\neq j and \\bigcup_{i} A_i = \\mathcal{D}.\n\nTo make our argument simpler, let’s just focus on the max pooling operator although what we will discuss will be relevant to any appropriate pooling operator."
  },
  {
    "objectID": "posts/2024-09-18-cnn-rotation-invariant-part-ii/index.html#max-pooling-and-invariance",
    "href": "posts/2024-09-18-cnn-rotation-invariant-part-ii/index.html#max-pooling-and-invariance",
    "title": "Part II: pooling makes convolutional neural net rotation invariant",
    "section": "Max pooling and invariance",
    "text": "Max pooling and invariance\nNow here’s the magic trick right. Define a 3 x 1 domain \\mathcal{D} = \\{ 0, 1, 2, 3, 4\\} \\times \\{ 0 \\} and consider the image J_k: \\mathcal{D} \\to \\mathbb{R} defined as J_k(x) = \\mathbb{1}_{\\{ x = (k, 0) \\}}(x) for a fix k \\in \\{ 0, 1, 2, 3, 4 \\}. If we identify 0 with \\textcolor{black}{\\rule{1.8ex}{1.8ex}} and 1 with \\textcolor{blue}{\\rule{1.8ex}{1.8ex}}, then we have the following image output:\n\nJ_0(x) = \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}},  \n\\; J_1(x) = \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}}\\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}}\\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}},\n\\; J_2(x) = \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{blue}{\\rule{1.8ex}{1.8ex}}\\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}}\\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}},\\\\[10pt]\n\\; J_3(x) = \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}}\\,\\textcolor{blue}{\\rule{1.8ex}{1.8ex}}\\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}},\n\\; J_4(x) = \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}}\\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}}\\,\\textcolor{blue}{\\rule{1.8ex}{1.8ex}}.\n\nIt should be clear that this is a linear translation of the blue pixel here, translating from left to right, although in principal all five J_k are entirely different images. But what happens if we were to apply the max pooling operator over the patch \\mathcal{P} = \\mathcal{D}? Well they result in the same reduction!\n\n\\mathrm{MaxPool}(J_0) = \\mathrm{MaxPool}(J_1) = \\mathrm{MaxPool}(J_2) = \\mathrm{MaxPool}(J_3) = \\mathrm{MaxPool}(J_4) = \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} =1.\n\nIf we consider 2 x 1 patches \\{ \\{i, i+1\\} \\times \\{0\\} \\} and slide max pooling over the patches, we would end up with the following reductions:\n\n\\mathrm{MaxPool}(J_0(x)) = \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}},\n\\; \\mathrm{MaxPool}(J_1(x)) = \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}},\n\\; \\mathrm{MaxPool}(J_2(x)) = \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}},\\\\[10pt]\n\\; \\mathrm{MaxPool}(J_3(x)) = \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}},\n\\; \\mathrm{MaxPool}(J_4(x)) = \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{blue}{\\rule{1.8ex}{1.8ex}}.\n\nThat is, we have\n\n\\mathrm{MaxPool}(J_0) = \\mathrm{MaxPool}(J_1) = \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}};\\\\[10pt]\n\\mathrm{MaxPool}(J_2) = \\mathrm{MaxPool}(J_3) = \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{black}{\\rule{1.8ex}{1.8ex}};\\\\[10pt]\n\\mathrm{MaxPool}(J_4) = \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\, \\textcolor{black}{\\rule{1.8ex}{1.8ex}} \\,\\textcolor{blue}{\\rule{1.8ex}{1.8ex}}.\n\nCan you start seeing how max pooling can help learn translation invariance?\nTo make this mathematically precise is a bit more work and pain, but essentially max pooling gives rise to an equivalence relation on the set \\mathcal{I} of images. So if \\mathcal{I} contains the full translation of a particular image, then they can be made equivalent under the max pooling operator.\nThis means that if we inject a max pooling layer in a convolutional neural net and assuming we have the complete full translation set of an image, the net can learn to recognize the image up to translation. In fact, we don’t have to provide the full set of translation, if we have a layer that kind of shifts the image pixel by pixel, then we can learn the image up to a certain amount of translation. Theoretically, it would be great to augment \\mathcal{I} with the full image translation though.\nAnd it should be clear that this idea extends quite easily to the rotation invariance as well.\n\nSomeone hungrier and smarter can you please formalize the idea of max pooling as an equivalence relation? I am very invested to see what sort of interesting things we can get when looking at the quotient space \\mathcal{I} / \\mathrm{MaxPool} of images modulo the max pooling operator."
  },
  {
    "objectID": "posts/2024-10-17-is-costco-fuel-worth-it/index.html",
    "href": "posts/2024-10-17-is-costco-fuel-worth-it/index.html",
    "title": "Is Costco fuel cheaper overall given the membership premium?",
    "section": "",
    "text": "Today, let’s talk about petrol prices. It is well known that there’s no petrol cheaper than Costco’s with several pence in difference, sometimes up to 5p. The problem is there’s a membership premium to even start purchasing Costco petrol and that Costco tends to be located significantly far away from the city centre.\nSo this begs the question: if we forget the add-on benefit of being able to shop at Costco with the membership, is it worth it to fuel your car at Costco despite the premium and the distance? That is, from the worst case scenario perspective of “I just want to get out, fill my car and go back home”, is it worth it?\nLet’s dive straight into modelling this problem and find the breakeven."
  },
  {
    "objectID": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#modelling-the-problem",
    "href": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#modelling-the-problem",
    "title": "Is Costco fuel cheaper overall given the membership premium?",
    "section": "Modelling the problem",
    "text": "Modelling the problem\nThis is blatantly obvious but remember that getting to the petrol station and back is not free, it costs money on its own. Costco tends to be on the outskirts, so to fill our car there requires a distance d^* that is much further than the distance d to our nearest petrol station (in my case, is an Esso). But remember, the advantage is that the petrol cost p^* at Costco is much cheaper compared to the petrol cost p at our nearest station. Here, d and d^* are measured in miles, and p and p^* are measured in £/litres. Naturally, d and d^* are not measured in the Euclidean distance, but rather measured as the sum of distances on the roads taken (think a path integral).\nNow fix a fuel economy e for the car you are driving. Typically, car makers report this in litres/100km or miles/gallon. I will be thinking of this in terms of litres/miles, and the conversion should be straightforward. With the fuel economy fixed, the total petrol we need to fill to cover the distance alone are\n\n\\begin{align*}\n\\text{Costco:} &\\quad v_{\\mathrm{cover}}^{*} = 2{d^*}e, \\\\\n\\text{Nearest:} &\\quad v_{\\mathrm{cover}} = 2de.\n\\end{align*}\n\nObserve that we slapped a 2 in front since you have to go to the petrol station and back. If you want to be pedantic, this is not really accurate as the route back might be different (shorter/longer) but I think this is a fair enough assumption. After all, I just want an estimation.\nSuppose that I want a volume V petrol when I reach back home after fuelling regardless of which petrol station I go to, then we want to fill in \n\\begin{align*}\n\\text{Costco:} &\\quad V + v_{\\mathrm{cover}}^{*}, \\\\\n\\text{Nearest:} &\\quad V + v_{\\mathrm{cover}},\n\\end{align*}\n\nrespectively at the petrol station. This implies that the total cost for filling in petrol is given by\n\n\\begin{align*}\n\\text{Costco:} &\\quad (V + v_{\\mathrm{cover}}^{*})p^*, \\\\\n\\text{Nearest:} &\\quad (V + v_{\\mathrm{cover}})p,\n\\end{align*}\n\nrespectively. This will be our total cost for the nearest petrol station. For Costco, however, we need to include the membership premium M which, as of 2024, is £33.60 annually. To bake this premium into the fuelling cost, I am going to multiply M with an annual fuelling frequency \\phi. This is essentially the cost of the premium per filling petrol. Now \\phi is not Costco-dependent (although it can be biased by wanting to make the most of the membership) but really user-dependent – it depends on how much you use your car and hence, how much you fill the car tank. For example, if I’m thinking of methodical use, I would put a fuelling frequency of once a week which puts \\phi = 1/52 for the 52 weeks in the year. Adding this premium implies the total cost to be:\n\n\\begin{align*}\n\\text{Costco:} &\\quad C^* = \\phi M + (V + v_{\\mathrm{cover}}^{*})p^*, \\\\\n\\text{Nearest:} &\\quad C = (V + v_{\\mathrm{cover}})p.\n\\end{align*}\n\nWe are almost there, there is one little thing to take care of. Now fuel prices changes all the time, so it’s not really right to think them as constants. So assume p and p^* are sampled from probability distributions \\mathcal{P}(.) and \\mathcal{P}^* respectively.\nTo answer our question, I am going to somehow bound p^*. Well, this is easy.\n\n\n\n\n\n\np^* criterion\n\n\n\nThe total cost is bounded C^* &lt; C iff \np^*\n&lt; \\frac{(V + v_{\\mathrm{cover}}) p - \\phi M}{V+v^*_{\\mathrm{cover}}}\n= \\frac{V + v_{\\mathrm{cover}}}{V+v^*_{\\mathrm{cover}}} p - \\frac{\\phi}{V+v^*_{\\mathrm{cover}}} M\n\n\n\nThe bound itself is useful (e.g. for algorithms), but the interpretation is much more interesting. Since d^* &gt; d, we have (V + v_{\\mathrm{cover}})/(V + v^*_{\\mathrm{cover}}) &lt; 1. The p^* criterion tells us that for Costco to be worth it, it is not enough that p^* &lt; p, but it needs to be less than p discounted by (V + v_{\\mathrm{cover}})/(V + v^*_{\\mathrm{cover}}), further tightened by a fraction of the premium \\phi/(V + v^*_{\\mathrm{cover}}) \\cdot M.\nThe good thing is we know how much it is being tightened by. Let’s first isolate constant-like terms. Since the premium M is beyond our control, we can treat it as a constant. In fact, we can view the distance d^* to the nearest Costco petrol station as fixed since there’s really one Costco station you can go to within the proximity of your house (at least in GB). This is in contrast to the distance d to the nearest petrol station which can be plenty – as we can have plenty of nearest petrol stations to choose from. We can also view the fuel economy e as fixed as it’s not like we change cars every time we fill in the tank. This implies that we can view the term V + v^*_{\\mathrm{cover}} as a constant.\nThus the tightness of the bound is really controlled by the fuelling frequency \\phi. The less time you go and fill your car at Costco, the bigger \\phi is and hence, the bigger the effect of the premium term. This implies a tighter bound! I guess this is sort of intuitive (?) I’m trying really hard to convince myself that this is the case. And note that the multipliers on p and M does not add up to 1, i.e. they’re not “weights” per se, so a bigger premium effect does mean pricier total cost.\nWe can get another interesting interpretation when we look at changes in the prices. Computing partial derivatives gives us \n\\frac{\\partial C^*}{\\partial p^*} = V + v^*_{\\mathrm{cover}}; \\quad \\frac{\\partial C}{\\partial p} = V + v_{\\mathrm{cover}}.\n\nand we can rewrite the p^* criterion with partial derivatives as follows.\n\n\n\n\n\n\np^* criterion (rate of change version)\n\n\n\nThe total cost is bounded C^* &lt; C iff \np^* \\frac{\\partial C^*}{\\partial p^*} &lt; p \\frac{\\partial C}{\\partial p} - \\phi M.\n\n\n\nNow this, in my opinion, is just as interesting as the interpretation we have before. This is especially true as we shall see when we consider the difference in C^* and C later below. The p^* criterion now tells us that we can bound C^* by C iff the sensitivity in C^* with respect to p^* is bounded by the sensitivity in C with respect to p further tightened by the annual premium M. This makes sense, and gives an extra human intuition into the criterion."
  },
  {
    "objectID": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#some-actual-numbers",
    "href": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#some-actual-numbers",
    "title": "Is Costco fuel cheaper overall given the membership premium?",
    "section": "Some actual numbers",
    "text": "Some actual numbers\nNow it’s just a matter of plugging in numbers. Here, I’ll deal with hard numbers but coding this should be straightforward. I’ll demo this using my specs, using the Esso in front of my house as my nearest petrol station which is 0.2 miles away. Let’s put some numbers to the constants.\n\n\n\n\n\n\nAnnual fuelling frequency\n\n\n\nI’ll be fixing the annual fuelling frequency to \\phi = 1/52 as before.\n\n\n\n\n\n\n\n\nNearest petrol station\n\n\n\nThe Esso nearest to my house is d=0.2 miles away and they sell petrol at p = 132.9/100 £/litre.\n\n\n\n\n\n\n\n\nCostco petrol station\n\n\n\nThe nearest Costco to my house is d^*=4.8 miles away and they sell petrol at p = 126.9/100 £/litre. As of 2024, the annual Costco individual membership is priced at M = 33.60 pounds.\n\n\n\n\n\n\n\n\nCar fuel economy\n\n\n\nMy car’s fuel economy is about 40 mpg. That converts to about 5.9 litres/100km or 0.059 litres/km or 0.095 litres/mile. So put e = 0.095 litres/mile.\n\n\n\n\n\n\n\n\nPetrol volume to fill\n\n\n\nLet’s say I want to fill in about V = 60 litres of petrol, which is about 80% into my full tank capacity.\n\n\n\nDirect compute\nNow we are ready to compute. The total volume we want to fill in at the petrol station is\n\n\\begin{align*}\n\\text{Costco:} &\\quad V + v_{\\mathrm{cover}}^{*} = V + 2{d^*}e = 60 + 0.9120 \\text{ litres}, \\\\\n\\text{Nearest:} &\\quad V + v_{\\mathrm{cover}} = V + 2de = 60 + 0.0019 \\text{ litres}.\n\\end{align*}\n\nThis gives the total fuelling cost to be\n\n\\begin{align*}\n\\text{Costco:} &\\quad C^* = \\phi M + (V + v_{\\mathrm{cover}}^{*})p^* = £77.33, \\\\\n\\text{Nearest:} &\\quad C = (V + v_{\\mathrm{cover}})p = £79.74,\n\\end{align*}\n\nand we see that fuelling at Costco today is much cheaper despite the premium and the distance!\n\n\nUsing the p^* criterion.\nWe could have equivalently compute the p^* criterion and see that\n\n\\frac{V + v_{\\mathrm{cover}}}{V+v^*_{\\mathrm{cover}}} p - \\frac{\\phi}{V+v^*_{\\mathrm{cover}}} M = 1.29120 &gt; 1.329 = p^*.\n\nIndeed, the criterion is met and we did see lower Costco total cost as opposed to the cost using our nearest Esso station."
  },
  {
    "objectID": "posts/2024-04-22-asbf-primer-01/index.html",
    "href": "posts/2024-04-22-asbf-primer-01/index.html",
    "title": "A mathematical take on when to take a loan such as ASB Financing",
    "section": "",
    "text": "Disclaimer: Nothing written here is investment advice."
  },
  {
    "objectID": "posts/2024-04-22-asbf-primer-01/index.html#measuring-poor-financing-choices-with-curves",
    "href": "posts/2024-04-22-asbf-primer-01/index.html#measuring-poor-financing-choices-with-curves",
    "title": "A mathematical take on when to take a loan such as ASB Financing",
    "section": "Measuring poor financing choices with curves",
    "text": "Measuring poor financing choices with curves\nRecall that returns is a function of capital, but also assumes a fixed dividend and interest rate r. By relaxing the interest rate r assumption in \\tilde{f}(C; d, r), we see that r defines the family of financed curves \\tilde{f}(C, r; d). If we further relax the fixed dividend rate d assumption, we obtain a wider family of curves for both f(C, d, r) and \\tilde{f}(C, d, r). So how can we measure which financed curves are worse? One metric that I think is useful is the area enclosed between f and \\tilde{f}, bounded by the minimum and maximum of f. Where f, \\tilde{f} are linear functions in C, this area looks like a rhombus and the goal is to make this rhombus as small as possible. Let C_0 be the minimum amount of invested capital where 0 \\leqslant C_0 \\leqslant C_1; this should be the value that minimizes f. Then using very basic geometry, you can obtain this metric as the following quantity:\n\n\\begin{align*}\n\\mathrm{Loss}(f, \\tilde{f}) &= \\int_{[C_0, C_1]} \\left(f(C) - \\min_{C} f(C)\\right) \\mathrm{d}C + \\int_{[C_1, C_2]} \\left(\\max_{C} f(C) - \\tilde{f}(C)\\right) \\mathrm{d}C \\\\\n&= \\int_{[C_0, C_1]} \\left(f(C) - f(C_0)\\right) \\mathrm{d}C + \\int_{[C_1, C_2]} \\left(f(C_1) - \\tilde{f}(C)\\right) \\mathrm{d}C.\n\\end{align*}\n\nSo a good financing option – where interest rate is minimized – should be where the loss metric above is minimized. It can be rather tedious to compute this, especially when the return models f and \\tilde{f} gets bloated with other discount rates to make the model more accurate. Thankfully, however, there is an equivalence between this metric and the parity difference under certain conditions suggesting that it can be sufficient to look at the parity difference.\nIf one examines the loss function above, one observe that the first part is entirely defined by f. This quantity acts only as a bias term in the loss computation. Rather, the second quantity is what we should put our focus on. In the second integral, note that f(C_1) is fixed when f is fixed. But both f and \\tilde{f} is dependent on the dividend rate d. So suppose we fix a dividend rate, we can then minimize the loss on \\tilde{f} just over the space of possible interest rates and functional forms. Let’s make this even simpler and just focus on linear forms. In this case, it’s just a matter of minimizing interest rates which is equivalent to minimizing the range [C_1, C_2]. And that was exactly the parity difference we defined before!"
  },
  {
    "objectID": "posts/2021-04-07-tcolorbox/index.html",
    "href": "posts/2021-04-07-tcolorbox/index.html",
    "title": "Making pastel-colored boxes using tcolorbox in LaTeX",
    "section": "",
    "text": "One of the many questions I get when people see my \\LaTeX-ed math notes is how do you create these nice pastel-colored boxes?\n \n\nTwo examples of pastel-colored boxes from my math notes\n\n\nI feel like this is a very relevant and important question, because I asked the exact same thing when I was new to \\LaTeX.\n\nSimple box using fbox and minipage\nMy first exposure to \\LaTeX box-like environments was my mentor Ghaleo’s genius tutorial notes. He used them for two main reasons:\n\nto highlight important theorems, lemmas, etc;\nan empty space for students to follow along and fill in the blanks live in class.\n\nI tried to reproduce his box environment when I was writing my very first math notes on \\LaTeX – my Linear Algebra & Geometry II notes.\n\n\nA snippet of my Linear Algebra & Geometry II notes\n\nIt was achievable using the fbox environment alone, but for it to generalize well, I have to throw in a minipage usage as well. The code implementation is quite simple and requires no package imports. Just use the following code directly in your document:\n% Simple box environment.\n\\fbox{ %\n\\begin{minipage}[t]{0.9\\textwidth}\n    % Your text here.\n\\end{minipage}\n}\nTo make the box look cleaner, use the center environment as well. Here is a concrete example of using this box. The code:\n\\begin{document}\n\n\\begin{center}\n\\fbox{ %\n\\begin{minipage}[t]{0.9\\textwidth}\n    This is a plain small box.\n\\end{minipage}\n}\n\\end{center}\n\n\\end{document}\nwould yield the box:\n\nNow the problem with this simple fbox is that… it’s just plain black and white. I want something more vibrant and colourful, so I decided to start learning the tcolorbox package.\n\n\n\nTransition to tcolorbox\nAs usual, the best way to learn is to straight dive in and do. After (very) little reading on the package’s documentation, I started to recreate pretty boxes that other people have made from scratch. These boxes are usually from notes I found online like:\n\nEvan Chen’s The Napkin project;\nTony Zhang’s notes snippets from his Quora post;\nor even the tcolorbox documentation itself.\n\nWith my then minimal knowledge, I started writing my Real Analysis notes. I was quite happy with the end product but the overall design felt a bit odd (as you can see in the figure below). Nevertheless, my initial goal was to just make prettier boxes and I think I’ve achieved just that.\n\n\nA snippet of my Real Analysis notes\n\nTo reproduce the (proof) box above with a Cerulean colour, use the following code in your document’s preamble:\n% In preamble.\n\\usepackage[dvipsnames]{xcolor}\n\\usepackage[many]{tcolorbox}\n\n\\newtcolorbox{myAwesomeBox}{\n    enhanced,\n    sharp corners,\n    breakable,  % Allows page break.\n    borderline west={2pt}{0pt}{Cerulean},\n    colback=Cerulean!10,  % Background color.\n    colframe=Cerulean!10  % Frame (border) color.\n}\nTo see this box in action, we instantiate this environment in our document:\n\\begin{document}\n\n\\begin{myAwesomeBox}\n    This nice blue pastel box!\n\\end{myAwesomeBox}\n\n\\end{document}\nThis code should then yield:\n\nI want to note two things regarding the imports:\n\nThe dvipsnames option for xcolor is simply a choice of the set of colors I want from xcolor. It does affect the name of colors you can pass as arguments, for example, here I used Cerulean. For a full set of possible colors in xcolor, see here.\nThe many option for tcolorbox loads additional libraries which allows us to use more features from tcolorbox. In particular for our use case, we want to be able to use enhanced. See the documentation for other options.\n\nHere are some tweaks that you might want to consider:\n\nIf you want to add a nice smooth border-like effect, do this: instead of using colframe=Cerulean!10, remove that line and put boxrule=0pt instead. In general, boxrule adjusts the border stroke.\nIf you want to remove padding inside the box, add boxsep=0pt. In general, boxsep controls the inside padding.\n\nThere are many more parameters that you can control and these can be found in the tcolorbox documentation.\nNowadays, I write my notes using the NotesTex package which coincidentally uses tcolorbox with a similar setting as mine. I guess this is one of the reasons why it was a no brainer for me to use it in the first place. The nice thing about the package is that it provides a complete framework to write notes in \\LaTeX. In particular, you don’t have to reinvent the wheel for everything (e.g. theorem environment, sidenotes). And for features that you feel are missing, you can simply add it on your own with ease. I’ve added plenty of features already and they integrate quite well. I’d highly recommend checking NotesTex out!"
  },
  {
    "objectID": "posts/2023-03-06-ls-aws-in-python/index.html",
    "href": "posts/2023-03-06-ls-aws-in-python/index.html",
    "title": "How to do aws s3 ls s3://bucket/ using boto3 in python?",
    "section": "",
    "text": "Today, we look at how can we perform the following terminal command in python."
  },
  {
    "objectID": "posts/2023-03-06-ls-aws-in-python/index.html#why-do-we-even-care",
    "href": "posts/2023-03-06-ls-aws-in-python/index.html#why-do-we-even-care",
    "title": "How to do aws s3 ls s3://bucket/ using boto3 in python?",
    "section": "Why do we even care?",
    "text": "Why do we even care?\nA lot of times, you just want to list all the existing subobjects in a given object without getting its content. A typical use case is to list all existing objects in the bucket, where here, the bucket is viewed as an object – the root object. This list action can be achieved using the simple aws s3 ls command in the terminal.\nBut what if we want to do it natively as part of a module in python? For example, if we want to trigger a callback on the server whenever a new subobject is found in our bucket. A fast way is to just perform an equivalent aws s3 ls in our python module, and this is our goal today.\nTo ensure clarity, let’s kick off with the definition of a path, a bucket and an object prefix.\n\ndefinition\n\nA path is a string that consists of an S3 tag, a bucket and an object prefix. For example, s3://bucket/object/subobject/... is a generic path where s3:// is the S3 tag, bucket is the bucket and object/subobject/... is the object prefix. Note the position of / carefully."
  },
  {
    "objectID": "posts/2023-03-06-ls-aws-in-python/index.html#aws-s3-ls-in-python",
    "href": "posts/2023-03-06-ls-aws-in-python/index.html#aws-s3-ls-in-python",
    "title": "How to do aws s3 ls s3://bucket/ using boto3 in python?",
    "section": "aws s3 ls in python",
    "text": "aws s3 ls in python\nI assume that you’ve done the standard AWS credentials step, storing it at ~/.aws/credentials for example. We can then initialize an S3 client in Python using boto3.session.Session, I hope this step is familiar to you.\nimport boto3\n\nsession = boto3.session.Session()\nclient = session.client(\"s3\")\nNow that we have our S3 client, define our bucket and object prefix of interest.\nbucket = \"bucket_name\"\nobj_prefix = \"obj_in_bucket/\"\nThen we can simply list all the existing subobjects of our given object prefix using the following function:\ndef aws_s3_ls(bucket: str, obj_prefix: str):\n    params = dict(Bucket=bucket, Prefix=obj_prefix, Delimiter=\"/\")\n\n    paginator = client.get_paginator(\"list_objects_v2\")\n    for page in paginator.paginate(**params):\n        for obj in page.get(\"CommonPrefix\", []):\n            print(\"PRE\", obj[\"Key\"])\n\naws_s3_ls(bucket, obj_prefix)\nPRE subobj_1/\nPRE subobj_2/\nPRE subobj_3/\n...\nAnd that’s it! That was a short one and I hope to cover more AWS things in the future."
  },
  {
    "objectID": "posts/2024-08-13-ensembling-bagging-rf/index.html",
    "href": "posts/2024-08-13-ensembling-bagging-rf/index.html",
    "title": "Quickest intro to random forest",
    "section": "",
    "text": "As its name suggests, ensembling is quite literally ensembling several machine learning models to create a meta-model that (in theory) is better than any of the individual models. The meta-model is built typically via an averaging procedure. The idea has a natural intuition behind it: if you consider n iid model outputs X_1, \\ldots, X_n with variance \\mathrm{Var}(X_i) = \\sigma^2, then the sample mean variance given by \\mathrm{Var}(\\bar{X}) = \\frac{\\sigma^2}{n} converges to 0 as n \\to \\infty.\nIn reality, however, you don’t really get independence so the X_i are at most only identically distributed. This changes the math a bit, so let’s derive the sample mean variance again:\n\n\\begin{align*}\n\\mathrm{Var}(\\bar{X})\n&= \\frac{1}{n^2} \\mathrm{Var}(X_i) + \\frac{1}{n^2} \\sum_{i, j} \\mathrm{Cov}(X_i, X_j) \\\\\n&= \\frac{\\sigma^2}{n} + \\frac{n-1}{n} \\rho \\sigma^2 \\\\\n&= \\rho \\sigma^2 + \\frac{1 - \\rho}{n}\\sigma^2,\n\\end{align*}\n\nwhere \\rho is the average correlation across the X_i. To create a meta-model now, we want two things to happen:\n\nWe want to consider as many different models as possible so that n \\to \\infty and hence (1-\\rho)\\sigma^2/n \\to 0.\nWe also want the models to be as decorrelated as possible so that \\rho \\to 0 and hence \\rho \\sigma^2 \\to 0 and we are back in the iid case.\n\nThis gives rise to several different ways of ensembling machine learning models. One that is quite popular is called bootstrap aggregation or simply bagging and it is a very simple procedure."
  },
  {
    "objectID": "posts/2024-08-13-ensembling-bagging-rf/index.html#why-ensemble",
    "href": "posts/2024-08-13-ensembling-bagging-rf/index.html#why-ensemble",
    "title": "Quickest intro to random forest",
    "section": "",
    "text": "As its name suggests, ensembling is quite literally ensembling several machine learning models to create a meta-model that (in theory) is better than any of the individual models. The meta-model is built typically via an averaging procedure. The idea has a natural intuition behind it: if you consider n iid model outputs X_1, \\ldots, X_n with variance \\mathrm{Var}(X_i) = \\sigma^2, then the sample mean variance given by \\mathrm{Var}(\\bar{X}) = \\frac{\\sigma^2}{n} converges to 0 as n \\to \\infty.\nIn reality, however, you don’t really get independence so the X_i are at most only identically distributed. This changes the math a bit, so let’s derive the sample mean variance again:\n\n\\begin{align*}\n\\mathrm{Var}(\\bar{X})\n&= \\frac{1}{n^2} \\mathrm{Var}(X_i) + \\frac{1}{n^2} \\sum_{i, j} \\mathrm{Cov}(X_i, X_j) \\\\\n&= \\frac{\\sigma^2}{n} + \\frac{n-1}{n} \\rho \\sigma^2 \\\\\n&= \\rho \\sigma^2 + \\frac{1 - \\rho}{n}\\sigma^2,\n\\end{align*}\n\nwhere \\rho is the average correlation across the X_i. To create a meta-model now, we want two things to happen:\n\nWe want to consider as many different models as possible so that n \\to \\infty and hence (1-\\rho)\\sigma^2/n \\to 0.\nWe also want the models to be as decorrelated as possible so that \\rho \\to 0 and hence \\rho \\sigma^2 \\to 0 and we are back in the iid case.\n\nThis gives rise to several different ways of ensembling machine learning models. One that is quite popular is called bootstrap aggregation or simply bagging and it is a very simple procedure."
  },
  {
    "objectID": "posts/2024-08-13-ensembling-bagging-rf/index.html#bagging-and-its-bias-variance-tradeoff",
    "href": "posts/2024-08-13-ensembling-bagging-rf/index.html#bagging-and-its-bias-variance-tradeoff",
    "title": "Quickest intro to random forest",
    "section": "Bagging and its bias-variance tradeoff",
    "text": "Bagging and its bias-variance tradeoff\nLet \\mathcal{P} be the true population, and suppose we have a training set \\mathcal{S} sampled from \\mathcal{P}. Then, we can create n bootstrap samples Z_1, \\ldots, Z_n by sampling from \\mathcal{S} with replacement. This is the bootstrap step. We then train n different models G_1, \\ldots, G_n on these bootstrap samples for each i. Further, we create a meta-model\nG(i) = \\frac{1}{n} \\sum_{i=1}^n G_i(x)\nthat essentially averages the predictions of the G_i. This is the aggregation step.\nLet’s consider the bias-variance tradeoff for the bagging procedure. Generally, bootstrapping decreases the average correlation \\rho as we perform random subsampling with replacement without accounting for specific features in the dataset. As a consequence, the correlation term \\rho \\sigma^2 decreases. Further, the variance term (1-\\rho)\\sigma^2/n decreases as we increase n. However, as a tradeoff, we have increased the bias since we are training on bootstrap samples Z_i \\subset \\mathcal{S}. This is a classic no free lunch scenario in machine learning where as the variance decreases, you risk increasing the bias.\nHaving said that, the decision tree is a perfect model to be used in constructing a meta-model via bagging. This is because the decision tree is a high variance, low bias model so the net effect of performing bagging is, in some sense, mitigated.\nWe can further add an additional step to further decorrelate the decision tree outputs. Rather than considering the entire feature space, we can consider only a fraction of the total features at each split. As a consequence, the correlation term \\rho \\sigma^2 decreases even further! This meta-model that arises from performing bagging using decision trees with this additional procedure is called a random forest.\nSo a random forest is essentially:\n\n\\mathrm{RandomForest} = \\mathrm{Bagging}(\\mathrm{DecisionTree}(\\mathrm{RandomFeatureSubspace}))."
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html",
    "href": "posts/2023-03-05-clash-is-prime/index.html",
    "title": "Clash Series: Checking Number is Prime",
    "section": "",
    "text": "This is the first of hopefully many more posts on Clash of Code tricks I learn along the way. I call this the Clash Series and in today’s series, we look at how to write an efficient script to check for prime numbers, and how to write it fast python for those fastest mode clashes.\nEquivalently, we say p is prime if p &gt; 1 and whenever it decomposes into p = ab, then either a=1, b=p or a=p, b=1. Thus, if p is not prime, then there is a decomposition of p into a product of positive integers a, b that are not necessarily 1 or p; hence, the name composite.\nThe first few prime numbers are 2, 3, 5, 7, 11 and so on, while the first few composite numbers are 1, 4, 6, 8, 9 and so on.\nNow here is what we are interested in."
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html#the-problem",
    "href": "posts/2023-03-05-clash-is-prime/index.html#the-problem",
    "title": "Clash Series: Checking Number is Prime",
    "section": "The problem",
    "text": "The problem\n\nproblem statement\n\nGiven an integer n \\in \\mathbb{Z}, how to enumerate all prime numbers less than n?\n\n\nA naive enumeration algorithm can be achieved in the following way: For any given integer n,\n\nIf n &lt; 2, then n cannot be prime.\nIf n = 2, then n is prime.\nLoop over all integers k \\in [3, n). If there exists k such that k \\equiv 0 \\bmod n, then n is not prime. Otherwise n is prime.\n\nA python code for this naive implementation is as follows.\n\ndef is_prime_naive(n: int) -&gt; bool:\n    # 1 and any negative integer are not prime.\n    if n &lt; 2:\n        return False\n    # 2 is prime.\n    if n == 2:\n        return True\n    for k in range(2, n):\n        # If n is divisible by k for any k &lt; n, n is not prime.\n        if n % k == 0:\n            return False\n    return True\n\nThe time complexity for this algorithm is \\mathcal{O}(n) where n is the given integer. This is already good in most cases, but can we do better?\nWell any integer n &gt; 2 is always divisible by 2 if it is even, and therefore cannot be prime. So we can add a step to check if n is even or not before doing the looping step. As a result, our algorithm running time is cut in half as we now only have to loop over only odd numbers less than n.\n\ndef is_prime_better(n: int) -&gt; bool:\n    # 1 and any negative integer are not prime.\n    if n &lt; 2:\n        return False\n    # 2 is prime.\n    if n == 2:\n        return True\n    # NEW: If n is even and n is not 2, then it is not prime.\n    if n % 2 == 0:\n        return False\n    for k in range(3, n, 2):\n        # NEW: If n is divisible by odd k for any k &lt; n, n is not prime.\n        if n % k == 0:\n            return False\n    return True\n\nThe problem, however, is that the theoretical time complexity of this algorithm is still \\mathcal{O}(n/2) = \\mathcal{O}(n). So good for small n but still problematic for super large n."
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html#efficient-algorithm-sqrt-trick",
    "href": "posts/2023-03-05-clash-is-prime/index.html#efficient-algorithm-sqrt-trick",
    "title": "Clash Series: Checking Number is Prime",
    "section": "Efficient algorithm (sqrt trick)",
    "text": "Efficient algorithm (sqrt trick)\nSo what can we do to make this code faster? Enter… the square root (sqrt) trick.\nFirst, without loss of generality, let’s assume that n is a positive integer since if n &lt; 2, then it’s taken care by the n &lt; 2 check. We now make the following claim.\n\nLemma 1 (Sqrt Decomposition) Let n &gt; 1 be a positive integer. If n = ab is a composite integer such that 0 &lt; a \\leqslant b, then a \\leqslant \\sqrt{n} \\leqslant b.\n\n\nProof. First observe that n = ab is a perfect square if and only if a = b = \\sqrt{n}, in which case we are done.\nSo suppose n = ab is not a perfect square. Then a and b cannot be \\sqrt{n} and moreover a &lt; b. We now claim that a &lt; \\sqrt{n} &lt; b. Suppose a &lt; \\sqrt{n} but also b &lt; \\sqrt{n}, then ab &lt; n which is a contradiction. Similarly, if both a &gt; \\sqrt{n} and b &gt; \\sqrt{n}, then ab &gt; n, also a contradiction. Thus, since a &lt; b, it has to be that a &lt; \\sqrt{n} &lt; b. \\blacksquare\n\nSo what’s the point of Lemma 1? Well the main takeaway is the following: if the number n is composite, then we will at least find one of its divisors before or equal its integer square root \\lfloor \\sqrt{n} \\rfloor.\n\nTheorem 1 Let n be a composite positive integer. Then there exists a prime divisor p of n such that p \\leqslant \\lfloor \\sqrt{n} \\rfloor.\n\n\nProof. By Lemma Lemma 1, we know that if n = kb is a composite integer with 0 &lt; k \\leqslant b, then k \\leqslant \\sqrt{n}, with equality iff n is a perfect square. Equivalently, this means that k \\leqslant \\lfloor \\sqrt{n} \\rfloor. If k is prime, then we are done. Otherwise, there exists a prime p that divides k (hence, divides n) that would also satisfy p \\leqslant \\lfloor \\sqrt{n} \\rfloor as desired. \\blacksquare\n\n\nNote: for a linear traversal check (like we are doing, starting from 3, 4, 5, …), we know that we will encounter any prime divisor p of any composite divisor k of n first. So the last part of the proof is unnecessary for our need, but we put it there for completeness.\n\nSo the idea of using sqrt decomposition speeds up our algorithm tremendously, especially for large n. In fact, even for small n like n=1080, its integer square root is 32 which is already two order of magnitudes lower.\nWe now add the sqrt trick to our python implementation.\n\ndef is_prime_sqrt_trick(n: int) -&gt; bool:\n    # 1 and any negative integer are not prime.\n    if n &lt; 2:\n        return False\n    # 2 is prime.\n    if n == 2:\n        return True\n    # If n is even and n is not 2, then it is not prime.\n    if n % 2 == 0:\n        return False\n    # NEW: loop until int(sqrt(n)) + 1.\n    # The + 1 is to handle if n is perfect square.\n    for k in range(3, int(n**.5)+1, 2):\n        # If n is divisible by odd k for any k &lt; n, n is not prime.\n        if n % k == 0:\n            return False\n    return True\n\nThe sqrt trick improves time complexity from \\mathcal{O}(n) to \\mathcal{O}(\\sqrt{n}) which is massive! To give you an idea of this speedup, we will run the naive and sqrt algorithms to check if n = 2{,}147{,}462{,}143 is prime. Note that this is a prime number on the order of 2^{31}.\n\nThe sqrt trick improves time complexity from \\mathcal{O}(n) to \\mathcal{O}(\\sqrt{n}) which is massive!\n\n\n\nspeedup comparison ⚡️\n\nimport time\n\nn = 2_147_462_143\n\nfor f in [is_prime_naive, is_prime_sqrt_trick]:\n    start = time.time()\n    f(n)\n    print(f\"Took total of {(time.time()-start):.10f} seconds using {f.__name__}\")\n\nTook total of 118.3165051937 seconds using is_prime_naive\nTook total of 0.0010521412 seconds using is_prime_sqrt_trick"
  },
  {
    "objectID": "posts/2023-03-05-clash-is-prime/index.html#efficient-writing",
    "href": "posts/2023-03-05-clash-is-prime/index.html#efficient-writing",
    "title": "Clash Series: Checking Number is Prime",
    "section": "Efficient writing",
    "text": "Efficient writing\nSo the impementation is super efficient \\mathcal{O}(\\sqrt{n}), but how do we make writing the code efficient for something like Clash of Code?\nWell first note that when writing in a clash, you don’t care about the 80 char PEP format or readability. So it’s finally time to write one-liner 100 chars fugly code.\nThe trick I use is to use all to replace the for-loop and replace the False checks with True statement by “bundling” their evaluation using and. This gives the following one-liner.\n\ndef is_prime_sqrt_short(n: int) -&gt; bool:\n    return n == 2 or (n &gt; 2 and n % 2 != 0 and all(n % k != 0 for k in range(3, int(n**.5)+1, 2)))\n\nIn fact, since the modulo operator % in python returns a positive integer, we can save writing one character for each != by writing &gt; instead.\n\ndef is_prime_sqrt_super_short(n: int) -&gt; bool:\n    return n == 2 or (n &gt; 2 and n % 2 &gt; 0 and all(n % k &gt; 0 for k in range(3, int(n**.5)+1, 2)))"
  },
  {
    "objectID": "posts/2024-09-16-naivety-and-desperation-in-math-modelling/index.html",
    "href": "posts/2024-09-16-naivety-and-desperation-in-math-modelling/index.html",
    "title": "Naivety and desperation in mathematical modelling can somewhat push you far",
    "section": "",
    "text": "When I was 19, I had to complete the notorious Math HL internal assessment (IA) for the International Baccalaureate (IB) diploma. The assessment is basically an open-ended project and was really a platform for students to explore a topic that interests them, showcasing their creativity. But for the average IB student like me, it was just another difficult assessment to complete so that I can get into a top university.\nMath was not really my strong suit back then contrary to what people today thought. I took Math HL – the harder version of math in the IB – because I wanted to do Computer Science at a good university and it was listed out as a requirement; and definitely not because I was good at math. Having been punished with internal exams then, I quickly realized that the final written exams is going to be hard. Luckily for me, the final marks was a weighted sum of the internal assessment and the written exam. So my logic was that if I can score really (and I mean, really) high in the internal assessment, then I would have a good shot at getting a good grade in the diploma.\nFrom there, the focus was clear. How do I make a damn good IA, I asked myself. I am not going to answer this here, but what I can tell you is that I basically just went against most of what my teachers advised me to do; and trusted my gut feelings instead. If you’re interested, however, in gaming the marking metrics, I wrote this Quora post back in 2019. It has accumulated 200k+ views since and I still receive loads of messages from people asking for my IA.\nRecalling what was going on in my mind 6 years ago, the equation that I was most proud of has to be:\n\n\\nu(Q) = \\left\\lfloor \\frac{1}{2 \\pi} \\exp \\left\\{ \\omega \\left ( \\tau_0 \\log \\left( \\frac{1}{K} \\log \\frac{\\Delta Q_S}{Q - Q_a} \\right) + \\tau_C \\right) \\right\\} \\right\\rfloor,\n\nand I’m not even going to detail what those variables and constants are. This equation was the final output of my IA research. It describes the number of spoon rotations \\nu that one should perform to get the temperature of coffee from a temperature \\Delta Q_S + Q_a to Q. You want your coffee to be 45 degrees? Turns out you need 1442 rotations according to the paper I wrote (of course this is assuming a lot of things). It was the culmination of intense thinking and hard work which I am still proud of today, especially because this is the first original work I have done in math, ever.\nMind you I was only exposed to very little math at this point. All I used was basic differential equations and a tiny bit of complex numbers. If only I knew about complex analysis back then… it would have been even more interesting.\nWhile the equation above is indeed cool, it’s not really the star of the paper I wrote. From my perspective today after having done several years of mathematical training, it was how I get to this equation is the remarkable bit. The true star of that paper is when I made this link: view a spoon rotation as a movement on a perfect circle of fixed radius r and forget the cup entirely. Then the spoon moves along this circle exactly like a complex number z(t) = r \\exp (i \\theta(t)) with angle \\theta(t) modulo (-\\pi, \\pi] which makes a lot of sense. What doesn’t make a lot of sense is what I do next.\nRather than just tracking for the position of spoon on this circle, I want a measure \\nu for the total number of distance traversed by this spoon on the circle. Essentially, I want the number of rotations done by the spoon, preferably an integer, but I’m happy taking fractional values as well since we can always floor a rational number. Since we are calculating \\theta(t) modulo (-\\pi, \\pi], we can do this by keeping track how many times we have gone outside of this range. But how can you do this in a single equation?\nBack in school, I remembered that a core element of the math HL IA was to use “HL mathematics”. Essentially, this means using mathematics that were taught only in the higher level HL syllabus but not covered in the standard level. This is because it was part of the marking criteria made public by the IBO. My teacher repeatedly reminded the class about this, so I became desperate. Remember, I need to score really high in the IA to offset my downside in the written portion of the exam. And boy, naivety and desperation really brings you far. I decided to model the number of spoon rotations… also using complex numbers, since it was covered only as part of the HL syllabus. So I asked myself, suppose a complex number z(t) equals the number of fractional rotations \\nu times the circumference of the circle C, that is, z = \\nu C, what happens then?\nAny person with some mathematical modelling experience would deem this not a great move. But it worked. This gave the equation I mentioned earlier in the post and it works like a charm then, it still works today. So let’s see why this works.\nPutting z = \\nu C means equivalently we have \\nu = z/C which is what we actually want. As before, write z(t) = r \\exp (i \\theta (t)), but now forget that \\theta is an angle, just view it purely as a function. I’m gonna skip being rigorous for now, like whether \\theta is complex-valued or real-valued, let’s just wing it since I’m talking about a high school project anyways. So modelling number of spoon rotations \\nu using this equation amounts to getting a sensible \\theta(t) function. Now we want \\nu to be nonnegative and the number of spoon rotation should grow with increasing t. Thus \\theta(t) must admit a form t \\mapsto i\\theta'(t) for some arbitrary function \\theta' to kill i. We don’t really need a negative sign to force the exponent terms to be positive (due to i^2=-1) since it is possible there is a “damping” effect with increasing number of rotations to cool down the coffee.\nFrom here, we can actually stop and see why it works. There is nothing stopping the function \\theta' to admit a logarithmic form, killing the exponent and allows z to take virtually any shape. For example, if \\theta'(t) is simply t \\mapsto -\\log(t+k) where k is a constant, then we get z(t) into a linear form.\n\nz(t) = r \\exp (i (i (- \\log(t+k)))) = r \\exp \\log(t+k) = rt + rk = rt + k'.\n\nIt should be clear from here that the possibilities are indeed endless since \\theta' can be almost anything.\nWhat’s funny is that this approach is the exact opposite to Occam’s razor, a philosophy that most modellers would follow. Rather than starting with a simpler model, here you start by opting for a more complex model that can be reduced (in an explainable way) to a simpler model. The only upside is that if the number of spoon rotations is indeed exponential, or requires damping, then we have the right model right away. Occam’s razor advises to do the opposite – start with a simpler model, see whether it breaks, and if it does, see where and how it breaks and improve on that. There is wisdom to be learnt here, however.\nFor a start, Occam’s razor can fail. Occam’s razor has led me to wrong analysis conclusions so many times in my years of experience as a data scientist, so it is refreshing to see an example where someone not obeying Occam’s razor has something that magically works right off the bat.\nSecondly, naivety and desperation can bring you quite far. My math HL IA was my first experience doing mathematical modelling from scratch. I didn’t even know what Occam’s razor was, or how a mathematician would actually do mathematical modelling and my teachers weren’t much of a help honestly. So I just winged it, asked the right questions but tried solutions that I don’t even know was going to work or not. What if the shape z(t) = r \\exp (i \\theta(t)) was indeed the only right shape for the problem? That would have been viewed as a spectacular (lucky) discovery rather than a goofy approach, no?\nSo would I do something like this today? Probably not. But I think this is a great counterpoint for why you want to hire fresh juniors sometimes since hungrier people with little training implies a fresh view of the problems you’ve been working on for a long time. Training them is another story though…"
  },
  {
    "objectID": "posts/2021-04-05-newton-method/index.html",
    "href": "posts/2021-04-05-newton-method/index.html",
    "title": "Root-hunting algorithm: Newton’s method",
    "section": "",
    "text": "Problem: Given a real-valued function f(x) in one real variable, what are the values x_0 \\in \\mathbb{R} such that f(x_0) = 0?\nIf the function f(x) is linear, then the problem is trivial. Explicitly, if f(x) = ax + b for some a, b \\in \\mathbb{R}, then x_0 = -b/a gives a solution as long as a \\neq 0. However, when the function is nonlinear, the problem can get complicated very fast. For example, try solving when the function is f(x) = \\sin(e^{x}) + \\cos(\\log x).\n\n\nNewton’s idea (an overview)\nOne way of solving this problem is to linearize the function f(x) around a certain point x_0 of our choice so that we can easily solve the resulting linear equation. Say we get x_1 as a solution, then we repeat linearizing f(x) around x_1; so on and so forth. The initial point x_0 is chosen such that it is close to our hoped solution, say, x^*. The idea is that if x_0 is suitably chosen, then the solutions x_1, x_2, x_3, \\ldots to each linear approximation of f(x) approximates x^* better and better, and in the limit converges to x^*. This whole process is known as the Newton’s method.\nA nice picture of the Newton’s method can be seen in Figure 1. Here, Newton’s method is applied to the function f(x) = x^2 over n = 10 iterations, starting at x_0 = 1. We see from Figure 1 that the x_i values converges to x^* = 0 which is expected since x^2 = 0 if and only if x = 0.\n\n\n\nNewton’s idea (the algebra)\nLet us make our discussion above more precise. Linearizing f(x) around x_0 simply means Taylor expanding f around x_0 and neglecting \\mathcal{O}(h^2) terms. Of course… this is assuming that we can actually perform Taylor expansion in the first place. With that disclaimer out of the way, the Taylor expansion of f around x_0 yields\nf(x) = f(x_0) + f'(x_0) (x - x_0) + \\mathcal{O}(h^2) \\approx f(x_0) + f'(x_0) (x - x_0). \nSo if we neglect \\mathcal{O}(h^2) terms, we get the linear equation\nf(x) = f(x_0) + f'(x_0) (x - x_0).\nSo a solution x_1 that satisfy f(x_1) = 0 is given by\nx_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}\nWe then repeat the process by linearizing f around x_1. In this case we have\nf(x) = f(x_1) + f'(x_1) (x - x_1) \\implies x_2 = x_1 - \\frac{f(x_1)}{f'(x_1)}, \nwith x_2 being a solution. Doing this iteratively yields a general formula\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)},\nknown as Newton’s formula. Here is the Newton’s method in one statement.\n\n\nTheorem 1 (Newton’s method) Let x^* \\in \\mathbb{R} be a solution to f(x) = 0. If x_n is an approximation of x^* and f'(x_n) \\neq 0, then the next approximation to x^* is given by x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}, with initial condition, a suitably chosen x_0 \\in \\mathbb{R}.\n\n\n\n\nCode implementation\nAn iterative implementation of the Newton’s method in Python is given below:\n\ndef iterative_newton(f, df, x0, n):\n    \"\"\"Solves f(x) = 0 using the Newton's method.\n\n    Args:\n        f: A callable, the function f(x) of interest.\n        df: A callable, the derivative of f(x).\n        x0: Initial good point to start linearizing.\n        n (Optional): Number of recursion steps to make.\n    \"\"\"\n    xs = [x0] # Sequence of xn.\n\n    # Get latest x value in sequence and\n    # apply the Newton recurrence formula.\n    for _ in range(n):\n        last = xs[-1]\n        res = last - f(last)/df(last)\n        xs.append(res)\n\n    return xs\n\nUsing the same parameters as above, we can also implement a one-liner recursive implementation:\n\ndef recursive_newton(f, df, x0, n):\n    return x0 if n &lt;= 0 else recursive_newton(f, df, x0 - f(x0)/df(x0), n-1)\n\nObserve that both algorithms have \\mathcal{O}(n) space complexity where n is the number of iterations or depth of the recursion. The time complexity for the iterative implementation is also \\mathcal{O}(n), but for the recursive implementation, it is a bit tricky to compute (so we leave it as an exercise!).\nNote that there is a small caveat to the Newton’s method which we have implicitly highlight in this post, can you spot it?\n\n\nExample usage: finding root of f(x) = x^2\n\n\nLibrary imports\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nWe first need a helper function to differentiate a function using finite difference approximation.\n\ndef finite_diff(f):\n    \"\"\" Returns the derivative of f(x) based on the\n    finite difference approximation.\n    \"\"\"\n    h = 10**(-8)\n    def df(x):\n        return (f(x+h)-f(x-h)) / (2*h)\n    return df\n\nWe then define the function f(x) = x^2, compute its derivative and apply Newton’s method over n = 10 iterations, starting at x_0 = 1.\n\nf = lambda x: x**2\ndf = finite_diff(f)  # Differentiate f(x).\nres = iterative_newton(f, df, 1, 10)\nres = np.array(res)  # To utilize numpy broadcasting later.\n\nFinally, we plot the function.\n\n\nPlotting code\nplt.style.use('bmh')\n\n# Bounds on the x-axis.\nlo = -0.1\nhi = 1.1\nmesh = abs(hi) + abs(lo)\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Points of the function f(x).\nxs = np.arange(start=lo, stop=hi, step=0.01)\nys = f(xs)\n\ndef tangent_line(f, x0, a, b):\n    \"\"\" Generates the tangent line to f(x) at x0 over\n    the interval [a, b]. Helps visualize the Newton's method.\n    \"\"\"\n    df = finite_diff(f)\n    x = np.linspace(a, b, 300)\n    ytan = (x - x0)*df(x0) + f(x0)\n\n    return x, ytan\n\n# Tangent lines to f(x) at the approximations.\nxtan0, ytan0 = tangent_line(f, res[0], 0.35*mesh, hi)\nxtan1, ytan1 = tangent_line(f, res[1], 0.1*mesh, hi)\nxtan2, ytan2 = tangent_line(f, res[2], lo, 0.7*mesh)\n\nax.plot(xs, ys, label=\"$f(x) = x^2$\", linewidth=3)\nax.plot(xtan0, ytan0, label=\"Linearization 1\", alpha=0.8)\nax.plot(xtan1, ytan1, label=\"Linearization 2\", alpha=0.8)\nax.plot(xtan2, ytan2, label=\"Linearization 3\", alpha=0.8)\nax.plot(res, f(res), color='darkmagenta',\n        label=\"Newton's method\\napproximations\",\n        marker='o', linestyle='None', markersize=6)\n\n# Labels for occurring approximations.\nfor i in range(0, 4):\n    ax.plot(res[i], 0, marker='+', color='k')\n    ax.vlines(res[i], ymin=0, ymax=f(res[i]),\n              linestyles='dotted', color='k', alpha=0.3)\n    plt.annotate(f\"$x_{i}$\",\n                 (res[i], 0),\n                 textcoords=\"offset points\",\n                 xytext=(0, -20),\n                 ha='center',\n                 fontsize=16)\n\n# Grid and xy-axis.\nax.grid(True, which='both')\nax.axvline(x = 0, color='k')\nax.axhline(y = 0, color='k')\n\n# Labels and legend.\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"Newton's method applied to $f(x) = x^2$\")\nplt.legend(fontsize=12, loc=9)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Newton’s method applied to f(x) = x^2, starting at x_0 = 1."
  },
  {
    "objectID": "posts/2024-09-16-latte-art-light-dark-roasts/index.html",
    "href": "posts/2024-09-16-latte-art-light-dark-roasts/index.html",
    "title": "TIL: latte art and light/dark roasts",
    "section": "",
    "text": "When I said I’m gonna write about my random technical thoughts, I never said it has to be software or mathematical. Brewing coffee is pretty technical I reckon.\nI attended the Bristol Coffee Festival last Saturday (14 September 2024) and drank almost enough espresso to make my general admission ticket worth it. While being super caffeinated, I had a good amount of chat with some of the 100+ coffee roasters and baristas that were there. Needless to say, I learnt a lot. Now I’m sure I could find this knowledge on YouTube if I spend enough time watching James Hoffman’s videos or simply in r/Coffee or r/JamesHoffman if I spend enough time there. However, nothing beats a good conversation with a knowledgable person where you can ask questions and get direct feedback.\nHere are several things I learnt."
  },
  {
    "objectID": "posts/2024-09-16-latte-art-light-dark-roasts/index.html#latte-art-is-a-function-of-coffee-crema-and-milk-texture",
    "href": "posts/2024-09-16-latte-art-light-dark-roasts/index.html#latte-art-is-a-function-of-coffee-crema-and-milk-texture",
    "title": "TIL: latte art and light/dark roasts",
    "section": "Latte art is a function of coffee crema and milk texture",
    "text": "Latte art is a function of coffee crema and milk texture\nI’ve been trying to learn latte art for a long time now, stretching all the way back to December 2020, wasting a lot of milk in the process. I never had professional guidance, I just kind of do it by trial and error, and it just never worked out nicely. I had suspicions for why it didn’t work out though, and I’m glad I finally confirmed my suspicions with professional baristas.\nIt turns out that the body of the coffee matters. Pulling a latte art on runny espresso is difficult; on a filter coffee even worse. So step one is to pull a good shot of espresso – which translates to first getting a decent espresso machine.\nSecondly, the milk texture is crucial. This means, the milk has to be aerated properly – so not only the surface that you can see but also the body beneath it too. I finally have someone telling me how to do it properly. It turns out, heating the milk to 65 degrees is key, and 65 is quite hot. This means that the milk pitcher will be hot and you can just know when to stop steaming by feel. The milk pitcher will be hot. It was very hot when I did the run. It was hot. It was very hot."
  },
  {
    "objectID": "posts/2024-09-16-latte-art-light-dark-roasts/index.html#taste-converges-with-roast-timing",
    "href": "posts/2024-09-16-latte-art-light-dark-roasts/index.html#taste-converges-with-roast-timing",
    "title": "TIL: latte art and light/dark roasts",
    "section": "Taste converges with roast timing",
    "text": "Taste converges with roast timing\nThe people at Clifton Coffee Roasters taught me a good chunk about coffee notes. I came in with the universal skeptic question of “are coffee notes even real?”. You know this question can trip some coffee snobs for sure, but the peeps at CCR were very cool in explaining it.\nIt turns out that the coffee notes are indeed real, and they proved it to me by giving me a taste of their range of filter coffees which were processed in different ways – natural, washed and a mixed process. You see, coffee meant for filter are usually lighter roasted and so the notes are preserved. Otoh, coffee meant for espresso is darker roasted so that is is more soluble, inducing a thicker body. If you go to a specialty coffee shop where there is some expectation that people order espresso for the sake of espresso, they will have a range of roasts for you to choose from, but most importantly, they will be mindful that some people will be ordering the espresso for the taste and not the milk.\nOtoh, the thing with coffee chains like Pret, Costa or Nero is that there is an almost universal understanding that people will come for milk-based coffee; and milk-based coffee has an espresso a base (has to be a truly shitty coffee chain if they use filter coffee as a base, right *cough* Greggs *cough*?). But for milk-based coffee, the milk will almost always dominate the coffee taste right. So in that sense, it doesn’t really matter how dark the roast is for the espresso, you just need it to be dark. How dark? Well, it’s easier to just roast them super dark rather than trying to find the right amount of darkness. As long as it’s dark, it’s soluble, so you can extract an espresso done. And this is what coffee chains do, which explains why their espresso tastes more or less the same – terrible.\nAnd what’s the price of doing a very dark roast? You lose the coffee notes."
  },
  {
    "objectID": "posts/2024-08-04-quantum-fibonacci/index.html",
    "href": "posts/2024-08-04-quantum-fibonacci/index.html",
    "title": "A benchmark for testing quantum computers with Clifford+T hardware",
    "section": "",
    "text": "I was revisiting some of my MSc stuff from two years ago and I came across one of my original work in Quantum Processes and Computations (QPC). It was one of the funner courses I took – I had a genuine interest in the hype behind quantum computing at the time and the prerequisite for the course was just simple linear algebra. Aleks Kissinger taught the course and he’s incredibly passionate about the subject and is also a good explainer. He was also the one who interviewed me to get into the program where he randomly asked me about my favourite theorem in Galois Theory, really peeking my interest to the point of “what the heck are they cooking in the quantum computing group” but that’s another story.\nIn QPC, a part of the take-home “exam” was to produce anything interesting using the ZX-calculus. Bonus points if it’s original, has a strong motivation and/or has a rigorous proof. “Original” here was a bit vague in the sense that various parts can be original – the idea, implementation, proof, etc. I remember just spending several days trying to figure out what to do — really went deep into theorems in number theory, for example, and I manage to land on something original and provided a working proof, but not so much on good motivation I confess. Ultimately, I decided to keep it simple and build on a Fibonacci heuristic proposed in (Gilliam, Pistoia, and Gonciulea 2020), where I modified the routine they wrote, implemented it in the ZX-calculus and proved that the algorithm is correct. The routine relies solely on Clifford+T gates and because of the simplicity of the Fibonacci algorithm (as we shall see), it ought to serve as a good benchmarking tool for quantum computers with hardware that accepts only Clifford+T gates.\nBefore I go any further, I have to warn you that this is neither an introduction to quantum computing nor ZX-calculus. What I can tell you is that ZX-calculus gives you superpowers for reasoning in quantum computing. If you want a primer on both at the same time, check out (Coecke and Kissinger 2017) which is where I learnt all the basics from. Another good resource is John’s ZX-calculus primer ZX-calculus for the working quantum computer scientist. John van de Wetering is also one of the guys who taught QPC alongside Aleks. I realize that nowadays, there’s also less dense intros to the subject as well like this one at PennyLane."
  },
  {
    "objectID": "posts/2024-08-04-quantum-fibonacci/index.html#what-does-combinatorics-tells-us-about-fibonacci-numbers",
    "href": "posts/2024-08-04-quantum-fibonacci/index.html#what-does-combinatorics-tells-us-about-fibonacci-numbers",
    "title": "A benchmark for testing quantum computers with Clifford+T hardware",
    "section": "What does combinatorics tells us about Fibonacci numbers?",
    "text": "What does combinatorics tells us about Fibonacci numbers?\nDefine a recurrence relation F_n = F_{n-1} + F_{n-2} with initial conditions F_0=1 and F_1 = 2 where n &gt; 1. The positive integers F_n generated by this recurrence relation is universally known today as the Fibonacci sequence 1, 2, 3, 5, 8, 13, ... and so on. At this point, it is not straightforward how we can encode this relation into a quantum circuit. So how do we encode this information to perform quantum computation? If I learn anything from my short stint of constructing quantum algorithms, you can always end up with a working algorithm by converting the problem into a counting problem. If you have a counting problem, you can perform what is called heuristic encoding where you can design a quantum circuit that “generates” the things you want to count, run the circuit repeatedly for a sufficiently long period, and then count the number of measurement outcomes, possibly with some required classical post-processing.\nFor the Fibonacci numbers, combinatorics tells us that there is an equivalence between the Fibonacci numbers and the number of length n binary sequences with no consecutive ones where this equivalence is given as F_n = \\left|B^n \\right| where\nB^n = \\left\\{ x \\in \\{0,1\\}^n : x \\text{ has no consecutive ones} \\right\\}.\nWith this formulation of the Fibonacci numbers, we now have a counting problem rather than a recurrence problem where we can perform heuristic encoding! So let’s now turn our attention to constructing a quantum routine that generates elements of B^n."
  },
  {
    "objectID": "posts/2024-08-04-quantum-fibonacci/index.html#building-a-quantum-routine-for-generating-elements-of-bn",
    "href": "posts/2024-08-04-quantum-fibonacci/index.html#building-a-quantum-routine-for-generating-elements-of-bn",
    "title": "A benchmark for testing quantum computers with Clifford+T hardware",
    "section": "Building a quantum routine for generating elements of B^n",
    "text": "Building a quantum routine for generating elements of B^n\nLet’s think about this from the ground up and consider n=2. How can we generate elements of B^2? Enumerating B^2 gives us the set\nB^2 = \\left\\{ 00, 01, 10 \\right\\},\nwhere it’s missing the sole binary sequence 11 \\in \\{ 0, 1\\}^2. One way to think about this is to build a quantum routine that generates the outcomes \\ket {00}, \\ket {01}, \\ket {10} but not \\ket {10}. To achieve this, you want a 2-qubit system such that\n\nqubit 1 admits equal superposition when measured; but\nqubit 2 to be in equal superposition only if the first qubit is in the state \\ket {0}.\n\nAn easy way to do this is to use two X_{\\pi/2} rotation gates together with a single CX_{\\pi/2} controlled rotation gate where X_{\\pi/2} is a 2x2 matrix given by\nX_{\\pi/2} = \\begin{pmatrix} \\cos{\\pi/4} & -i \\sin{\\pi/4} \\\\ -i \\sin{\\pi/4} & \\cos{\\pi/4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} \\end{pmatrix},\nand CX_{\\pi/2} is a 4x4 matrix given by CX_{\\pi/2} = \\begin{pmatrix} I_2 & 0 \\\\ 0 & X_{\\pi/2} \\end{pmatrix}.\nHere I_2 is the 2x2 identity matrix and 0 is a block 2x2 matrix of zeros. For a sanity check, you can evaluate the product\n (CX_{\\pi/2})  (X_{\\pi/2} \\otimes X_{\\pi/2})  (\\ket {0} \\otimes \\ket {0})\nto see that we indeed have a quantum state that yields binary sequences in B^2. Here, \\otimes is the Kronecker product induced by the tensor product of linear maps. If you are lazy, you can skip the sanity check as we are going to prove this using ZX-calculus next anyways which will be much simpler and more elegant, especially for the general case.\nNow we go to n=3 and enumerate B^3. Here we have the set\nB^3 = \\left\\{ 000, 001, 010, 100, 101 \\right\\},\nwhere it’s missing the binary sequences 110 and 111 that has two and three consecutive ones respectively. As before, think about state outcomes. It is slightly complicated as you now have 2^3 potential outcomes. However, if you think inductively, there is a pattern here. In this case, we now want\n\nqubit 1 to be in equal superposition;\nqubit 2 to be in equal superposition only if qubit 1 is in the state \\ket {0}; and\nqubit 3 to be in equal superposition only if qubit 2 is in the state \\ket {0}.\n\nIn this case, we end up with outcomes exactly as in B^3. Observe that the control on qubit 3 is dependent only on the measurement of qubit 2. This means that we can reuse the same principle we had in the case of n=2. In fact, we can simply extend the previous routine by introducing an X_{\\pi/2} rotation gate on qubit 3 and a single CX_{\\pi/2} controlled rotation gate which conditions on qubit 2.\nFurther observe that qubit 2 is dependent only on the measurement of qubit 1. Thus going from the n=2 case to n=3 is simply extending qubit 3 with 2 new rotation gates. It should be clear that we can reuse this same principle to go from n=3 to n=4 and so on; so we conjecture the following:\n\nConjecture: For n &gt; 1, B^n can be generated by an n-bit quantum routine that performs X_{\\pi/2} rotation gates to all qubits n and then applying n-1 controlled rotation gates CX_{\\pi/2} such that the jth controlled rotation gate conditions on qubit j-1 for j &gt; 2.\n\nSo how do we go about proving this conjecture? Well, it’s time to turn to ZX-calculus."
  },
  {
    "objectID": "posts/2024-08-04-quantum-fibonacci/index.html#fibonacci-in-the-zx-calculus",
    "href": "posts/2024-08-04-quantum-fibonacci/index.html#fibonacci-in-the-zx-calculus",
    "title": "A benchmark for testing quantum computers with Clifford+T hardware",
    "section": "Fibonacci in the ZX-calculus",
    "text": "Fibonacci in the ZX-calculus\nA short note before we proceed. It has been incredibly hard to embed tikz files (which I originally wrote all my ZX diagrams in) into a HTML format, so I will be taking screenshots from my original solution for my sanity – I’ve already wasted 3 hours trying, please believe me. I hate low quality screenshots as well so if you have an elegant solution on how to do this, please let me know.\nI claim that the following ZXFibo algorithm is a working n-bit quantum routine that generates B^n for n &gt; 1, and hence the Fibonacci sequence F_n.\n\nAlgorithm. ZXFibo\n\nGiven. An integer n &gt; 1, for the Fibonacci number F_n to be computed.\nPerform.\n\n\n\n\n\nwhere the process \\widehat{f_n} is given by\n\n\n\n\n\nwhere\n\n\n\n\n\nis called the y-box (since it topologically looks like a Y, sorry I’m clearly not creative with my naming).\nMeasuring this circuit yields an outcome of 0 up to a number for Z-measurement outcomes \\ket {k_1 \\ldots k_n} with consecutive ones and not 0 (up to a number) otherwise.\n\nIt shouldn’t be too difficult to see that the process \\widehat{f_n} given above is indeed equivalent to spamming X_{\\pi/2} rotations and putting a control CX_{\\pi/2} gate on each qubit. But if you’re not convinced or your ZX-calculus is rusty, I’ll probably do it as an appendix at some point. What’s important, however, is to recognize that the process only uses Clifford+T gates.\nNow, let’s prove that the ZXFibo algorithm is correct. It’s a counting algorithm, so naturally we prove by induction on n. But before we start, let’s recall some of the spider rules in ZX-calculus. Core to this proof are the copy rule (c), commute rule (\\pi) and the eliminate rule (e) given below:\n\n\n\n\n\nThe commute rule is especially used when k = 0. We can also combine the commute rule, the eliminate rule and phase spider fusion to arrive at what I like to call the (\\alpha \\pi) rule:\n\n\n\n\n\nThe (\\alpha \\pi) rule is an important one and we will be using it several times in our correctness proof of ZXFibo. With these rules refreshed, we are ready to prove the base case.\nBase case. For n=2, just using definitions of \\widehat{f_2} and the y-box \\widehat{y}, and using phase spider fusion, we get\n\n\n\n\n\nWe can then look at individual measurement outcomes. For k, \\ell \\in \\{0, 1\\} we have\n\n\n\n\n\nThen using spider fusion, the colour change rule and the copy rule, we end up with\n\n\n\n\n\nWe can now apply the (\\alpha \\pi) rule to end up with\n\n\n\n\n\nFrom here, we can evaluate each pair (k, \\ell) separately. Recall what these spiders actually mean – the representation of an undoubled phase state \\alpha \\in \\mathbb{C}^2 is given by the matrix\n\n\n\n\n\nwhere the phase effect version is simply attained by taking the adjoint of the phase state matrix. This implies that\n\n\n\n\n\nConsequently, the undoubled number\n\n\n\n\n\ncan be tabulated into the following table of numbers and their corresponding complex amplitudes\n\n\n\n\n\nIn particular the measurement outcome pair (k, \\ell) = (1, 1) attains a zero complex amplitude. Moreover, where k and \\ell are not both 1, the amplitude is nonzero. This is exactly what we want and so completes the base case. In fact, you can actually get the actual probabilities pretty easily from here, but we leave it as an exercise for the reader.\n\nExercise. Show that the actual probabilities for each measurement outcome are tabulated as follows:  Hint: look at the complex amplitudes of the doubled version.\n\nInductive step. Now suppose that the ZXFibo algorithm is correct for all 2 \\leq n &lt; N + 1. Then for n = N+1, we can look at individual measurement outcomes k_1, \\ldots, k_{N+1} \\in \\{0, 1\\} to have\n\n\n\n\n\nWe can apply the copy rule on the N-th measurement so that we have\n\n\n\n\n\nThe inductive hypothesis takes care of the evaluated \\widehat{f_N}, so we know that it will not be 0 up to a number for Z-measurement outcomes \\ket {k_1 \\ldots k_N} with no consecutive ones. So now we focus on the rightmost y-box. To complete the inductive step, we just need to prove that if k_N = k_{N+1} = 1, then the y-box evaluates to 0, and not 0 otherwise.\nUsing the y-box definition, applying phase spider fusion and using the colour change rule, we have\n\n\n\n\n\nWe can then apply the eliminate rule and the (\\alpha \\pi) rule to further obtain\n\n\n\n\n\nThis number is exactly what we had before with \\ell = k_{N+1} and k = k_N. Thus by using the same logic as we did before, we can conclude that the pair (k_N, k_{N+1}) = (1, 1) which gives the only remaining measurement outcome with consecutive ones will evaluate the whole diagram to 0 (up to a number) and not 0 otherwise. This completes the inductive step and the correctness proof of the ZXFibo algorithm. As a bonus, we have also implicitly proven that the algorithm terminates for all qubits n &gt; 1. \\blacksquare"
  },
  {
    "objectID": "posts/2024-08-04-quantum-fibonacci/index.html#zxfibo-on-an-ibm-quantum-computer",
    "href": "posts/2024-08-04-quantum-fibonacci/index.html#zxfibo-on-an-ibm-quantum-computer",
    "title": "A benchmark for testing quantum computers with Clifford+T hardware",
    "section": "ZXFibo on an IBM quantum computer",
    "text": "ZXFibo on an IBM quantum computer\nNow that we have proven the correctness of the ZXFibo algorithm, let’s see how it performs on an IBM quantum computer simulator. We will implement ZXFibo using the IBM Qiskit library (Javadi-Abhari et al. 2024) and the PyZX library (Kissinger and Wetering 2020) which we import below.\n\n\nPyZX and IBM Qiskit base imports\nimport math\nfrom fractions import Fraction\n\nimport pyzx as zx\nfrom pyzx import print_matrix\nfrom pyzx.basicrules import *\nzx.settings.drawing_backend = 'matplotlib'\n\nimport qiskit\nfrom qiskit.test.mock import FakeAthens\nfrom qiskit import QuantumCircuit, Aer, IBMQ, execute\nfrom qiskit.compiler import assemble\nfrom qiskit.tools.monitor import job_monitor\n\nimport matplotlib.pyplot as plt\n\n\nWe can then draw ZXFibo using PyZX as follows.\n\ndef add_cx_alpha_gate(\n    circuit: zx.Circuit, alpha: Fraction | int, control: int, target: int\n) -&gt; zx.Circuit:\n    circuit.add_gate(\"HAD\", target)\n    circuit.add_gate(\"ZPhase\", control, phase=alpha * Fraction(1, 2))\n    circuit.add_gate(\"ZPhase\", target, phase=alpha * Fraction(1, 2))\n    circuit.add_gate(\"CNOT\", control, target)\n    circuit.add_gate(\"ZPhase\", target, phase=alpha * Fraction(-1, 2))\n    circuit.add_gate(\"CNOT\", control, target)\n    circuit.add_gate(\"HAD\", target)\n    return circuit\n\n\ndef zxfibo(n: int, graph: bool = False) -&gt; zx.Circuit:\n    circ = zx.Circuit(n)\n    # For each qubit, add an X(\\pi/2) gate.\n    for i in range(n):\n        circ.add_gate(\"XPhase\", i, Fraction(1, 2))\n    # For each qubit n &gt; 1, add a controlled X(\\pi/2) gate.\n    for i in range(n - 1):\n        add_cx_alpha_gate(circ, Fraction(-1, 2), i, i + 1)\n    if graph:\n        return circ.to_graph()\n    return circ\n\nGiven a PyZX circuit, we can convert it into a Qiskit circuit using the following function (I believe this was Aleks’s code).\n\ndef pyzx_to_qiskit(circ: zx.Circuit) -&gt; qiskit.QuantumCircuit:\n    # converts all gates to CNOT, CZ, HAD, ZPhase, and XPhase\n    circ = circ.to_basic_gates()\n    q = circ.qubits\n    ibm_circ = QuantumCircuit(q, q)\n    for g in circ.gates:\n        if isinstance(g, zx.gates.CNOT): ibm_circ.cnot(g.control, g.target)\n        elif isinstance(g, zx.gates.CZ): ibm_circ.cz(g.control, g.target)\n        elif isinstance(g, zx.gates.HAD): ibm_circ.h(g.target)\n        elif isinstance(g, zx.gates.ZPhase): ibm_circ.rz(math.pi * g.phase, g.target)\n        elif isinstance(g, zx.gates.XPhase): ibm_circ.rx(math.pi * g.phase, g.target)\n    \n    # measure everything\n    ibm_circ.measure(range(q), range(q))\n    return ibm_circ\n\nFor the base case n=2 of ZXFibo, we can then obtain its ZX-diagram and Qiskit quantum circuit by running:\nzxfibo2_pyzx = zxfibo(2)  # PyZX\nzxfibo2_ibm_circ = pyzx_to_qiskit(zxfibo2_pyzx)  # Qiskit\n\n\n\n\n\n\n\n\n\n\n\n(a) PyZX\n\n\n\n\n\n\n\n\n\n\n\n(b) Qiskit\n\n\n\n\n\n\n\nFigure 1: ZXFibo for n=2 in its PyZX and Qiskit circuit representation.\n\n\n\nUsing the Qiskit API, we can then run ZXFibo using an IBM backend. To not spend any money, I will be running the algorithm on a non-noisy simulator where we can get ideal counts (suppressing probability 0 events) and on a noisy simulator. For the noisy simulator, I will be using the the FakeAthens backend, which is a 5 qubit fake backend that mimics the IBM Athens device.\n\ntry:\n    IBMQ.load_account()\n    provider = IBMQ.get_provider(hub=\"ibm-q\", group=\"open\", project=\"main\")\n    backend = provider.get_backend('ibmq_qasm_simulator')\nexcept:\n    print(\"Error:\", sys.exc_info()[1])\n    print(\"Setting backend to qasm_simulator\")\n    backend = FakeAthens()\n\nWe then run the ZXFibo algorithm for 1000 shots.\n\ndef run(\n    circ: qiskit.QuantumCircuit, backend: qiskit.providers.Backend, shots: int\n) -&gt; dict[str, int]:\n    job = execute(circ, backend, shots=shots)\n    result = job.result()\n    counts = result.get_counts()\n    return counts\n\ncounts = run(zxfibo2_ibm_circ, backend=backend, shots=1000)\nqiskit.visualization.plot_histogram(counts)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Ideal counts\n\n\n\n\n\n\n\n\n\n\n\n(b) Counts on a noisy simulator\n\n\n\n\n\n\n\nFigure 2: Simulation results for ZXFibo with n=2 using the IBM backend with 1000 shots.\n\n\n\nFrom the simulation results for n=2, we can see that the algorithm works as expected. In fact, it works too well. On the non-noisy simulator, the histogram depicts exactly the probabilities of 0.25, 0.25, 0.5 for the pairs (0, 0), (0, 1) and (1, 0) respectively. On the noisy simulator otoh, we attain a similar distribution but with ~0.5% noise in the (1, 1) measurement outcome which is tiny relative to the minimum probability for the measurement outcomes with non-consecutive ones of 25%. Setting a 5% threshold in post-processing discards the 0.5% noise and gives us the correct distribution and yields B^2. We then obtain F_2 as |B^2|.\nWe repeat the experiment by running 1000 shots of the ZXFibo algorithm but now with n=3.\n\n\n\n\n\n\n\n\n\n\n\n(a) Ideal counts\n\n\n\n\n\n\n\n\n\n\n\n(b) Counts on a noisy simulator\n\n\n\n\n\n\n\nFigure 3: Simulation results for ZXFibo with n=3 using the IBM backend with 1000 shots.\n\n\n\nWe see again that the ZXFibo algorithm works extremely well! The noisy measurement outcomes (i.e. with consecutive ones) accounts for only about 1% of the total probabilities with a single sequence attaining a maximum of only 0.7%. This number is significantly tiny compared to the minimum probability for elements in B^3 of 12.5%. We can again set a 5% threshold in post-processing to get the correct probability distribution and obtain F_3.\nLet’s now fire 1000 shots of ZXFibo with n=5 to compute the Fibonacci number F_5.\n\n\n\n\n\n\n\n\n\n\n\n(a) Ideal counts\n\n\n\n\n\n\n\n\n\n\n\n(b) Counts on a noisy simulator\n\n\n\n\n\n\n\nFigure 4: Simulation results for ZXFibo with n=5 using the IBM backend with 1000 shots.\n\n\n\nAgain, ZXFibo works really well where the noisy measurement outcomes attains less than 1% in probability on average. But we can now start to see an impending doom… the minimum probability for elements in B^5 is now 3.9%. This is still significantly larger than the maximum attained by the noisy measurement outcome of 0.7% but we can no longer set a 5% threshold in post-processing. Rather, we need to choose a safer threshold, say, at about 2.5% to discard the noisy outcomes and obtain F_5 with confidence. This phenomenon can seem problematic as we increase n so let’s discuss a bit more about it.\n\nAn impending doom\nBecause this is a heuristic approach, we have no reason to conclude that the maximum attained by the noisy measurement outcomes will exceed beyond 1% as we have seen this is not the case even for increasing n = 2, 3, 5. The only way to verify this is to run more experiments with higher number of qubits. However, the doom pattern that we can clearly see is that as n increases, the minimum probability for elements in B^n decreases. This means that the threshold for discarding the noisy measurement outcomes will have to be set lower and lower, and might eventually reach a point where the threshold is lower than the maximum attained by the noisy measurement outcomes. What is this breaking point though?\nFix n &gt; 1 and define p_{\\mathrm{min}} be the expected probability for the element B^n with lowest probability (relative to other elements in B^n). Assume an error of \\varepsilon &gt; 0 on p_{\\mathrm{min}}. Suppose the error arising from the noisy measurement outcomes is \\delta \\approx 0. Then a reasonable post-processing probability threshold \\tau for discarding noisy outcomes should satisfy \\delta &lt; \\tau &lt; p_{\\mathrm{min}} - \\varepsilon. The problem is that the Fibonacci sequence F_n behaves like an exponential function, which equivalently means that the number of elements in B^n increases exponentially with n. This implies that p_{\\mathrm{min}} will decrease exponentially fast as n increases! Obviously, if \\delta &lt; p_{\\mathrm{min}} - \\varepsilon, then we can always take the midpoint\n \\tau = \\frac{p_{\\mathrm{min}} - \\varepsilon + \\delta}{2} \nto be the count threshold. However, the impending doom is when p_{\\mathrm{min}} - \\varepsilon \\approx \\delta for some sufficiently large N. In this case, there does not exist a stable count threshold \\tau for any n &gt; N + 1 as for n = N, we have\n\\tau = \\frac{p_{\\mathrm{min}} - \\varepsilon + \\delta}{2} \\approx \\frac{2 \\delta}{2} = \\delta.\nSo if we are serious in counting large Fibonacci numbers, we ought to think of a better algorithm and this opens room for further research. For our purpose, however, we just want a benchmark for running Clifford+T hardware. This heuristic is excellent in doing exactly that, especially as per our discussion, for benchmarking how noisy a quantum computer is by looking at the noisy measurement outcomes. In fact, one can use the probability threshold \\tau above as a metric to measure the noise levels."
  },
  {
    "objectID": "posts/2021-05-03-unit-testing-with-pytest/index.html",
    "href": "posts/2021-05-03-unit-testing-with-pytest/index.html",
    "title": "Unit Testing with pytest",
    "section": "",
    "text": "It is a common scene that we want to test our functions after we’ve done writing them. In Python, we can usually do this by printing to the console. For example, suppose we are interested in testing the following function:\nwhich returns the sum of the two integers x and y if the output is strictly positive, or else return None. Since we are adding two positive integers, we expect that the add_positive function returns a positive integer as well. So we can try a few pairs, say, add_positive(2, 3) and add_positive(1, 1), and we expect these to return 5 and 2 respectively; both greater than 0. If we try add_positive(-1, -5), then this should return None as it yields -6 instead which is less than 0. If we want to test our function with multiple inputs, we can do something like a for loop. So maybe something like:\nwhich should yield\nand then we eyeball each of the output to see if it as expected.\nThis is of course a correct way to do it as we can evaluate whether the output on the console is as expected; but is it efficient?"
  },
  {
    "objectID": "posts/2021-05-03-unit-testing-with-pytest/index.html#why-unit-testing-is-a-no-brainer",
    "href": "posts/2021-05-03-unit-testing-with-pytest/index.html#why-unit-testing-is-a-no-brainer",
    "title": "Unit Testing with pytest",
    "section": "🧠 Why unit testing is a no-brainer",
    "text": "🧠 Why unit testing is a no-brainer\nIn a typical life cycle of a function, there are three situations where we would want to test our functions:\n\nThe first time it is implemented.\nWhen a test fails, we fix the bug, and then we have to redo testing.\nWhen implementing a new feature or refactoring code.\n\nImagine how many times we would have to manually test this function. If it would take 3 minutes per (manual) test, and if we would have to do it 100 times over the function’s whole life cycle, then we would have effectively spent 3 x 100 = 300 minutes, which is roughly 5 hours of testing! Now if we have 10 functions to test, it would take us 50 hours or roughly 2 days worth of time to test these functions! I don’t know about you, but that sounds like a lot of time to me for only 10 functions. This is why we would want to automate the testing process by writing unit tests, which, together with the planning phase, requires about an hour to write in perpetuity (in theory).\n\nManually testing a function throughout its whole life cycle may take you 5 hours; whereas writing a properly planned unit test may take you only an hour, and this is one-fifth of the former.\n\nThere are a variety of Python libraries to do unit testing, some of which are:\n\npytest\nunittest\nnosetests\ndoctest"
  },
  {
    "objectID": "posts/2021-05-03-unit-testing-with-pytest/index.html#basic-unit-testing-procedure",
    "href": "posts/2021-05-03-unit-testing-with-pytest/index.html#basic-unit-testing-procedure",
    "title": "Unit Testing with pytest",
    "section": "🧪 Basic unit testing procedure",
    "text": "🧪 Basic unit testing procedure\nIn this post, we will look at pytest because it is simply the most popular (hence, a lot of support, say, on StackOverflow) and easiest to use. We start by installing pytest if it’s not already installed:\npip install pytest\n\nStep 1: Creating the test module\nAssume that the add_positive() function lies in a module called add_positive.py. We begin the unit testing process by creating a file called test_add_positive.py in the same directory as add_positive.py. Such a file will contain the unit tests of functions in add_positive.py, and is called a test module. The test_ in front of test_add_positive.py is important as it lets pytest knows that this is a file containing unit tests.\n_Remark: The test module for the module add_positive.py does not have to be named test_add_positive.py. You can name it test_x.py or test_covid.py or whatever as long as it is prepended with test_. But it is good practice to follow the naming convention of test*module_name to trace which module is this test module testing.*\n\n\nStep 2: Importing inside the test module\nThe next step is to (mainly) import two things:\n\nThe pytest module;\nand the module (or function) we want to test.\n\nSo at the top of our test module test_add_positive.py, we would have:\nimport pytest\nfrom add_positive import add_positive\n\n\nStep 3: Begin writing unit tests\nA unit test is basically a Python function, no more and no less. The only thing special about a unit test is that it is prepended by test_ in its name. This is just to tell pytest to use it as part of the testing procedure. Here is an example of a unit test declaration:\ndef test_for_positive_pairs():\n    ...\nInside the body of a unit test are assertions, and this is the actual testing process. We want to test if our add_positive() function returns correctly if positive pairs (x, y) are passed into the function. So we assert the following:\ndef test_for_positive_pairs():\n    assert add_positive(3, 5) == 8\n    assert add_positive(99999, 99999) == 199998\nHow about if we want to add a second unit test which tests when a zero pair i.e. (0, 0) is passed into the function? Well we just write another function right below it. We expect add_positive(0, 0) to return None, so we write exactly that:\ndef test_for_positive_pairs():\n    assert add_positive(3, 5) == 8\n    assert add_positive(99999, 99999) == 199998\n\ndef test_for_zero_pair():\n    assert add_positive(0, 0) is None\nI would like to add two more tests which test the add_positive function when negative and mixed pairs are fed into the function. The whole test module should now look something like this:\nimport pytest\nfrom add_positive import add_positive\n\ndef test_for_positive_pairs():\n    assert add_positive(3, 5) == 8\n    assert add_positive(99999, 99999) == 199998\n\ndef test_for_negative_pairs():\n    assert add_positive(-1, -6) is None\n    assert add_positive(-99, -99) is None\n\ndef test_for_zero_pair():\n    assert add_positive(0, 0) is None\n\ndef test_for_mixed_pairs():\n    assert add_positive(-1, 2) == 1\n    assert add_positive(-9, 100) &gt; 0\n    assert add_positive(-900, 2) is None\nRemark: Note that you can get as creative as you’d like with the values you’re testing. It is good practice to always consider edge cases in the unit test, but this is a topic of a later post.\n\n\nStep 4: Running pytest and reading the test report\nOnce you’re done with writing the test module, testing is as easy as opening your terminal, cd-ing into the directory containing the test module, and executing:\npytest test_add_positive.py\nIf the implementation of add_positive() is correct (with respect to our tests), you should see a test report like this.\n\n\n\nPass test\n\n\nJust ignore everything above “collected 4 items” as it is not too important. The breakdown of this test report is the following:\n\n“Collected 4 items” means that we will be executing 4 unit tests.\nThe 4 green dots indicates that we have pass all 4 of them, and they are sequential (see what happens when 1 fail below).\nThe obvious “4 passed in 0.02s” message is the summary."
  },
  {
    "objectID": "posts/2021-05-03-unit-testing-with-pytest/index.html#why-unit-tests-are-important-in-prod",
    "href": "posts/2021-05-03-unit-testing-with-pytest/index.html#why-unit-tests-are-important-in-prod",
    "title": "Unit Testing with pytest",
    "section": "📌 Why unit tests are important in prod",
    "text": "📌 Why unit tests are important in prod\nThree months after the add_positive.py code has been deployed, your colleague Gilfoyle modified the add_positive() function returning only the sum:\ndef add_positive(x, y):\n    \"\"\"Add two positive integers x and y. If the sum\n     is negative or zero, return None.\"\"\"\n    return x + y\nApparently, he didn’t read the short description of the function; he thought that you’re adding an unnecesary check. However, the function is supposed to work like that – only returning if the sum is positive (think of another module which relies on the correctness of this function).\nIf there were no unit tests written to check this modification, this seemingly trivial edit by Gilfoyle could have been merged into the main branch and could possibly crash the whole system! Thankfully, we implemented continuous integration and upon making a pull request, Gilfoyle was bombarded with the following test report:\n\n\n\nFail test\n\n\nHere is a breakdown of the report:\n\n“Collected 4 items” means the same as before.\nThe first green dot means we passed the first unit test, but the subsequent F’s means we failed the second, third and fourth unit test;\nthis is reflected in the FAILURES section below it, showing which exact lines contribute to these failures.\nThe “3 failed, 1 passed in 0.19s” message is the summary.\n\nSometimes, we just want to run the test up until the first failure. This can be done by adding a -x flag like so:\npytest -x test_add_positive.py\nNow you might be wondering, what to do if we have multiple functions to test in the same module given that this one single add_positive function alone requires 4 unit tests. Things can start getting really messy right? To solve this issue, we can create a class containing these unit tests for each function we want to test. This will be a topic of a future post on testing."
  },
  {
    "objectID": "posts/2024-08-30-searching-song-in-deep-playlist/index.html",
    "href": "posts/2024-08-30-searching-song-in-deep-playlist/index.html",
    "title": "Searching for my favorite song in a deep playlist",
    "section": "",
    "text": "TLDR; Keep your playlists small. There’s a 50% chance of finding your favourite song in less than or equal to 4 steps in a playlist of length 9. With each additional song in your playlist, you decrease the speed of finding your favourite song by roughly 10%.\nI love music. I listen to music all the time and I especially listen to music when I go running, where I would put it on shuffle mode. Now a few runs ago, I was aggressively double tapping my airpods to jump to the next song, skipping a lot of songs in the process until it reaches a song that I was keen on listening to at that moment. That’s when I started thinking, given that I started at a particular song in the playlist, can I quantify how many steps do I need to take to reach a particular favourite song in my playlist?\nNow it is important to note that modern music streaming services like Spotify do not necessarily implement naive shuffle functions which samples the next song uniformly at random. It is more likely that they implement a weighted shuffle function which samples the next music with replacement, assuming some weight on the song based on, for example, popularity of the song (global recommendation approach) or occurrence of listening to the song (a personalized approach). For the sake of this problem, however, we will assume that the songs in the playlist are shuffled uniformly at random.\nThere are many ways to approach this problem (which is the beauty of this problem). For example, one can form a complete digraph for the songs in the playlist, remove the directed path from your favourite song to any other node (but keep the inverse so that your favourite song is a sink) and then count the number of all possible paths between any node (song) and the target favourite song. Note that this is different to finding a shortest path which we have Dijsktra’s algorithm for. That algorithm runs in O\\left(|E| + |V| \\log |V| \\right) time where E is the multiset of edges and V the set of nodes. Rather, the problem of finding the number of paths is much harder and would not have an algorithm that enjoy such speeds.\nInstead of the graph approach, I will go down the dark hole known as combinatorics. The downside is that the setup will be less natural and will require some level of abstraction. The upside, however, is that once everything is set up nicely, solving the problem essentially amounts to careful counting. Let’s start seeing an example on how we can do this."
  },
  {
    "objectID": "posts/2024-08-30-searching-song-in-deep-playlist/index.html#painting-paths-in-a-playlist",
    "href": "posts/2024-08-30-searching-song-in-deep-playlist/index.html#painting-paths-in-a-playlist",
    "title": "Searching for my favorite song in a deep playlist",
    "section": "Painting paths in a playlist",
    "text": "Painting paths in a playlist\nSuppose a playlist contains three songs s_1, s_2, s_T. Let’s ask the most basic question of how many paths can I reach the target song s_T? Now the answer is not 3!=6 because you can reach s_T without visiting all the other songs in the playlist. For example, you can reach s_T via the route s_1 \\to s_T or s_2 \\to s_T respectively. In total, you can reach s_T in 5 different ways. These are:\n\ns_T \\to s_T\ns_1 \\to s_T\ns_2 \\to s_T\ns_1 \\to s_2 \\to s_T\ns_2 \\to s_1 \\to s_T\n\nwhere the first path s_T \\to s_T is the tautological path of starting and ending at s_T.\nHow about if the playlist contain four songs s_1, s_2, s_3, s_T, how many paths can I reach the target song s_T? You should be convinced by now that the answer is not 4!=24, but how many? The answer is that there are 16 paths in total. For your sanity, I will enumerate this long list below:\n\ns_T \\to s_T\ns_1 \\to s_T\ns_2 \\to s_T\ns_3 \\to s_T\ns_1 \\to s_2 \\to s_T\ns_2 \\to s_1 \\to s_T\ns_3 \\to s_2 \\to s_T\ns_2 \\to s_3 \\to s_T\ns_1 \\to s_3 \\to s_T\ns_3 \\to s_1 \\to s_T\ns_1 \\to s_2 \\to s_3 \\to s_T\ns_2 \\to s_1 \\to s_3 \\to s_T\ns_3 \\to s_2 \\to s_1 \\to s_T\ns_2 \\to s_3 \\to s_1 \\to s_T\ns_1 \\to s_3 \\to s_2 \\to s_T\ns_3 \\to s_1 \\to s_2 \\to s_T.\n\nNow you should start to see a pattern. In both cases where the playlist is of length three and four, we observe that the target song s_T remains fixed and the only thing that changes is the order of the songs prior to s_T. Further, the number of songs that are reordered is progressively unconstrained to acknowledge that you can reach the target song in an increasing number of steps. Thus, the number of paths to reach the target song s_T is just a sum of k-permutations of n-1 songs where k is the number of steps allowed to reach s_T and n is the length of the playlist. This sum is done from 0, which represents the tautological path s_T \\to s_T, up to n-1 which represents a permutation of the path s_1 \\to s_2 \\to \\cdots \\to s_{n-1} \\to s_T. This argument essentially is a proof of the following result.\n\n\nProposition 1 (Number of paths given playlist) The number of paths to reach the target song s_T in a playlist \\mathcal{P} of length n is given by: N(n; \\mathcal{P}) = \\sum_{j=0}^{n-1} P(n-1, j).\nwhere P(m, k) is the k-permutation of m objects.\n\n\nIn the case of n=4 that we have just enumerated above, the number of steps is given by:\n\nN(4) = P(3, 0) + P(3, 1) + P(3, 2) + P(3, 3) = 1 + 3 + 6 + 6 = 16\n\nwhich agrees with our brute-force enumeration. Similarly, you can check that N(3) = 5 as we have enumerated before.\nSo this is a good result, but it has not answer our main question:\n\nGiven a song s in playlist \\mathcal{P}, how many ways can you reach your favourite song s_T \\in \\mathcal{P} in less than or equal to t \\in \\mathbb{Z}^+ steps?\n\nIt has, however, answered the easier question:\n\nGiven a playlist \\mathcal{P}, how many ways can you reach your favourite song s_T \\in \\mathcal{P} in less than or equal to t \\in \\mathbb{Z}^+ steps?\n\nThis question is answered by using Proposition 1 and simply truncating the summation at t \\leq n-1.\nThe difference between the main question and this easier question is that the former essentially wants N | s, \\mathcal{P} whereas what we have at the moment gives N | \\mathcal{P}. So let’s move towards a solution, suppose we start at a song s \\in \\mathcal{P}, what then?"
  },
  {
    "objectID": "posts/2024-08-30-searching-song-in-deep-playlist/index.html#answering-the-main-question",
    "href": "posts/2024-08-30-searching-song-in-deep-playlist/index.html#answering-the-main-question",
    "title": "Searching for my favorite song in a deep playlist",
    "section": "Answering the main question",
    "text": "Answering the main question\nI have been using the term playlist quite loosely throughout this article so let’s make it concrete. A playlist \\mathcal{P} of length n is simply a finite set of songs \\{ s_1, s_2, \\ldots, s_{n-1}, s_T\\}. It does not assume any ordering, and our logic does not assume ordering unless mentioned otherwise. This definition aligns with what we had before and so the result(s) remain.\nAs per Proposition 1, we know that N(4) = 16 and reaching the target song in less than or equal to k \\leq n-1 steps is given by \\sum_{j=0}^{k} P(n-1, j). These makes no assumption on where we start the path at. Suppose now you start at song s_3 in the playlist. How many paths can you take take to reach your target song s_T? Let’s start enumerating all the possible paths to reach s_T again, but this time we highlight how many steps it takes to reach s_T from s_3.\n\ns_T \\to s_T (impossible)\ns_1 \\to s_T (impossible)\ns_2 \\to s_T (impossible)\ns_3 \\to s_T (1)\ns_1 \\to s_2 \\to s_T (impossible)\ns_2 \\to s_1 \\to s_T (impossible)\ns_3 \\to s_2 \\to s_T (2)\ns_2 \\to s_3 \\to s_T (1)\ns_1 \\to s_3 \\to s_T (1)\ns_3 \\to s_1 \\to s_T (2)\ns_1 \\to s_2 \\to s_3 \\to s_T (1)\ns_2 \\to s_1 \\to s_3 \\to s_T (1)\ns_3 \\to s_2 \\to s_1 \\to s_T (3)\ns_2 \\to s_3 \\to s_1 \\to s_T (2)\ns_1 \\to s_3 \\to s_2 \\to s_T (2)\ns_3 \\to s_1 \\to s_2 \\to s_T (3).\n\nTo argue easier, we first make the following definition.\n\n\nDefinition 1 (t-path) Let \\mathcal{P} be a playlist of length n. A t-path from s_j \\in \\mathcal{P} is a path that starts at s_j and ends at s_T \\in \\mathcal{P} in t \\in \\mathbb{Z}^+ steps.\nIt is trivially non-unique as we have seen. So we denote the number of t-paths from s_j by N_t(n; s_j). If the dependence on s_j is obvious, we simply write N_t(n).\nHere are some examples. s_1 \\to s_T is a 1-path from s_1. Another example is that s_3 \\to s_4 \\to s_1 \\to s_T is a 2-path from s_4 and a 1-path from s_1.\n\n\nBack to our enumeration of the playlist of length 4, we observe firstly that there are some impossible paths where s_3 is not visited at all. Then you have five 1-paths to reach s_T from s_3, four 2-paths and two 3-paths.\nJust focusing on 1-paths here, it is easy to see that the number of 1-paths to reach s_T from s_3 is given by N(n-1) because we can “group” s_3 \\to s_T together and treat it as a single end point s_T'. It is also easy to see that the number of 3-paths is essentially a permutation on songs that are left, which in this case is 2! = 2. It is tempting now to say that the case of 2-paths is simply N(n-1) - 1 since we are discarding the unique s_3 \\to s_T case which exists only for 1-paths and that everything else is just a permutation of the remaining songs; and you are not wrong to think so as we shall see! The problem is that it does not give a strong explanation for what a general pattern would be.\nSo let’s look at a playlist of length n=5 and count each t-path from s_4. At this point enumeration of the playlist can get a bit crazy as we know by Proposition 1 that there are N(5) = 65 paths to reach s_T from s_4. I highly recommend you enumerate these paths by writing code (I’ve provided a fugly python code to do this in the appendix. See Listing 1). Now if we focus on each t-paths, we see that there are\n\n16 counts of 1-paths,\n15 counts of 2-paths,\n12 counts of 3-paths,\n6 counts of 4-paths.\n\nAs we have observed before, the number of 1-paths is indeed N(n-1) = N(4) = 16. Discard the unique s_4 \\to s_T path and we have 15 count of 2-paths. And that the number of 4-paths is just a permutation on songs that are left giving us (5-2)! = 3! = 6. So how do we explain the 12 counts of 3-paths? Now a 3-path from s_4 would look like the following\n\n\\textcolor{blue}{\\rule{1.8ex}{1.8ex}} \\to s_4 \\to \\rule{1.8ex}{1.8ex} \\to \\rule{1.8ex}{1.8ex} \\to s_T,\n\nwhere the \\textcolor{blue}{\\rule{1.8ex}{1.8ex}}, \\rule{1.8ex}{1.8ex} are placeholders for other songs. Let’s start counting and consider parity. If we assume \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} is null, i.e., we start at s_4, then there are P(3, 2) possible ways to fill in \\rule{1.8ex}{1.8ex}, and hence, P(3, 2) number of steps to reach s_T. On the other hand, if we assume \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} is not null, i.e., the path starts at one of s_1, s_2, s_3, then there are P(3, 3) possible ways to fill in \\textcolor{blue}{\\rule{1.8ex}{1.8ex}}, \\rule{1.8ex}{1.8ex}. So in total, the number of 3-paths is P(3, 2) + P(3, 3) = 12 which is what we observed in the enumeration!\nIn fact, we can run this same argument for all the other t-paths as well. For a 4-path from s_4, we would have the following sequence\n\ns_4 \\to \\rule{1.8ex}{1.8ex} \\to \\rule{1.8ex}{1.8ex} \\to \\rule{1.8ex}{1.8ex} \\to s_T.\n\nIn this case, it is trivial to see that there are P(3, 3) = 6 ways to fill in the \\rule{1.8ex}{1.8ex} boxes.\nFor 2-paths, we have the following sequence\n\n\\textcolor{green}{\\rule{1.8ex}{1.8ex}} \\to \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} \\to s_4 \\to \\rule{1.8ex}{1.8ex} \\to s_T.\n\nIf we assume both \\textcolor{green}{\\rule{1.8ex}{1.8ex}}, \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} to be null, then there are P(3, 1) possible ways to fill in \\textcolor{black}{\\rule{1.8ex}{1.8ex}}. The case where \\textcolor{blue}{\\rule{1.8ex}{1.8ex}} is null but \\textcolor{green}{\\rule{1.8ex}{1.8ex}} is not null is impossible, so suppose only \\textcolor{green}{\\rule{1.8ex}{1.8ex}} is null. Then there are P(3, 2) possible ways to fill in \\textcolor{blue}{\\rule{1.8ex}{1.8ex}}, \\textcolor{black}{\\rule{1.8ex}{1.8ex}} with songs. And finally if \\textcolor{green}{\\rule{1.8ex}{1.8ex}} not null, then there are P(3, 3) ways to fill in \\textcolor{green}{\\rule{1.8ex}{1.8ex}}, \\textcolor{blue}{\\rule{1.8ex}{1.8ex}}, \\textcolor{black}{\\rule{1.8ex}{1.8ex}}. Thus in total, we have P(3, 1) + P(3, 2) + P(3, 3) = 15 paths to reach s_T.\nThe 1-path case is just considering an additional box \\textcolor{red}{\\rule{1.8ex}{1.8ex}} that can take a null value and running the same argument should lead you to 16 steps in total. It should not be difficult to see how we can generalize this pattern to arbitrary playlists of length n so I leave the proof as an exercise for you to do.\nSuch a proof would yield the following result.\n\n\nTheorem 1 (Number of t-paths) Let \\mathcal{P} be a playlist of length n. Given a starting song s \\in \\mathcal{P} and a target song s_T \\in \\mathcal{P}, the number of t-paths from s to s_T is given by:\n\n% N_{\\text{t-path}}(t, n; s, s_T) = \\sum_{j=0}^{n-1-t} P(n-2, n-2-j).\nN_{t}(n) = \\sum_{j=0}^{n-1-t} P(n-2, n-2-j).\n\n\n\nThis is a great result as it gives us a way to compute the number of paths to reach a song in a playlist of length n in exactly t steps. As a consequence, we can now compute the number of paths to reach a song in a playlist of length n in less than or equal to t steps.\n\n\nCorollary 1 (Number of paths to reach favourite song in \\leq t steps) Let \\mathcal{P} be a playlist of length n. Given a starting song s \\in \\mathcal{P} and a target song s_T \\in \\mathcal{P}, the number of paths to reach s_T in less than or equal t \\in \\mathbb{Z}^+ steps is given by:\n\nN_{\\leq t}(n) = \\sum_{\\ell=1}^{t} N_{\\ell}(n).\n\n\n\nWhat’s great about Corollary 1? Well it gives a satisfying answer to our main question of\n\nGiven a song s in playlist \\mathcal{P}, how many ways can you reach your favourite song s_T \\in \\mathcal{P} in less than or equal to t \\in \\mathbb{Z}^+ steps?\n\nwhich is all that we wanted. To see this in practice, fix t=4 to consider the number of paths to reach your favourite song in less than or equal to 4 steps. Then we look at N_{\\leq t} with varying n in the range 5 \\leq n \\leq 14. We will also consider the probability of reaching your favourite song in less than or equal to t steps by computing N_{\\leq t}/N(n). The numbers are tabulated below.\n\n\n\n\n\n\n\n\n\nn\nN_{\\leq t}\n\\lfloor \\log N_{\\leq t}\\rfloor\nN_{\\leq t}/N(n)\n\n\n\n\n5\n49\n1\n0.75\n\n\n6\n237\n2\n0.73\n\n\n7\n1271\n3\n0.65\n\n\n8\n7783\n3\n0.57\n\n\n9\n54741\n4\n0.50\n\n\n10\n438329\n5\n0.44\n\n\n11\n3945547\n6\n0.40\n\n\n12\n39456291\n7\n0.36\n\n\n13\n434020313\n8\n0.33\n\n\n14\n5208245221\n9\n0.31\n\n\n\nIt is not suprising to see that with increasing number of playlist length n, the number of paths N_{\\leq t} increases dramatically. In fact, as we have computed with \\lfloor \\log N_{\\leq t} \\rfloor, the order of magnitude increases roughly by one at every increasing length n. The probability, however, does not decrease as dramatically, so you still have a good chance of finding your favourite songs with ease.\nToday, I have a playlist with 53 songs. The number of possible paths to reach my current favourite song in this playlist in less than or equal to t=4 steps given an abitrary starting song is\n\n16865511683372560412495381946812946399349813999270471596889656524800 \\sim 10^{67}\n\npaths which is on the order of 67. This is an exorbitantly huge number with 67 zeros. If we were to compute N_{\\leq t}/N(n), we get 7.7% which is honestly not too bad. But still… an 8% chance of finding my favourite song is quite low. Moral of the story? Try to make your playlists small."
  },
  {
    "objectID": "posts/2024-08-30-searching-song-in-deep-playlist/index.html#what-next",
    "href": "posts/2024-08-30-searching-song-in-deep-playlist/index.html#what-next",
    "title": "Searching for my favorite song in a deep playlist",
    "section": "What next?",
    "text": "What next?\nHere are some key directions we can take to explore.\n\nWhat if instead of a single target favourite song s_T, we have a set of target songs \\{s_{T_1}, s_{T_2}, ...\\}. How will the analysis differ when we start at any arbitrary song? This seems easy… I think.\nSimilarly, what if instead of a single starting song s, we have a set of arbitrary starting songs \\{s_1, s_2, ...\\}. How will the number of paths change?\nDo a precise analysis on the order of growth in the number of paths. If you divide N_{\\leq t} for n with n-1 successively, you will see that the growth is factorial. This is kind of expected but can you prove it? It is easy to see that the growth in the total number of paths is factorial by using Proposition 1, but how about for paths up to t steps?\nDo a precise analysis on N_{\\leq t}/N(n). Seems like there’s approximately a ~90% drop for the numbers that we have tabulated as n increases. Finding a bound would be excellent.\nI’m pretty sure we can try solving this problem using group theory.\nConsider weighted shuffle on playlists now, how would you solve the same problem? Maybe a graph approach is inevitable?"
  },
  {
    "objectID": "posts/2024-08-30-searching-song-in-deep-playlist/index.html#appendix",
    "href": "posts/2024-08-30-searching-song-in-deep-playlist/index.html#appendix",
    "title": "Searching for my favorite song in a deep playlist",
    "section": "Appendix",
    "text": "Appendix\n\nEnumerating playlist of length n\nThe code below enumerates a playlist of length n but it is highly inefficient. A back of the envelope estimate gives me a rough O(n!) time complexity. Can you do better?\n\n\n\nListing 1: Enumerate playlist of length n=4\n\n\nfrom itertools import permutations\n\nNULL = 0\nE = -1\n\n\ndef enum(seq: list[int]) -&gt; set[tuple[int, ...]]:\n    n = len(seq)\n\n    pool = set(range(1, n + 1))\n\n    # Find indexes where values are not empty.\n    # Since we only care about permutations, values are unique at each index (except\n    # for the NULL value). So remove encountered filled values from the permutation\n    # pool.\n    filled_idxs: list[int] = []\n    for idx, v in enumerate(seq):\n        if v != E:\n            filled_idxs.append(idx)\n            pool.remove(v)\n\n    # Make pool into a list because we may append &gt;=1 NULLs to it.\n    pool = list(pool)\n    # The least index of filled values determine the number of NULLs possible for\n    # the permutations.\n    for _ in range(min(filled_idxs)):\n        pool.append(NULL)\n    all_perm = list(permutations(pool, r=(n - len(filled_idxs))))\n    all_perm = set(all_perm)  # remove dups\n\n\n    # Find invalid NULL configurations such as (NULL, 1, NULL, ...). NULLs cannot\n    # appear after valid positive integer.\n    bad_seqs = set()\n    for p in all_perm:\n        num_found = False\n        for i in p:\n            if i != NULL:\n                num_found = True\n                continue\n            if num_found and i == NULL:\n                bad_seqs.add(p)\n                break\n\n    # Sucks this is python and you have to do something like this\n    assert isinstance(all_perm, set), \"`all_perm` is not a set.\"\n\n    good_seqs = all_perm - bad_seqs\n    return good_seqs\n\n\nif __name__ == \"__main__\":\n\n    def generate_new_seq(n: int):\n        new_seq = []\n        init = [E] * n\n        init[-1] = n\n        for i in range(n - 2, E, E):\n            init[i] = n - 1\n            if i != n - 2:\n                init[i + 1] = E\n            new_seq.append(init.copy())\n        return new_seq\n\n    seqs = generate_new_seq(5)\n\n    full_enumeration = []\n    for seq in seqs:\n        valid_perms = enum(seq)\n        for perm_seq in valid_perms:\n            seq2 = seq.copy()\n            idx = 0\n            for i in perm_seq:\n                while seq2[idx] != E:\n                    idx += 1\n                seq2[idx] = i\n            full_enumeration.append(seq2)\n\n    full_enumeration.sort()\n    N = len(full_enumeration)\n    warn_out = (\n        \"WARNING: Note that we use '0' as NULL pointers in the following enumeration.\"\n    )\n    print(\"-\" * len(warn_out))\n    print(warn_out)\n    print(\"-\" * len(warn_out))\n    for e in full_enumeration:\n        print(e)\n    print()\n    print(f\"N_t({len(seq)}) = {N}\")"
  },
  {
    "objectID": "posts/2024-10-03-how-to-ask-amex-retention-offer/index.html",
    "href": "posts/2024-10-03-how-to-ask-amex-retention-offer/index.html",
    "title": "How to ask Amex for a retention offer?",
    "section": "",
    "text": "I’ve scoured the internet (read: reddit) for this one. I found a lot of people sharing what was offered to them as their retention offer but close to none of people sharing what they actually said or how they actually contacted Amex to get it.\nSome people would just say, oh I go on the chat and then just ask for the retention offer. That’s actually quite helpful considering most people would guess that retentions are most probably done via phone – broadband and telco retention comes to mind. But what do you say to ask them for a retention offer? Do you fluff them up first? Do you go straight into asking with a straight expression? What do you do?\nI speak English as a second language. If you’ve learnt another language in your life beyond your mother tongue, you would know that knowing what and how to speak is deeply contextual. I’ve scored English essays, passed English vivas, but when I was trying to get my first haircut in the UK, I have no idea how to describe what I wanted. I showed a picture, but the barber wanted a precise description like “how many inches do you want to take off the fringe?” for which I replied with “what is a fringe…?”. So now, I’m gonna tell you what I ended up doing to ask Amex for a retention offer.\nObviously, I just winged it. I went on to chat, and literally just went through the no BS route. Feel free to copy this. I said:\n\nMe: Hi Amex, is there any retention offer for me? The fee seems quite high and I’m considering closing it. It just doesn’t make sense to keep it at the moment…\n\nThat’s it. No fluff, no introductions, just went straight begging for a retention offer. They’ll come back saying their fees is justified with the benefits. Well if you did run the math, you’ll know that the Gold card (pretty sure ditto for the Plat) indeed yields a benefit that is at least worth £285 on “tangible” benefits when the card itself costs £195 annually (as of 2024). You get £120 worth of Deliveroo annually (assuming you do at least a total of 4x take-outs and/or groceries each month) and roughly (£69 + £24 x 4 =) £165 worth in airport lounges. And this is just on “tangible” benefits where I haven’t factored in other beneftis like free travel insurance, 2x points on airline spend, discount on car hires and so on.\nSo even minus the extra Amex offers, and assuming you’re not inflating your lifestyle for the sake of matching the card benefits, you break even pretty easily with this card. Despite this, however, you should come back at them saying:\n\nMe: No, I get that the fees are justified, I’m just not happy that the benefits are netted out by the fees. Is there a retention offer for me to stay with the card?\n\nAfter you said this, they should give in and come out with a retention offer. This offer, unfortunately, is take it or leave it. I’ve tried negotiating but the rep said no. It does make sense since I’m pretty sure the retention offers are done by a single-output ML algorithm and so it’s really beyond the rep’s control. Now it’s up to you to decide to take it or leave it and cancel the card.\nThat’s it. That’s how you ask for a retention offer from Amex."
  },
  {
    "objectID": "posts/2021-08-18-coding-kata/index.html",
    "href": "posts/2021-08-18-coding-kata/index.html",
    "title": "The Coding Kata",
    "section": "",
    "text": "A coding kata is an exercise to sharpen your programming skills and muscle memory via repetition. I learnt this concept when I was reading The Clean Coder (highly recommended!) and have since adopted it as part of my programming routine. It helps my hands “know” what to type when I need to type.\nI have also adapted the concept of kata as a means to learn/revise programming languages effectively. Instead of writing a random script doing god knows what after learning the syntax, I would implement the simple linear regression algorithm from scratch in this language. Why this is a good kata you ask? Here is why among others:\n\nYou can practice using OOP in the new lang\nYou will deal with ints, doubles, static and dynamic arrays.\nYou will do some basic math operations.\nYou will implement at least one function.\nYou will implement at least a for loop.\nYou might use an import.\n\nTo put simply, it ensures you use a lot of the functionalities in the language and actually spend time doing it.\nI recently wanted to refresh my mind on the C++ lang and I also want to reinforce my JavaScript knowledge. So here’s my linear regression implementation in these two languages (although the way data is expected is a bit different in the JS implementation compared to the C++ implementation).\n\nC++JavaScript\n\n\n#include &lt;vector&gt;\nusing namespace std;\n\nclass LinearRegression {\n    private:\n        int m_epoch;\n        double m_learningRate;\n        vector&lt;double&gt; weights;\n\n    public:\n        LinearRegression(int epoch, double learningRate) {\n            this-&gt;m_epoch = epoch;\n            this-&gt;m_learningRate = learningRate;\n        }\n\n        void fit(vector&lt;double&gt; x, vector&lt;double&gt; y) {\n            vector&lt;double&gt; weights = {0, 0};\n            int dataLength = x.size();\n\n            int epoch = this-&gt;m_epoch;\n            for (int e = 0; e &lt; epoch; e++) {\n                for (int i = 0; i &lt; dataLength; i++) {\n                    double estimate = weights[0] + x[i]*weights[1];\n                    double error = y[i] - estimate;\n\n                    weights[0] += this-&gt;m_learningRate * error;\n                    weights[1] += this-&gt;m_learningRate * error * x[i];\n                }\n            }\n\n            this-&gt;weights = weights;\n        }\n\n        vector&lt;double&gt; predict(vector&lt;double&gt; x) {\n            int dataLength = x.size();\n            vector&lt;double&gt; yPred;\n            yPred.reserve(dataLength);  // Preallocate length of yPred based on size of x.\n\n            vector&lt;double&gt; weights = this-&gt;weights;\n            for (int i = 0; i &lt; dataLength; i++) {\n                yPred.push_back(weights[0] + x[i]*weights[1]);\n            }\n\n            return yPred;\n        }\n};\n\n\nclass LinearRegression {\n  constructor(params = {}) {\n    this.weights = params.weights || [];\n    this.learningRate = params.learningRate || 0.01;\n    this.data = [];\n    this.fittedValues = [];\n  }\n\n  estimator(x, weights) {\n    const [w0, w1] = weights;\n    return w0 + x * w1;\n  }\n\n  fit(data) {\n    this.data = data;\n    if (this.weights === undefined || this.weights.length === 0) {\n      this.weights = [0, 0];\n    }\n\n    for (let i = 0; i &lt; this.data.length; i++) {\n      const { x, y } = this.data[i];\n\n      const estimate = this.estimator(x, this.weights);\n      const error = y - estimate;\n\n      let [w0, w1] = this.weights;\n\n      w0 += this.learningRate * error;\n      w1 += this.learningRate * error * x;\n\n      this.weights = [w0, w1];\n    }\n\n    this.fittedValues = this.getFittedValues();\n  }\n\n  getFittedValues() {\n    return this.data.map(({ x, y }) =&gt; {\n      return { x: x, y: this.estimator(x, this.weights) };\n    });\n  }\n}\n\n\n\nI leave the Python implementation to you ;)"
  },
  {
    "objectID": "posts/2021-06-13-plotly-theme-in-react-plotly-js/index.html",
    "href": "posts/2021-06-13-plotly-theme-in-react-plotly-js/index.html",
    "title": "Plotly.py main theme in Plotly.js",
    "section": "",
    "text": "If you’re like me who is used to using Plotly.py (i.e. Plotly Python) and then suddenly decided to use Plotly.js directly, you might immediately realize that there are some significant differences in terms of the plot design.\n\n\n\nPlotly Python Plot\n\n\nFor one, your default plot in Plotly.js has a white background and not the steel blue colour as you would expect from a default Plotly.py plot. The reason is that the plots produced in the python version of Plotly come prepackaged with nice out-of-the-box themes. For example, the plot image above is a Plotly.py scatter plot with the default plotly template. So how do we reproduce these themes inside of Plotly.js?\nDiving deeper into the Plotly documentation, I discovered that you can actually get the “theme settings” for the Plotly.py templates with just 4 lines of Python code.\nimport plotly.io as pio\ntemplate = \"plotly\"\nplotly_template = pio.templates[template]\nprint(plotly_template.layout)\nThe Python code above prints the “theme settings” of the plot or in proper Plotly lingo, the plot layout settings. In our particular case above where we set template = \"plotly\", we get the default plotly theme layout settings:\nLayout({\n    'annotationdefaults': {'arrowcolor': '#2a3f5f', 'arrowhead': 0, 'arrowwidth': 1},\n    'autotypenumbers': 'strict',\n    'coloraxis': {'colorbar': {'outlinewidth': 0, 'ticks': ''}},\n    ...\n})\nAs you can see, it is in serialized JSON format. The idea now then is to deserialize this layout settings into a JavaScript object literal and pass it into the layout of your plot in Plotly.js. I did the deserialize process manually because I cannot find a way to automate this process (I would love to hear if you are able to do it!). For the default plotly theme, the JavaScript object literal is given below:\n\n\nNote: Stating the obvious but since the layout setting is extracted from the Plotly documentation, it is the hard work of the people behind Plotly. Thank you for the awesome theme!\n\nThe final step is to pass this layout into the layout parameter of the Plotly plot, and you get your favorite default Plotly.py theme again but now directly in Plotly.js!"
  },
  {
    "objectID": "posts/2021-04-05-first-post/index.html",
    "href": "posts/2021-04-05-first-post/index.html",
    "title": "Blogging again",
    "section": "",
    "text": "I’ve been an active blogger since 2011 up until 2016 (although most of the earlier contents are now hidden due to cringiness) where I talked about various things – life events, some tech stuff and my scholarship interview experiences. I stopped blogging when I started my International Baccalaureate (IB) diploma program. Throughout my undergraduate years, I channelled most of my writing energy answering IB questions on Quora and writing my notes using LaTeX. Today, I’ve decided to start blogging again, focusing on math, statistics and ML.\nDo note that in the early stages of this blog, the contents might be inconsistent or quite random. This is because I am still finding the right pace and momentum in writing and generating content. Thanks for understanding!"
  },
  {
    "objectID": "posts/2021-09-07-preparing-image-dataset-in-pytorch/index.html",
    "href": "posts/2021-09-07-preparing-image-dataset-in-pytorch/index.html",
    "title": "Preparing Image Dataset for Neural Networks in PyTorch",
    "section": "",
    "text": "Preparing and handling data is a core step of any machine learning pipeline. Today, we will look at handling data when the data is an image (or image-like) in PyTorch.\n\nPyTorch and Torchvision\nPyTorch provides us with the amazing torchvision package for dealing with common image transformations such as normalizing, scaling, random flipping and converting arrays to PyTorch tensors. It also provides us with common computer vision datasets such as MNIST, Fashion MNIST and CIFAR-10. In this post, we will focus on preparing the Fashion MNIST dataset.\nTo begin, we start by importing torch and torchvision.\nimport torch\nfrom torchvision import datasets, transforms\n\nNote that we will refer to the submodule datasets and transforms directly from now on (i.e. we will not emphasize that it’s part of torchvision).\n\n\n\n👜 Fashion MNIST dataset and composing transformations\nThe Fashion MNIST dataset by Zalando Research is a famous benchmark dataset in computer vision, perhaps second only to MNIST. It is a dataset containing 60,000 training examples and 10,000 test examples where each example is a 28 x 28 grayscale image. Since the images are in grayscale, they only have a single channel. If the image is in RGB format instead (e.g. if we are dealing with CIFAR-10), then it has 3 channels one for each red, green and blue.\nAs mentioned before, the Fashion MNIST dataset is already part of PyTorch. However, this does not mean that the dataset is already in perfect shape to pass into a PyTorch neural net. We would need to apply some (image) transformations to the dataset upon fetching. For brevity, we will apply only two simple transformations:\n\nConverting the images to a PyTorch tensor – by using transforms.ToTensor().\nNormalize the channel of the resulting tensor – by using transforms.Normalize().\n\nWhy do we do these transformations?\n\nSince we will be working with neural nets in PyTorch, it is only natural that we want the image to be a PyTorch tensor. This enables the PyTorch API to interact properly with our dataset.\nNormalization is important to ensure that our neural nets learn better. For an idea of how normalization works, check out this discussion.\n\nWe can then compose these transformations using transforms.Compose() as below.\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5), std=(0.5)),\n])\n\nNote that the mean and standard deviation value of 0.5 should be calculated from the training set in advance. Here, we just assume that mean = std = 0.5 for simplicity.\n\n\n\n💾 From dataset to DataLoader\nThe next step is to finally fetch the dataset, passing our transform above as an argument. The FashionMNIST dataset can be accessed via datasets.FashionMNIST, no surprise there. We can then fetch the 60,000 training examples using the following code:\ntrainset = datasets.FashionMNIST(root='./data',\n                                 download=True,\n                                 train=True,\n                                 transform=transform)\nLet us break down what each argument means.\n\nroot specifies the location of the dataset. Here, we specify that it should be in the directory './data'.\ndownload is a boolean flag which determines if we want to download the dataset if the data is not already in root.\ntrain is another boolean flag which determines if we want the training set. Getting the test set is as simple as passing train=False.\ntransform is the transformations we would like to apply to the dataset upon fetching.\n\nOnce we have our transformed train set, we can now start training neural nets on this data using PyTorch. However, let us take a second to think about the following:\n\nWhat if we want to work with minibatches of this dataset instead of single examples? This is definitely a need when the dataset is too large like ours to be trained entirely.\nWe would also want to reshuffle this dataset on each epoch so that our neural net generalizes better.\nIf the data is big, we might even want to load the data in parallel using multiprocessing workers to retrieve our data faster.\n\nThis is where PyTorch’s so-called DataLoader comes in. It is an iterable that provides all the above features out of the box on top of providing a smooth API for working with data!\nTo use the DataLoader object on our train set, we simply access torch.utils.data.DataLoader and feed trainset into it.\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n                                          shuffle=True, num_workers=0)\nHere, we have decided to use a batch_size of 64 images, which are sampled randomly on each epoch due to shuffle=True. We also put num_workers=0 meaning we are not loading the data in parallel.\nWe can fetch the Fashion MNIST test dataset in a similar fashion. The only difference is that we now have train=False.\ntestset = datasets.FashionMNIST(root='./data',\n                                download=True,\n                                train=False,\n                                transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64,\n                                         shuffle=True, num_workers=0)\n\n\n🕵️ Inspecting the dataset in DataLoader form\nOnce we have the dataset in DataLoader form, we can start inspecting our dataset. For example, we can get the shapes of our trainset.\nprint(\"Train shape:\", trainloader.dataset.data.shape)\nprint(\"Test shape:\", testloader.dataset.data.shape)\nTrain shape: torch.Size([60000, 28, 28])\nTest shape: torch.Size([10000, 28, 28])\nWe can also get the minibatch size as specified when initializing the DataLoader.\nprint(\"Train batch size:\", trainloader.batch_size)\nprint(\"Test batch size:\", testloader.batch_size)\nTrain batch size: 64\nTest batch size: 64\nFor a more advanced inspection, we can even look at the sampler and the collate function used in the DataLoader. The sampler determines how the data is shuffled and the collate function specifies how the data is batched.\nprint(\"Sampler:\", trainloader.sampler)\nprint(\"Collate function:\", trainloader.collate_fn)\nSampler: &lt;torch.utils.data.sampler.RandomSampler object at 0x7fcc02b23b90&gt;\nCollate function: &lt;function default_collate at 0x7fcc05c9a710&gt;\nSince we did not pass anything during initialization, we get the default RandomSampler object for the sampler and the default default_collate collate function as expected.\nAs we are dealing with an image dataset, it is a shame if we are not plotting anything during inspection. Let’s plot the first image from the first batch in trainloader.\nimages, labels = next(iter(trainloader))  # Gets a batch of 64 images in the training set\nfirst_image = images[0]  # Get the first image out of the 64 images.\n\nimport matplotlib.pyplot as plt\nplt.imshow(first_image.numpy().squeeze(), cmap='Greys_r')\nplt.show()\nHere, we get a t-shirt which is expected since we are dealing with a fashion dataset after all. If you run the exact code, you might get a different output since the dataset is shuffled and I did not specify a seed.\n\n\n\nPlot of image from Fashion MNIST\n\n\nFor the simplified version of this post in jupyter notebook format: notebook version."
  },
  {
    "objectID": "posts/2021-04-30-relational-databases-in-python-part-I/index.html",
    "href": "posts/2021-04-30-relational-databases-in-python-part-I/index.html",
    "title": "Relational Databases in Python (part I)",
    "section": "",
    "text": "Post header\nWe usually hear the word databases being thrown around especially when talking about data-related things. So what is it, and what is the more precise term relational databases?\nA relational database is like an Excel file. It is made up of tables (note that this is plural) which holds data in the form of columns and rows. In the Excel analogy, tables are basically sheets. Moreover, tables can be related to each other but they need a column to act as a bridge linking them. Such a column is usually called a key (either primary key or foreign key). This feature of being related explains the relational term in relational databases. Relational databases is part of the relational model which is a much more general framework of structuring and handling data management.\nFrom now on, we shall simply refer to relational databases as just databases."
  },
  {
    "objectID": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#introduction-to-sqlalchemy",
    "href": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#introduction-to-sqlalchemy",
    "title": "Relational Databases in Python (part I)",
    "section": "Introduction to SQLAlchemy",
    "text": "Introduction to SQLAlchemy\npip install sqlalchemy\nGreat, we now know what databases are, but how do we work with them? If this is an Excel file, we just open it and the rest is obvious (at least we think it’s obvious because we’re used to working with it). How do we open and interact with a database? There are many ways to do this and there’s no one right way. For example, you can work directly with SQLite or MySQL in the command line but things can really get messy if you do it this way.\nIf you’re using Python, then enter SQLAlchemy! SQLAlchemy gives you the power of interacting with databases using SQL queries straight from Python. Plus, it helps us abstract away complex queries and the difference in databases (remember that there are a lot of popular databases e.g. MySQL, PostgreSQL and Oracle with subtle differences among them). So querying databases becomes cleaner (and more exciting?) via SQLAlchemy. To install SQLAlchemy is as easy as executing the line of code you see below this section’s title in your favorite terminal. If you’re using Anaconda, then it is already shipped and ready to use!\nAdditional notes:\n\nThis is not an excuse to not learn writing raw SQL queries because understanding SQL is still important when working with tools like SQLAlchemy!\nThose of you who have built web applications with Flask or Django might realize that we usually handle data using an object-oriented approach – using so-called data models. This is the Object Relational Model (ORM) approach, which is one of the two main components of SQLAlchemy. The other main component is called the “core part” of SQLAlchemy which is really centred around the relational model of the database. The latter is the one that we will be focusing on in this post today."
  },
  {
    "objectID": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#talking-to-a-database-first-steps",
    "href": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#talking-to-a-database-first-steps",
    "title": "Relational Databases in Python (part I)",
    "section": "💬 Talking to a database, first steps",
    "text": "💬 Talking to a database, first steps\n\nStep 1: Create an engine\nTo write something on a paper, you would need a pencil. To turn the lights on, you would need to switch the toggle. To interact with a database, you would need a so-called engine.\nIn theory, a database relies on this engine just like how a car would rely on its (car) engine. But I found that thinking of this engine as a mediator rather than a literal engine is much easier to digest.\nTo create an engine, you first have to import the create_engine function from sqlalchemy. Then, you need to pass in a connection string which in its simplest form specifies two things: (i) the database you want to talk to, and (ii) the path to the database. For example, if I want to connect to race_data.db which is a SQLite database in my current directory, the connection string would be \"sqlite:///race_data.db\". So together, they would look something like this:\nengine = create_engine(\"sqlite:///race_data.db\")\n\nKey terms\n\nThe engine is a mediator for SQLAlchemy to interact with a database.\nA connection string specifies the details of the database we want to interact with.\n\n\n\n\nStep 2: Establish a connection\nIn the previous step, we have simply created an engine but have yet to connect to it! To establish a connection, you can simply invoke another one-liner:\nconnection = engine.connect()\nOne thing worth pointing out is that SQLAlchemy is clever enough to not actually make a connection until we pass in some queries for it to execute.\n\n\nStep 3: Checking table names\nRecall that tables are to databases just like sheets are to Excel files. So you’d want to know what tables (not columns of a table, yet!) are available before making queries. To do this, you can simply execute\nprint(engine.table_names())\nto get a list of available tables to work with. For me, this returns\n&gt;&gt;&gt; ['race_table']\nwhich means that I have only one table named 'race_table'. In practice, you would usually have a few tables."
  },
  {
    "objectID": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#querying-the-database-using-sqlalchemy",
    "href": "posts/2021-04-30-relational-databases-in-python-part-I/index.html#querying-the-database-using-sqlalchemy",
    "title": "Relational Databases in Python (part I)",
    "section": "📝 Querying the database using SQLAlchemy",
    "text": "📝 Querying the database using SQLAlchemy\nFrom now on, we assume that we have instantiated an engine and connection object exactly like what we did previously.\n\nRaw SQL queries, ResultProxy and ResultSet\nRecall that we use the SQL language to make CRUD operations – create, read, update and delete. If you are not familiar with this, I highly recommend that you learn a bit of SQL after reading this - my recommendation is Mike Dane’s{:target=“_blank”} free and complete SQL course, from installing MySQL to joining tables. However, for now, it is sufficient to know the “Hello World” of SQL – which is SELECT * FROM this_table where this_table is some table in the database. The query SELECT * FROM this_table does exactly what you expect it to do, it selects every possible row (symbolized by the asterisk *) in the table this_table and returns it to the user. In my case, with my race_data.db database, I would want to execute SELECT * FROM race_table. So how would I do this with SQLAlchemy? This is where connection from Step 2 comes in.\nThe connection object has a method .execute() which can execute raw SQL queries like SELECT * FROM race_table. This will then return a ResultProxy object which can be utilized in various ways to get the data we want (based on our query). Here are some examples of how we would want our data:\n\nSometimes, we know our query will return a unique result (e.g. because we query based on a unique ID), so it is trivial that we want the first and only result;\nSometimes, we want the whole result;\nSometimes, we want only the top 10.\n\nImagine processing 500k rows of data just because we want the top 10, not so efficient right? This is why we would want a two-layer process before getting our actual data, the first layer being the ResultProxy object. Getting the actual data from the ResultProxy object is simply a matter of invoking a method. For example, if we would want to get the whole result, we use the .fetchall() method. If we want the top 10 result, we use the .fetchmany() method with size argument set to 10. Invoking these methods returns a ResultSet object, which basically contains the data we want. Here is an example of a complete implementation:\nq = \"SELECT * FROM race_table\"\nresult_proxy = connection.execute(q)  # ResultProxy\nresults = result_proxy.fetchmany(size=10)  # ResultSet\n\nQuerying data in Python is a two-layer process:\n\nA ResultProxy gives us flexibility to access the data that we queried – do you want 1, 10 or 500k?.\nA ResultSet contains the actual data that we queried, retrieved via a ResultProxy method.\n\n\n\n\nWorking with ResultSet\nThe ResultSet results is a list of tuples whose entries corresponds to columns in the table. Let’s get the first row in results. Since it is a list, we do this by accessing the zeroth element in the list via row = results[0]. We can print the row to get the actual data, a tuple with entries:\n&gt;&gt;&gt; (1, 88, 0.95, 1, 5, 436, '2013-11-03 13:19:25')\nTo get the column names that correspond to each entry, we can invoke row.keys() to get:\n&gt;&gt;&gt; ['Race #', 'WPM', 'Accuracy', 'Rank', '# Racers', 'Text ID', 'Date/Time (UTC)']\nIf we already know which column of interest we want to look at for a particular row, we can access the attribute of the row just like how we would normally do to access the value of a key in a dictionary. For example, if I’m interested in knowing the Accuracy of this row, I would just execute either row.Accuracy or row[\"Accuracy\"] which returns 1 as expected.\n\n\nSQLAlchemy queries\nI know I promised that SQLAlchemy can help abstract away complex SQL queries and database differences, and using raw SQL queries like we did so far doesn’t seem to agree with this promise. Enter SQLAlchemy’s neat and Pythonic way of querying using table reflection.\nA table reflection or just reflection is basically a process which reads the desired database, looks for your desired table and copies it into a Table object (see below) as if you wrote a whole raw SQL query to create this table. Personally, the term reflection is quite misleading for me and I would more prefer the term autocopy – because that is literally what the process does, and my philosophy is that explicit is better than nano-misleadings.\nTo make a reflection, you would need to import two classes from the sqlalchemy library: MetaData and Table. It is worth understanding a basic idea of what these things do:\n\nThe MetaData class can be thought of a folder or catalog that keeps information about database stuffs such as tables. In this way, we don’t have to keep looking up table information because we have “organized” things nicely.\nThe Table class does exactly what you expect it to do. It stores and handles the reflected (i.e. autocopied) version of your desired table so that SQLAlchemy can interact with it.\n\nNow we know the required ingredients, let’s see how to actually do a table reflection. Recall that we have instantiated an engine and connection object from the previous section. We now instantiate a MetaData object via metadata = MetaData(); and then instantiate a Table object passing the arguments:\n\nOur desired table; in my case, \"race_table\" (this is a string, cf. Step 3 of checking table names).\nThe instantiated metadata.\nSet autoload to True, and put autoload_with our engine.\n\nOverall, it should look like this:\nfrom sqlalchemy import MetaData, Table\nmetadata = MetaData()\n\n# Table reflection\nrace_table = Table(\"race_table\", metadata, autoload=True, autoload_with=engine)\nThe first thing you might want to do then is to use the repr function on race_table to learn our table’s details such as the column names together with their types such as REAL, INTEGER or TEXT. Mine returns this:\n&gt;&gt;&gt; Table('race_table', MetaData(bind=None), Column('Race #', INTEGER(), table=&lt;race_table&gt;), Column('WPM', INTEGER(), table=&lt;race_table&gt;), Column('Accuracy', REAL(), table=&lt;race_table&gt;), Column('Rank', INTEGER(), table=&lt;race_table&gt;), Column('# Racers', INTEGER(), table=&lt;race_table&gt;), Column('Text ID', INTEGER(), table=&lt;race_table&gt;), Column('Date/Time (UTC)', TEXT(), table=&lt;race_table&gt;), schema=None)\nThe true power of the SQLAlchemy way of querying comes now. To replicate our raw SQL query of SELECT * FROM race_table, we can import and use the select function from the sqlalchemy library. The select function can take a (list of) Table object to select all the rows in that table. The equivalent query to SELECT * FROM race_table is then select([race_table]). More completely, we execute the following code:\nq = select([race_table])\nresult_proxy = connection.execute(q)  # ResultProxy\nresults = result_proxy.fetchmany(size=10)  # ResultSet\nObserve that the last two lines are identical as to when we were writing raw SQL queries, the only difference being the first line. Hence, the way we access results is exactly the same as we’ve discussed earlier. For the SELECT query, it might be trivial to use the select function but you can imagine that when the queries gets more complex, this will be really nice and clean. Note that you can get the raw SQL query of q by simply printing it to the console.\nReferences\n\n[1] Introduction to Databases in Python, DataCamp\n[2] Sam Hartman’s Answer to Understanding MetaData() from SQLAlchemy in Python, Stack Overflow"
  },
  {
    "objectID": "posts/2024-08-09-cnn-rotation-invariant/index.html",
    "href": "posts/2024-08-09-cnn-rotation-invariant/index.html",
    "title": "Convolutional neural networks are not rotation invariant… since when?",
    "section": "",
    "text": "“Why is a convolutional neural network not rotation invariant?”.\nThis question popped up in an interview recently, and it is surprisingly deeper than I initially thought. For a start, the statement “a convolutional neural network is not rotation invariant” is not entirely true, but only half-true, maybe a quarter-true. So why is this a popular statement being asked in the machine learning domain? I feel like this is mainly due to a poor understanding of the convolutional neural net architecture.\nWhat’s surprising is how little I have been able to find on the internet about this question. The articles that I have found are unoriginal and poor derivatives of what was written in (Goodfellow, Bengio, and Courville 2016); and like any good mathematics textbook, they do not really do a deep dive into the mathematical reasoning behind the statement – which make interpretations written around it even worse. In fact, Goodfellow, Bengio, and Courville (2016) mentions that a convolutional neural net is rotation invariant with the right layers… so what the heck is going on here?\nToday, I am putting this question to bed with a somewhat acceptable mathematical reasoning. Let’s get one important thing out of the way first because this might confuse some people. The statement “convolutional net is not rotation invariant” is somewhat misleading; and this is a consequence of people not understanding the difference betwen a convolutional neural net, a convolutional layer and a convolutional operator. So what is actually “convolution” in convolutional neural net?"
  },
  {
    "objectID": "posts/2024-08-09-cnn-rotation-invariant/index.html#convolutional-neural-net-convolution-operators",
    "href": "posts/2024-08-09-cnn-rotation-invariant/index.html#convolutional-neural-net-convolution-operators",
    "title": "Convolutional neural networks are not rotation invariant… since when?",
    "section": "Convolutional neural net, convolution operators",
    "text": "Convolutional neural net, convolution operators\nA convolutional neural network is simply a neural network where at least one of its layers utilize the convolutional operator rather than a full-blown matrix multiplication (Goodfellow, Bengio, and Courville 2016). This definition is so general that it renders the statement “why is a convolutional neural network (CNN) not rotation invariant?” to be false, especially with our breadth of understanding on deep learning today. I will demonstrate why precisely in the rest of this article.\nSo let’s go down one step in the rabbit hole… what is a convolution? A convolution * of two complex-valued functions I and K is simply the integral transform\nJ(t) := (I * K)(t) = \\int_{\\mathbb{R}^d} I(\\tau) K(t - \\tau) d\\tau = \\int_{\\mathbb{R}^d} I(t - \\tau) K(t) d\\tau.\nIn computer vision, we are, at the most basic level, concerned with the discrete convolution of real-valued objects (i.e. images) over a finite two-dimensional integer support. Although, this is not exactly true because with modern convolutional neural nets, you actually perform multi-channel convolution which is typically modelled with a four-dimensional kernel tensor instead. Let’s focus only on the basics though. In the two-dimensional case, the convolution above reduces (and simplifies) to\nJ[i, j] = \\sum_{m, n} I[i- m, j- n] K[m, n],\nwhere I, K are real-valued functions over \\mathbb{Z}^2 and the summation is done over the support of K. Here, I use square brackets instead of parentheses to align with literature on discrete convolution.\nNow, I claim that the convolution operator is not rotation invariant. But before that, let’s look at something even simpler. I claim that the convolution operator is not even translation invariant!"
  },
  {
    "objectID": "posts/2024-08-09-cnn-rotation-invariant/index.html#convolution-is-not-translation-invariant",
    "href": "posts/2024-08-09-cnn-rotation-invariant/index.html#convolution-is-not-translation-invariant",
    "title": "Convolutional neural networks are not rotation invariant… since when?",
    "section": "Convolution is not translation invariant",
    "text": "Convolution is not translation invariant\nTo prove that convolution is not translation invariant, it suffices to look at a single nonzero shift s in the x-coordinate where we observe that\n\n\\begin{align*}\nJ[i, j]\n&= \\sum_{m, n} I[i-m, j-n] K[m, n] \\\\\n&\\neq \\sum_{m, n} I[i-m+s, j-n] K[m, n] \\\\\n&= J[i + s, j].\n\\end{align*}\n\nObviously, this is a sum of (a lot of) product terms so there are several ways where equality J[i,j] = J[i + s, j] can be achieved; but it is not a guarantee. \\blacksquare\nHere’s a simple concrete example to see where they are not equal. Define I to be the function [m, n] \\mapsto 1 and let K be defined by\n\n[m, n] \\mapsto\n\\begin{cases}\n1 & \\text{if } m = n \\text{ and } m, n \\neq 0, \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\nover \\{ 0, 1, 2\\} \\times \\{ 0, 1, 2 \\} \\subseteq \\mathbb{Z}^2. Then evaluating J at (i, j) = (0, 1) gives J[i, j] = 0 but J[i+1, j] = 1.\nNow was that surprising? Did you feel lied to all your life that convolutional neural nets are translation invariant? Well, we’ll get to why that statement is commonly made later at the end of this post. For now, let me show you that the convolution operator is not rotation invariant."
  },
  {
    "objectID": "posts/2024-08-09-cnn-rotation-invariant/index.html#convolution-is-not-rotation-invariant",
    "href": "posts/2024-08-09-cnn-rotation-invariant/index.html#convolution-is-not-rotation-invariant",
    "title": "Convolutional neural networks are not rotation invariant… since when?",
    "section": "Convolution is not rotation invariant",
    "text": "Convolution is not rotation invariant\nNow that you’ve got a basic idea on how to disprove convolution being translation invariant, it should be straightforward to see that convolution is not rotation invariant… at least a sketch proof should be present in your mind. Reasoning it formally, however, takes a bit more effort as we need to do some setup first. For a start, I am going to make things simple by expanding the domain back to \\mathbb{R}^2 because I do not want to talk about rotation matrices in \\mathbb{Z}^2 which is not pretty. As a bonus to doing this, we are now back in the world of vector spaces… although we can always make an extra effort and start talking about \\mathbb{Z}^2 as a module over itself. Let’s now start putting the pieces together.\nLet p_{\\mathrm{center}} = (0, 0) \\in \\mathbb{R}^2 be the image center; then define a two-dimensional Cartesian coordinate system centered at p_{\\mathrm{center}} and fix the standard basis. This defines the axes that we will play around to perform rotation. Now that we have that set up, for any vector x \\in \\mathbb{R}^2, we can perform a counterclockwise rotation of \\theta degrees around p_{\\mathrm{center}} by applying the rotation matrix\nR_\\theta = \\begin{pmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{pmatrix}.\nThen for an image I_0(x): \\mathcal{D} \\subseteq \\mathbb{R}^2 \\to \\mathbb{R}, the corresponding image I_\\theta rotated by \\theta degrees around p_{\\mathrm{center}} is given by the image I_\\theta(y): \\mathcal{D}' \\subseteq \\mathbb{R}^2 \\to \\mathbb{R} where I_\\theta(y) = I_0(x) and y = R_\\theta x (or equivalently, x = R_\\theta^{-1} y).\nNow fix a finite canvas to draw our images on so let \\Omega \\subseteq \\mathbb{R}^2 be a bounded domain such that \\mathcal{D}, \\mathcal{D'} \\subseteq \\Omega. Suppose now that the point q \\in \\mathcal{D} is mapped (bijectively) to q' \\in \\mathcal{D}' under the rotation R_\\theta. Then we can now see that the convolution with rotation is not equal to the convolution without rotation as follows\n\n\\begin{align*}\nJ_\\theta(q') &= \\sum_{p \\in \\Omega} I_{\\theta}(q'-p) K(p) \\\\\n&= \\sum_{p \\in \\Omega} I_0(R_{-\\theta} q' - R_{-\\theta} p) K(p) \\\\\n&= \\sum_{p \\in \\Omega} I_0(q - R_{-\\theta} p) K(p) \\\\\n&\\neq \\sum_{p \\in \\Omega} I_0(q - p) K(p) \\\\\n&= J_0(q).\n\\end{align*}\n\nIn other words, convolution is not rotation invariant. \\blacksquare"
  },
  {
    "objectID": "posts/2024-08-09-cnn-rotation-invariant/index.html#seems-like-convolutional-neural-nets-are-not-rotation-invariant-at-all",
    "href": "posts/2024-08-09-cnn-rotation-invariant/index.html#seems-like-convolutional-neural-nets-are-not-rotation-invariant-at-all",
    "title": "Convolutional neural networks are not rotation invariant… since when?",
    "section": "Seems like convolutional neural nets are not rotation invariant at all…?",
    "text": "Seems like convolutional neural nets are not rotation invariant at all…?\nSo the convolution operator is neither translation nor rotation invariant. So why did I say the statement “a convolutional neural network is not rotation invariant” is only partially true? It should be borderline false now, no? Well there’s a stark difference between a convolution operator and a convolutional neural net. What we have shown is that the former is not translation/rotation invariant but the latter is a different beast completely, it is much more complex.\nA convolutional neural net typically composes of multiple kernel convolution operators, pooling operators and activation functions. See the difference now? A convolutional neural net has a convolution operator as a layer. So in its most basic form where the net only has convolutional layers, it is neither translation nor rotation invariant. But… since we have other layers, we can actually make the net learn these invariances by combining the right layers (and the right data).\nAs in MLPs, the activation function introduces non-linearity so it’s not really a bonus. So based on the layers I have mentioned, it should be obvious now. A convolutional operator and the activation function themselves alone will not be helpful; so the secret sauce must be the pooling operator… plus a sufficiently broad dataset… ish. Read on in part II.\nEdit (19/09/2024): Added link to part II."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "My Oxford MSc thesis supervised by Seth Flaxman, Swapnil Mishra and Elizaveta Semenova.\n\nI made the observation that the PriorVAE framework in “PriorVAE: encoding spatial priors with variational autoencoders for small-area estimation” (Semenova et. al, 2022) does not make efficient use of the data’s local neighbourhood structure when learning spatial priors for MCMC sampling.\nSo I proposed a new framework, called PriorVGAE, which exploits the underlying local neighbourhood structure and propagates this information efficiently via the use of graph convolutional networks (GCN). Under this new framework, we were able to perform MCMC inference with far superior performance as compared to PriorVAE while enjoying better sampling speeds.\nThe breakthrough: naively injecting a GCN into a VAE framework would not make the architecture work. The key observation was to realize that spatial priors inherently admits global information. To this end, I introduced a local-to-global scheme that consolidates all the local information into a global representation, relying on the universal approximator theorem of MLPs, to efficiently learn the spatial priors.\nSome bonus advantages of this framework as compared to PriorVAE include getting computationally cheaper training. We now have fewer learnable parameters since we replace MLPs with GCNs and GCN weights are shared across the support. Out-of-sample predictions are also now tractable as restarting compute training is relatively inexpensive.\n\n\n\n\nWe reproduced some of the key interesting experiments in the paper “Inference Suboptimality in Variational Autoencoders” (Cremer et. al, 2018) using JAX and extended the result to the Kuzushiji-MNIST dataset. This work was done jointly with Basim Khajwal, Snehal Raj and Vasileios Ntogram.\n\n\n\n\n\nI reproduced the variational graph autoencoder (VGAE) deep learning model using JAX to be used as a component in my MSc thesis."
  },
  {
    "objectID": "projects.html#research-ml",
    "href": "projects.html#research-ml",
    "title": "projects",
    "section": "",
    "text": "My Oxford MSc thesis supervised by Seth Flaxman, Swapnil Mishra and Elizaveta Semenova.\n\nI made the observation that the PriorVAE framework in “PriorVAE: encoding spatial priors with variational autoencoders for small-area estimation” (Semenova et. al, 2022) does not make efficient use of the data’s local neighbourhood structure when learning spatial priors for MCMC sampling.\nSo I proposed a new framework, called PriorVGAE, which exploits the underlying local neighbourhood structure and propagates this information efficiently via the use of graph convolutional networks (GCN). Under this new framework, we were able to perform MCMC inference with far superior performance as compared to PriorVAE while enjoying better sampling speeds.\nThe breakthrough: naively injecting a GCN into a VAE framework would not make the architecture work. The key observation was to realize that spatial priors inherently admits global information. To this end, I introduced a local-to-global scheme that consolidates all the local information into a global representation, relying on the universal approximator theorem of MLPs, to efficiently learn the spatial priors.\nSome bonus advantages of this framework as compared to PriorVAE include getting computationally cheaper training. We now have fewer learnable parameters since we replace MLPs with GCNs and GCN weights are shared across the support. Out-of-sample predictions are also now tractable as restarting compute training is relatively inexpensive.\n\n\n\n\nWe reproduced some of the key interesting experiments in the paper “Inference Suboptimality in Variational Autoencoders” (Cremer et. al, 2018) using JAX and extended the result to the Kuzushiji-MNIST dataset. This work was done jointly with Basim Khajwal, Snehal Raj and Vasileios Ntogram.\n\n\n\n\n\nI reproduced the variational graph autoencoder (VGAE) deep learning model using JAX to be used as a component in my MSc thesis."
  },
  {
    "objectID": "projects.html#data-science",
    "href": "projects.html#data-science",
    "title": "projects",
    "section": "🔍 data science",
    "text": "🔍 data science\n\nFinancial transactions fraud detection using machine learning\nThis project covers strategy research, experimentation (with reporting) and deployment via Google Cloud.\n\n\nResearched and verified the class weight strategy in “Fraud Detection using Machine Learning” (Oza, 2018); and further extended the paper’s analysis with new models and metrics. I wrote a full experiment report on this strategy verification which can be found here: Replication & extension of “Fraud Detection using Machine Learning” (Oza, 2018).\nTrained logistic regression, SVMs and decision trees to perform financial transactions fraud detection in an imbalanced dataset of 6+ million rows and 0.1% sample fraud transactions (Python, scikit-learn).\nBuilt a web app to visualise streaming transactions data and perform live fraud detection using trained models. (Python, Streamlit, Docker, Google Artifact Registry, Google Cloud Run, Google Cloud Pub/Sub, Google Vertex AI).\n\n\n\n\nMeasuring the impact of a website redesign the Bayesian way\nProblem statement: How can we determine if the change in observed conversion rates after website redesign is not due to random chance? Moreover, can we get an exact probability of how likely a test group is to perform better/worse than the control group?\n\n\nIn this mini project, I analysed the conversion rates of four different user groups (one of which is the control group) using pivot tables and bar plots to get a first big picture.\nThen, using PyMC3, I applied Bayesian A/B testing based on a Metropolis sampler to measure which group is most likely to have a higher conversion rate than the control group. The Bayesian way yields a solid probability unlike the frequentist approach. For example, in this project, I was able to deduce that “there is a 99% probability that user group B has a higher conversion rate than the control group”.\nThis exact probability gives us the confidence required to make important business decisions. In this case, we can be really confident in our decision of switching to the design version exposed to group B since there is only a 1% chance of B being worse than the control. This is one of the many awesome reasons why I favor the Bayesian approach for data tasks; this also helps us answer the second problem.\n\nThis project is hosted on DataCamp Workspace."
  },
  {
    "objectID": "projects.html#web-app",
    "href": "projects.html#web-app",
    "title": "projects",
    "section": "💻 web app",
    "text": "💻 web app\n\nTradeLogger\nOne of the problems with stock traders is that they do not keep the big picture of where they are in their trading journey. For example. if they are losing, do they realise that they are losing 5 trades consecutively? This tool is designed for traders to trade better.\nWritten with Flask."
  },
  {
    "objectID": "projects.html#utilities",
    "href": "projects.html#utilities",
    "title": "projects",
    "section": "🔧 utilities",
    "text": "🔧 utilities\n\nEasyPS\nA simple, out of the box personal statement LaTeX framework. It helps handling duplicated tex files when writing personal statements for multiple universities, scholarships or jobs.\n\n\n\n\nPyEasyPS\nPyEasyPS is EasyPS (see above) ported to Python. It is even easier to use since the manual processes required in EasyPS (e.g. error-handling, targeted-updating of personal statements) are done for you. Essentially, it lets you focus on what’s most important – your personal statement content."
  },
  {
    "objectID": "projects.html#open-source",
    "href": "projects.html#open-source",
    "title": "projects",
    "section": "📭 open source",
    "text": "📭 open source\n\nJraph\nI am a key contributor of Jraph which is Google DeepMind’s open source library for graph neural networks using JAX."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I am currently a gardener 🪴 in the tech space.\nI was a Senior Data Scientist at Dyson where I’ve helped critical decision-making for ~10 Dyson products including supporting the 5-year Home strategy plan. I’ve also brought an estimate of 4m+ people into the Dyson comms funnel through Dyson’s first-ever Global Air Quality Research project and various Dyson Air Quality backpack projects. I was also the tech lead for the data science team in the Home Business Unit and have consulted on mathematical modelling, general data science and data acquisition across various different teams within Dyson.\nPrior to work, I studied Mathematics at King’s College London, and then Computer Science at Keble College, University of Oxford.\nWhen I’m not writing code, I like to explore new hobbies. Most recently, I picked up cycling and running. I enjoy good acoustic music and am always down for a game of table tennis, poker or chess.\nMy tech stack has changed over the years, with some technologies I haven’t used for quite a while now. I’ve done a good chunk of web dev, a bit of iOS mobile dev and a lot of research and applied machine learning. Here are some of them:\n\nProgramming languages: Python, R, C/C++ (prior experience), Swift (prior experience), SQL\nData analysis: SQL, PySpark, PyData stack, Plotly, NetworkX, Google BigQuery, AWS Redshift\nML & stats modelling: scikit-learn, statsmodels, scipy, PyMC3, PyTorch, Jax\nWeb & dashboard: HTML/CSS/JavaScript, ReactJS, Flask, Streamlit, Tableau\nDeployment: Git, Docker, Google Cloud (GCP) stack\nOthers: LaTeX\n\nI’ve basically worked with almost the entire GCP stack including BigQuery, DataProc, Pub/Sub, App Engine, Vertex AI, among others."
  },
  {
    "objectID": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#so-is-it-truly-worth-it",
    "href": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#so-is-it-truly-worth-it",
    "title": "Is Costco fuel cheaper overall given the membership premium?",
    "section": "So is it truly worth it?",
    "text": "So is it truly worth it?"
  },
  {
    "objectID": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#so-is-costco-truly-worth-it",
    "href": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#so-is-costco-truly-worth-it",
    "title": "Is Costco fuel cheaper overall given the membership premium?",
    "section": "So is Costco truly worth it?",
    "text": "So is Costco truly worth it?\nBased on today’s petrol price computation, it seems that we save \\Delta := C - C^* = 2.41 pounds. Is it really worth it that extra time to go to Costco and save £2.41? Well, for a start, recall that p, p^* are random variables. This means that the savings will be a random variable as well. Now suppose we are modelling for the expectation \\mathbb{E}[p] and \\mathbb{E}[p^*] instead, what then? Well long-term savings still pile up in my opinion as the annual savings is given by \\mathbb{E}[\\Delta]/\\phi. Assuming \\mathbb{E}[\\Delta] = 2.41, then annually we would have saved about £125. That is significant.\nSo yes, Costco solely for the fuel is worth it in my opinion. I’m buying that membership card yesterday. And this is not a Costco ad."
  },
  {
    "objectID": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#sensitivity-against-prices.",
    "href": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#sensitivity-against-prices.",
    "title": "Is Costco fuel cheaper overall given the membership premium?",
    "section": "Sensitivity against prices.",
    "text": "Sensitivity against prices."
  },
  {
    "objectID": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#price-sensitivity",
    "href": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#price-sensitivity",
    "title": "Is Costco fuel cheaper overall given the membership premium?",
    "section": "Price sensitivity",
    "text": "Price sensitivity\nOne useful metric to compute is how sensitive is \\Delta to price changes in p and p^*. To measure this, we can compute the total differential assuming \\Delta is only a function of price \\Delta(p, p^*). The total differential can then be computed:\n\nd\\Delta = \\frac{\\partial \\Delta}{\\partial p^*} dp^* + \\frac{\\partial \\Delta}{\\partial p} dp =  - (V + v^*_{\\mathrm{cover}}) dp^* + (V + v_{\\mathrm{cover}}) dp.\n\nNaturally, since d^* &gt; d, then v^*_{\\mathrm{cover}} &gt; v_{\\mathrm{cover}}. Consequently, a tiny change in p^* would affect \\Delta more significantly as compared to p. This is somewhat expected but it’s just nice to see our intuition being confirmed by the mathematics."
  },
  {
    "objectID": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#one-last-thing-price-sensitivity",
    "href": "posts/2024-10-17-is-costco-fuel-worth-it/index.html#one-last-thing-price-sensitivity",
    "title": "Is Costco fuel cheaper overall given the membership premium?",
    "section": "One last thing: price sensitivity",
    "text": "One last thing: price sensitivity\nOne metric that can be important is to look at how sensitive \\Delta is relative to price changes in p and p^*. To measure this, we can compute the total differential assuming \\Delta is only a function of price \\Delta(p, p^*). The total differential can then be computed:\n\nd\\Delta = \\frac{\\partial \\Delta}{\\partial p^*} dp^* + \\frac{\\partial \\Delta}{\\partial p} dp =  - (V + v^*_{\\mathrm{cover}}) dp^* + (V + v_{\\mathrm{cover}}) dp.\n\nNaturally, since d^* &gt; d, then v^*_{\\mathrm{cover}} &gt; v_{\\mathrm{cover}}. Consequently, a tiny change in p^* would affect \\Delta more significantly as compared to p. This is somewhat expected but it’s just nice to see our intuition being confirmed by the mathematics."
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html",
    "href": "posts/2025-01-05-to-pension-or-not/index.html",
    "title": "Migrant mathematics for pension",
    "section": "",
    "text": "I am not a financial adviser. This is not financial advice. I don’t represent my company.\nThe universal question that I get from friends and colleagues alike is whether we should contribute into pension as a migrant. This stems from the fact that some of us are not staying in this country forever depending on one’s personal goals; which makes sense. There’s a branching question on how to move pensions if one were to contribute to a private pension facility here in the UK such as using a QROPS, but that will not be the scope of this post.\nWell, what makes contributing into pension attractive as a migrant? After all, you’re chopping off money that you won’t have access to for a long time – unless you want to pay a penalty – and putting it into a pot that can go both up and down. Not to mention that there are management fees that comes with using this pot. Parking your money in a pot in a country that you have minimal ties to past your working residency is just horrifying to hear. Plus, when you reach pension age, you still have to pay income tax when taking out your pension money assuming it goes above the personal allowance at that time. Otoh, if you were to take this money straight out of PAYE, you would have to pay income taxes on it today; and we all know UK taxes are huge at 20%, 40% and even 45%. This begs the question, are we just in a limbo? Surely given a binary decision, one should be marginally better?\nWell you would be pleased to here that the answer to that question is a positive. There is something that makes contribution to pension marginally better and that is if your company matches your pension contribution. We shall refer to this phenomenon as pension matching from now on.\nDepending on your employer, this matching figure can go to wildly high numbers. The government state pension matching goes beyond 30% for example (though this comes at a significantly lower base pay). Some companies do a 6% matching to 3%. I’ve even heard a 10% matching to 1%. But I’ve also heard smaller firms do half matches such as a 2% matching to 4%; and that is just sad to hear. Regardless, these are tax-free boosters to your income though they are only accessible when you reach pension age. I mean if your annual income is X, then a 10% matching puts your income to 1.1X which is a huge boost to be fair.\nBut let’s not move away from the main premise. I want to find precise evidence that pension matching is a viable tradeoff for migrants. Naturally, these are tax-free boosters but let’s not forget the fact that you wouldn’t be able to access this money for at least 30 years and they can go both up and/or down. For me, this begs the question, what if we look at the extreme case: what if I put money into pension, get pension matching, but then decide to take out my money before pension age and pay the HMRC penalty of 55%? Would pension matching still be worth it? Or is one better off just pay PAYE tax and getting quick cash that can be used to invest in other asset classes considering one might not be staying long in the UK?\nTo answer this question, we focus on the pension matching coefficient because only that gives the edge for pension matching. Let’s make this precise.\nLet X be our anual income, and fix c_1 to be your pension contribution percentage and c_2 to be your company matching contribution percentage, i.e., your pension matching coefficient.\nNow, tax is progressively paid but we only want to focus on extremes so put m to be your maximal tax percentage – maximal in the sense of which tax bracket you are in – and let e be the early pension withdrawal tax percentage, i.e., your penalty for taking out your pension before of age. As of 2025, this is 55%. I also just learnt that apparently it is not straightforward to take out your pension early so you might need to get a lawyer which would cost \\ell.\nSo now we have two scenarios:"
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#scenario-1",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#scenario-1",
    "title": "Migrant mathematics for pension",
    "section": "Scenario 1",
    "text": "Scenario 1\nSuppose you were to put money into your pension and do an early withdrawal.\nYour pension contribution amounts to (c_1+c_2)X. If your perform an early withdrawal, this reduces to (1 - e)(c_1+c_2)X but remember we have to pay lawyer fees so it actually becomes (1 - e)(c_1+c_2)X - \\ell. Now, I’m not sure if the early withdrawal amount will be slapped with a further income tax – but knowing the UK, I think it will, so let’s represent this possible income tax with \\tau. Thus, the amount of money we could obtain annually is P_1 = (1 - e)(c_1+c_2)X\\tau - \\ell."
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#scenario-2",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#scenario-2",
    "title": "Migrant mathematics for pension",
    "section": "Scenario 2",
    "text": "Scenario 2\nOtoh, suppose you decide to not contribute to pension at and lose the company match but obtain quick cash. You would have gain P_2 = (1-m)c_1 X at the end of each financial year. No lawyer fees, no extra complication."
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#diffs",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#diffs",
    "title": "Migrant mathematics for pension",
    "section": "Diffs",
    "text": "Diffs\nNow we can quantify the difference here or look at ratios. But if you were to plug in real numbers from your income, you might have seen that Scenario 1 actually leads you to more income in the long run which is just insane considering you are already paying for the 55% early withdrawal tax!\nSo the next question is when does this result gets overturned? In other words, when is P_2 \\geqslant P_1? Thankfully this is fairly straightforward maths, you just have to be careful when dividing by possible negatives.\n\n\n\n\n\n\nc_2 criterion\n\n\n\nIt is more worth it to pursue Scenario 2 iff \nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)\\tau} - 1 \\right] + \\frac{\\ell}{(1-e)\\tau X}.\n\n\n\nNotice that there are two terms in the bound where one is on c_1 and the other is on the lawyer fees. Let’s analyse this criterion further.\nFrom now on, I am going to assume that we are going to pay the maximal tax bracket on the early-withdrawn pension. That means that we put \\tau = 1-m. This aligns with our spirit of analysing for the extreme case (since the alternative \\tau = 1 is that we are not taxed on the already early-withdrawn-taxed pension amount)."
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#to-pension-or-not-to-pension",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#to-pension-or-not-to-pension",
    "title": "Migrant mathematics for pension",
    "section": "To pension or not to pension",
    "text": "To pension or not to pension\nIt is nothing other than the company matching in my opinion. Depending on your employer, this can go to wildly high numbers.\nTo pension or not to pension.\nGiven an annual capital X.\nFix c_1 to be your pension contribution percentage and c_2 to be your company matching contribution. Let e be the early withdrawal tax and put m to be your maximal tax bracket."
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#no-lawyers-involved-pay-maximal-tax-bracket-on-early-withdrawn-pension",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#no-lawyers-involved-pay-maximal-tax-bracket-on-early-withdrawn-pension",
    "title": "Migrant mathematics for pension",
    "section": "No lawyers involved, pay maximal tax bracket on early-withdrawn pension",
    "text": "No lawyers involved, pay maximal tax bracket on early-withdrawn pension\nSuppose you don’t get lawyers involved and suppose you pay the maximal tax bracket on the early-withdrawn pension. Then the criterion simplifies to:\n\nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)(1-m)} - 1 \\right] + 0\n= c_1 \\frac{e}{1-e}.\n\nSo if you were to contribute c_1 = 6\\% of your salary and the early withdrawal tax is at e = 55\\%, then it only make sense to pursue Scenario 2 iff\n\nc_2 \\leqslant 0.06 \\frac{0.55}{1-0.55} = 0.73.\n\nThat is, if you were to contribute 6% of your salary, it only makes sense to take advantage of pension matching and do early withdrawal if and only if your company is matching contribution at 7.3% or more!\nThis is a staggering find! It means even if your pension matching coefficient is more than your contribution, it is still possible that it’s more worth it to just grab that quick cash and pay income tax on it!"
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#outcome-1-no-lawyers-involved-pay-maximal-tax-bracket-on-early-withdrawn-pension",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#outcome-1-no-lawyers-involved-pay-maximal-tax-bracket-on-early-withdrawn-pension",
    "title": "Migrant mathematics for pension",
    "section": "Outcome 1: No lawyers involved, pay maximal tax bracket on early-withdrawn pension",
    "text": "Outcome 1: No lawyers involved, pay maximal tax bracket on early-withdrawn pension\nSuppose you don’t get lawyers involved and suppose you pay the maximal tax bracket on the early-withdrawn pension. Then the criterion simplifies to:\n\nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)(1-m)} - 1 \\right] + 0\n= c_1 \\frac{e}{1-e}.\n\nSo if you were to contribute c_1 = 6\\% of your salary and the early withdrawal tax is at e = 55\\%, then it only make sense to pursue Scenario 2 iff\n\nc_2 \\leqslant 0.06 \\frac{0.55}{1-0.55} = 0.73.\n\nThat is, if you were to contribute 6% of your salary, it only makes sense to take advantage of pension matching and do early withdrawal if and only if your company is matching contribution at 7.3% or more!\nThis is a staggering find! It means even if your pension matching coefficient is more than your contribution, it is still possible that it’s more worth it to just grab that quick cash and pay income tax on it!"
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#outcome-2-no-lawyers-involved-pay-maximal-tax-bracket-on-early-withdrawn-pension",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#outcome-2-no-lawyers-involved-pay-maximal-tax-bracket-on-early-withdrawn-pension",
    "title": "Migrant mathematics for pension",
    "section": "Outcome 2: No lawyers involved, pay maximal tax bracket on early-withdrawn pension",
    "text": "Outcome 2: No lawyers involved, pay maximal tax bracket on early-withdrawn pension"
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#outcome-1-no-lawyers-involved",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#outcome-1-no-lawyers-involved",
    "title": "Migrant mathematics for pension",
    "section": "Outcome 1: No lawyers involved",
    "text": "Outcome 1: No lawyers involved\nSuppose you don’t get lawyers involved and suppose you pay the maximal tax bracket on the early-withdrawn pension. Then the criterion simplifies to:\n\nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)(1-m)} - 1 \\right] + 0\n= c_1 \\frac{e}{1-e}.\n\nSo if you were to contribute c_1 = 6\\% of your salary and the early withdrawal tax is at e = 55\\%, then it only make sense to pursue Scenario 2 iff\n\nc_2 \\leqslant 0.06 \\frac{0.55}{1-0.55} = 0.073.\n\nThat is, if you were to contribute 6% of your salary, it only makes sense to take advantage of pension matching and do early withdrawal if and only if your company is matching contribution at 7.3% or more!\nThis is a staggering find! It means even if your pension matching coefficient is more than your contribution, it is still possible that it’s more worth it to just grab that quick cash and pay income tax on it!"
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#outcome-2-lawyers-charge-you-1-of-your-income",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#outcome-2-lawyers-charge-you-1-of-your-income",
    "title": "Migrant mathematics for pension",
    "section": "Outcome 2: Lawyers charge you 1% of your income",
    "text": "Outcome 2: Lawyers charge you 1% of your income"
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#outcome-2-lawyers-charge-you-1-of-your-annual-income",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#outcome-2-lawyers-charge-you-1-of-your-annual-income",
    "title": "Migrant mathematics for pension",
    "section": "Outcome 2: Lawyers charge you 1% of your annual income",
    "text": "Outcome 2: Lawyers charge you 1% of your annual income\nIn this case, we have \\ell = 0.01X we simplifies our criterion to:\n\nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)(1-m)} - 1 \\right] + \\frac{0.01}{(1-e)(1-m)}\n= c_1 \\frac{e}{1-e} + \\frac{0.01}{(1-e)(1-m)}.\n\nIn this case, if you were to contribute 6% of your salary, it only makes sense to pursue Scenario 2 iff your company is matching contribution at 11% or more:\n\nc2 \\leqslant\n0.06 \\frac{0.55}{1-0.55} + \\frac{0.01}{(1-0.55)(1-0.4)}\n= 0.11.\n\nThat scaled quite fast."
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#outcome-1-6-contribution.-no-lawyers-involved",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#outcome-1-6-contribution.-no-lawyers-involved",
    "title": "Migrant mathematics for pension",
    "section": "Outcome 1: 6% contribution. No lawyers involved",
    "text": "Outcome 1: 6% contribution. No lawyers involved\nSuppose you don’t get lawyers involved and suppose you pay the maximal tax bracket on the early-withdrawn pension. Then the criterion simplifies to:\n\nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)(1-m)} - 1 \\right] + 0\n= c_1 \\frac{e}{1-e}.\n\nSo if you were to contribute c_1 = 6\\% of your salary and the early withdrawal tax is at e = 55\\%, then it only make sense to pursue Scenario 2 iff\n\nc_2 \\leqslant 0.06 \\frac{0.55}{1-0.55} = 0.073.\n\nThat is, if you were to contribute 6% of your salary, it only makes sense to take advantage of pension matching and do early withdrawal if and only if your company is matching contribution at 7.3% or more!\nThis is a staggering find! It means even if your pension matching coefficient is more than your contribution, it is still possible that it’s more worth it to just grab that quick cash and pay income tax on it!"
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#outcome-2-6-contribution.-lawyers-charge-you-1-of-your-annual-income",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#outcome-2-6-contribution.-lawyers-charge-you-1-of-your-annual-income",
    "title": "Migrant mathematics for pension",
    "section": "Outcome 2: 6% contribution. Lawyers charge you 1% of your annual income",
    "text": "Outcome 2: 6% contribution. Lawyers charge you 1% of your annual income\nIn this case, we have \\ell = 0.01X which simplifies our criterion to:\n\nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)(1-m)} - 1 \\right] + \\frac{0.01}{(1-e)(1-m)}\n= c_1 \\frac{e}{1-e} + \\frac{0.01}{(1-e)(1-m)}.\n\nIn this case, if you were to contribute 6% of your salary, it only makes sense to pursue Scenario 2 iff your company is matching contribution at 11% or more:\n\nc2 \\leqslant\n0.06 \\frac{0.55}{1-0.55} + \\frac{0.01}{(1-0.55)(1-0.4)}\n= 0.11.\n\nFrom 7.3% to 11%. That scaled up quite fast!"
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#the-c_2-criterion",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#the-c_2-criterion",
    "title": "Migrant mathematics for pension",
    "section": "The c_2 criterion",
    "text": "The c_2 criterion\nNow we can quantify the difference here or look at ratios. But if you were to plug in real numbers from your income, you might have seen that Scenario 1 actually leads you to more income in the long run which is quite a revelation considering you are already paying for the 55% early withdrawal tax!\nSo the next question is when does this result gets overturned? In other words, when is P_2 \\geqslant P_1? Thankfully this is fairly straightforward maths, you just have to be careful when dividing by possible negatives. We obtain the following bound.\n\n\n\n\n\n\nc_2 criterion\n\n\n\nIt is more worth it to pursue Scenario 2 iff \nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)\\tau} - 1 \\right] + \\frac{\\ell}{(1-e)\\tau X}.\n\n\n\nNotice that there are two terms in the bound where one is on c_1 and the other is on the lawyer fees. Let’s analyse this criterion further.\nFrom now on, I am going to assume that we are going to pay the maximal tax bracket on the early-withdrawn pension. That means that we put \\tau = 1-m. This aligns with our spirit of analysing for the extreme case (since the alternative \\tau = 1 is that we are not taxed on the already early-withdrawn-taxed pension amount)."
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#a-general-outcome-of-pension-contribution-versus-lawyer-fees",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#a-general-outcome-of-pension-contribution-versus-lawyer-fees",
    "title": "Migrant mathematics for pension",
    "section": "A general outcome of pension contribution versus lawyer fees",
    "text": "A general outcome of pension contribution versus lawyer fees\nNote that there’s nothing special about pension contribution being at 6%. To look at the general case, we plot a heatmap of c_1 versus \\ell as a percent of annual income. We consider c_1 to range from 0 to 20 percent and lawyer fees to range from 0 to 10 percent and see how these values affect the c_2 criterion. In the heatmap, we mark high values of c_2 up to 20% by red to acknowledge that this is the boundary that corporations can go up to, and anything beyond by gray.\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nX = 100_000\n\ne = 0.55\nm = 0.4\n\nc1s = np.arange(0, 21)/ 100  # c1\n\nys_perc = np.arange(0, 11)/100\nys = ys_perc*X # lawyer fees\n\nXS, YS = np.meshgrid(c1s, ys)\n\ndef c2_crit(c1, m, e, t, l, X):\n    return c1*((1-m)/((1-e)*t)-1) + l/(1-e)/t/X\nZ = c2_crit(c1=XS, m=m, e=e, t=1-m, l=YS , X=X)\n\n# Create the heatmap\nplt.figure(figsize=(10, 8))\n\n# Create a custom colormap\n# Values &lt;= 0.20 will use viridis colormap\n# Values &gt; 0.20 will be red\ncolors = plt.cm.viridis(np.linspace(0, 1, 256))\ncustom_cmap = plt.cm.bwr.copy()\ncustom_cmap.set_over('slategray')\n\nsns.heatmap(Z*100, \n            xticklabels=[f\"{val*100:.0f}\" for val in c1s],\n            yticklabels=[f\"{val*100:.0f}\" for val in ys_perc],\n            cmap=custom_cmap,  # You can change the colormap\n            annot=True,      # Show values in cells\n            fmt='.0f',\n            vmax=20)         # Format numbers as general numbers\n\nplt.title('Company matching $c_2$ bound that makes Scenario 2 more worth it')\nplt.xlabel('Your pension contribution $c_1$ (%)')\nplt.ylabel('Lawyer fees (as % of annual income)')\nplt.show()\n\n\n\n\n\n\n\n\nIn the heatmap, we can see the outcomes of 7 and 11 percent in c_2 that we have computed before with c_1 = 0.06 and \\ell being 0 and 1 percent of your annual income respectively. In general, for low lawyer fees &lt; 2%, we see that the criterion is obeyed with relatively sensible c_1 versus c_2 values. For example, at \\ell = 0.01X and c_1 = 10\\%, Scenario 2 is only worth it iff c_2 \\leqslant 16\\%.\nHowever, the tradeoff becomes worse quite fast. At 5% (of annual income) lawyer fees, the only sensible company matching is very high and it is unlikely you would find companies that are willing to match these values. In these cases, it is much worth it to grab that quick cash and pay income tax."
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#a-general-outcome-of-c_1-versus-ell",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#a-general-outcome-of-c_1-versus-ell",
    "title": "Migrant mathematics for pension",
    "section": "A general outcome of c_1 versus \\ell",
    "text": "A general outcome of c_1 versus \\ell\nNote that there’s nothing special about pension contribution being at 6%. To look at the general case, we plot a heatmap of c_1 versus \\ell as a percent of annual income. We consider c_1 to range from 0 to 20 percent and lawyer fees to range from 0 to 10 percent and see how these values affect the c_2 criterion. In the heatmap, we mark high values of c_2 up to 20% by red to acknowledge that this is the boundary that corporations can go up to, and anything beyond by gray.\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nX = 100_000\n\ne = 0.55\nm = 0.4\n\nc1s = np.arange(0, 21)/ 100  # c1\n\nys_perc = np.arange(0, 11)/100\nys = ys_perc*X # lawyer fees\n\nXS, YS = np.meshgrid(c1s, ys)\n\ndef c2_crit(c1, m, e, t, l, X):\n    return c1*((1-m)/((1-e)*t)-1) + l/(1-e)/t/X\nZ = c2_crit(c1=XS, m=m, e=e, t=1-m, l=YS , X=X)\n\n# Create the heatmap\nplt.figure(figsize=(10, 8))\n\n# Create a custom colormap\n# Values &lt;= 0.20 will use viridis colormap\n# Values &gt; 0.20 will be red\ncolors = plt.cm.viridis(np.linspace(0, 1, 256))\ncustom_cmap = plt.cm.bwr.copy()\ncustom_cmap.set_over('slategray')\n\nsns.heatmap(Z*100, \n            xticklabels=[f\"{val*100:.0f}\" for val in c1s],\n            yticklabels=[f\"{val*100:.0f}\" for val in ys_perc],\n            cmap=custom_cmap,  # You can change the colormap\n            annot=True,      # Show values in cells\n            fmt='.0f',\n            vmax=20)         # Format numbers as general numbers\n\nplt.title('Company matching $c_2$ bound that makes Scenario 2 more worth it')\nplt.xlabel('Your pension contribution $c_1$ (%)')\nplt.ylabel('Lawyer fees (as % of annual income)')\nplt.show()\n\n\n\n\n\n\n\n\nIn the heatmap, we can see the outcomes of 7 and 11 percent in c_2 that we have computed before with c_1 = 0.06 and \\ell being 0 and 1 percent of your annual income respectively. In general, for low lawyer fees &lt; 2%, we see that the criterion is obeyed with relatively sensible c_1 versus c_2 values. For example, at \\ell = 0.01X and c_1 = 10\\%, Scenario 2 is only worth it iff c_2 \\leqslant 16\\%.\nHowever, the tradeoff becomes worse quite fast. At 5% (of annual income) lawyer fees, the only sensible company matching is very high and it is unlikely you would find companies that are willing to match these values. In these cases, it is much worth it to grab that quick cash and pay income tax."
  },
  {
    "objectID": "posts/2025-01-05-to-pension-or-not/index.html#a-general-outcome-of-c_1-versus-lawyer-fees",
    "href": "posts/2025-01-05-to-pension-or-not/index.html#a-general-outcome-of-c_1-versus-lawyer-fees",
    "title": "Migrant mathematics for pension",
    "section": "A general outcome of c_1 versus lawyer fees",
    "text": "A general outcome of c_1 versus lawyer fees\nThere’s nothing special about pension contribution being at 6%. To look at the general case, we plot a heatmap of c_1 versus \\ell as a percent of annual income. We consider c_1 to range from 0 to 20 percent and lawyer fees to range from 0 to 10 percent and see how these values affect the c_2 criterion. In the heatmap, we mark high values of c_2 up to 20% by red to acknowledge that this is the boundary that corporations can go up to, and anything beyond by gray.\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nX = 100_000\n\ne = 0.55\nm = 0.4\n\nc1s = np.arange(0, 21)/ 100  # c1\n\nys_perc = np.arange(0, 11)/100\nys = ys_perc*X # lawyer fees\n\nXS, YS = np.meshgrid(c1s, ys)\n\ndef c2_crit(c1, m, e, t, l, X):\n    return c1*((1-m)/((1-e)*t)-1) + l/(1-e)/t/X\nZ = c2_crit(c1=XS, m=m, e=e, t=1-m, l=YS , X=X)\n\n# Create the heatmap\nplt.figure(figsize=(10, 8))\n\n# Create a custom colormap\n# Values &lt;= 0.20 will use bwr colormap\n# Values &gt; 0.20 will be slategray\ncustom_cmap = plt.cm.bwr.copy()\ncustom_cmap.set_over('slategray')\nvmax = 20\n\nsns.heatmap(Z*100, \n            xticklabels=[f\"{val*100:.0f}\" for val in c1s],\n            yticklabels=[f\"{val*100:.0f}\" for val in ys_perc],\n            cmap=custom_cmap,  # You can change the colormap\n            annot=True,      # Show values in cells\n            fmt='.0f',\n            vmax=vmax)\n\nplt.title('Company matching $c_2$ bound that makes Scenario 2 more worth it')\nplt.xlabel('Your pension contribution $c_1$ (%)')\nplt.ylabel('Lawyer fees (as % of annual income)')\nplt.show()\n\n\n\n\n\n\n\n\nIn the heatmap, we can see the outcomes of 7 and 11 percent in c_2 that we have computed before with c_1 = 0.06 and \\ell being 0 and 1 percent of your annual income respectively. In general, for low lawyer fees &lt; 2%, we see that the criterion is obeyed with relatively sensible c_1 versus c_2 values. For example, at \\ell = 0.01X and c_1 = 10\\%, Scenario 2 is only worth it iff c_2 \\leqslant 16\\%.\nHowever, the tradeoff becomes worse quite fast. At 5% (of annual income) lawyer fees, the only sensible company matching is very high and it is unlikely you would find companies that are willing to match these values. In these cases, it is much worth it to grab that quick cash and pay income tax.\nSo what’s the verdict? Well it’s up to you. We’ve done the maths and crunched the numbers. If you find that your company is matching well outside the range of the c_2 criterion with your pension contribution of c_1, then maybe it’s more sensible to just put your money in pension and get that extra tax-free booster. Sure, you’ll get the money at pension age, and you might not even live to see that money, but the hedging still makes sense especially if you can work out a will for your future generation. Plus, do you really need that excess money today anyways? I would personally rather focus on reworking my personal finance to fit the post-taxed post-pension-sacrificed money."
  },
  {
    "objectID": "posts/2025-01-05-migrant-pension-mathematics/index.html",
    "href": "posts/2025-01-05-migrant-pension-mathematics/index.html",
    "title": "Migrant pension mathematics",
    "section": "",
    "text": "I am not a financial adviser. This is not financial advice. I don’t represent my company.\nThe universal question that I get from friends and colleagues alike is whether we should contribute into pension as a migrant. This stems from the fact that some of us are not staying in this country forever depending on one’s personal goals; which makes sense. There’s a branching question on how to move pensions if one were to contribute to a private pension facility here in the UK such as using a QROPS, but that will not be the scope of this post.\nWell, what makes contributing into pension attractive as a migrant? After all, you’re chopping off money that you won’t have access to for a long time – unless you want to pay a penalty – and putting it into a pot that can go both up and down. Not to mention that there are management fees that comes with using this pot. Parking your money in a pot in a country that you have minimal ties to past your working residency is just horrifying to hear. Plus, when you reach pension age, you still have to pay income tax when taking out your pension money assuming it goes above the personal allowance at that time. Otoh, if you were to take this money straight out of PAYE, you would have to pay income taxes on it today; and we all know UK taxes are huge at 20%, 40% and even 45%. This begs the question, are we just in a limbo? Surely given a binary decision, one should be marginally better?\nWell you would be pleased to here that the answer to that question is a positive. There is something that makes contribution to pension marginally better and that is if your company matches your pension contribution. We shall refer to this phenomenon as pension matching from now on.\nDepending on your employer, this matching figure can go to wildly high numbers. The government state pension matching goes beyond 30% for example (though this comes at a significantly lower base pay). Some companies do a 6% matching to 3%. I’ve even heard a 10% matching to 1%. But I’ve also heard smaller firms do half matches such as a 2% matching to 4%; and that is just sad to hear. Regardless, these are tax-free boosters to your income though they are only accessible when you reach pension age. I mean if your annual income is X, then a 10% matching puts your income to 1.1X which is a huge boost to be fair.\nBut let’s not move away from the main premise. I want to find precise evidence that pension matching is a viable tradeoff for migrants. Naturally, these are tax-free boosters but let’s not forget the fact that you wouldn’t be able to access this money for at least 30 years and they can go both up and/or down. For me, this begs the question, what if we look at the extreme case: what if I put money into pension, get pension matching, but then decide to take out my money before pension age and pay the HMRC penalty of 55%? Would pension matching still be worth it? Or is one better off just pay PAYE tax and getting quick cash that can be used to invest in other asset classes considering one might not be staying long in the UK?\nTo answer this question, we focus on the pension matching coefficient because only that gives the edge for pension matching. Let’s make this precise.\nLet X be our anual income, and fix c_1 to be your pension contribution percentage and c_2 to be your company matching contribution percentage, i.e., your pension matching coefficient.\nNow, tax is progressively paid but we only want to focus on extremes so put m to be your maximal tax percentage – maximal in the sense of which tax bracket you are in – and let e be the early pension withdrawal tax percentage, i.e., your penalty for taking out your pension before of age. As of 2025, this is 55%. I also just learnt that apparently it is not straightforward to take out your pension early so you might need to get a lawyer which would cost \\ell.\nSo now we have two scenarios:"
  },
  {
    "objectID": "posts/2025-01-05-migrant-pension-mathematics/index.html#the-c_2-criterion",
    "href": "posts/2025-01-05-migrant-pension-mathematics/index.html#the-c_2-criterion",
    "title": "Migrant pension mathematics",
    "section": "The c_2 criterion",
    "text": "The c_2 criterion\nNow we can quantify the difference here or look at ratios. But if you were to plug in real numbers from your income, you might have seen that Scenario 1 actually leads you to more income in the long run which is quite a revelation considering you are already paying for the 55% early withdrawal tax!\nSo the next question is when does this result gets overturned? In other words, when is P_2 \\geqslant P_1? Thankfully this is fairly straightforward maths, you just have to be careful when dividing by possible negatives. We obtain the following bound.\n\n\n\n\n\n\nc_2 criterion\n\n\n\nIt is more worth it to pursue Scenario 2 iff \nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)\\tau} - 1 \\right] + \\frac{\\ell}{(1-e)\\tau X}.\n\n\n\nNotice that there are two terms in the bound where one is on c_1 and the other is on the lawyer fees. Let’s analyse this criterion further.\nFrom now on, I am going to assume that we are going to pay the maximal tax bracket on the early-withdrawn pension. That means that we put \\tau = 1-m. This aligns with our spirit of analysing for the extreme case (since the alternative \\tau = 1 is that we are not taxed on the already early-withdrawn-taxed pension amount)."
  },
  {
    "objectID": "posts/2025-01-05-migrant-pension-mathematics/index.html#outcome-1-6-contribution.-no-lawyers-involved",
    "href": "posts/2025-01-05-migrant-pension-mathematics/index.html#outcome-1-6-contribution.-no-lawyers-involved",
    "title": "Migrant pension mathematics",
    "section": "Outcome 1: 6% contribution. No lawyers involved",
    "text": "Outcome 1: 6% contribution. No lawyers involved\nSuppose you don’t get lawyers involved and suppose you pay the maximal tax bracket on the early-withdrawn pension. Then the criterion simplifies to:\n\nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)(1-m)} - 1 \\right] + 0\n= c_1 \\frac{e}{1-e}.\n\nSo if you were to contribute c_1 = 6\\% of your salary and the early withdrawal tax is at e = 55\\%, then it only make sense to pursue Scenario 2 iff\n\nc_2 \\leqslant 0.06 \\frac{0.55}{1-0.55} = 0.073.\n\nThat is, if you were to contribute 6% of your salary, it only makes sense to take advantage of pension matching and do early withdrawal if and only if your company is matching contribution at 7.3% or more!\nThis is a staggering find! It means even if your pension matching coefficient is more than your contribution, it is still possible that it’s more worth it to just grab that quick cash and pay income tax on it!"
  },
  {
    "objectID": "posts/2025-01-05-migrant-pension-mathematics/index.html#outcome-2-6-contribution.-lawyers-charge-you-1-of-your-annual-income",
    "href": "posts/2025-01-05-migrant-pension-mathematics/index.html#outcome-2-6-contribution.-lawyers-charge-you-1-of-your-annual-income",
    "title": "Migrant pension mathematics",
    "section": "Outcome 2: 6% contribution. Lawyers charge you 1% of your annual income",
    "text": "Outcome 2: 6% contribution. Lawyers charge you 1% of your annual income\nIn this case, we have \\ell = 0.01X which simplifies our criterion to:\n\nc_2\n\\leqslant c_1 \\left[ \\frac{1-m}{(1-e)(1-m)} - 1 \\right] + \\frac{0.01}{(1-e)(1-m)}\n= c_1 \\frac{e}{1-e} + \\frac{0.01}{(1-e)(1-m)}.\n\nIn this case, if you were to contribute 6% of your salary, it only makes sense to pursue Scenario 2 iff your company is matching contribution at 11% or more:\n\nc2 \\leqslant\n0.06 \\frac{0.55}{1-0.55} + \\frac{0.01}{(1-0.55)(1-0.4)}\n= 0.11.\n\nFrom 7.3% to 11%. That scaled up quite fast!"
  },
  {
    "objectID": "posts/2025-01-05-migrant-pension-mathematics/index.html#a-general-outcome-of-c_1-versus-lawyer-fees",
    "href": "posts/2025-01-05-migrant-pension-mathematics/index.html#a-general-outcome-of-c_1-versus-lawyer-fees",
    "title": "Migrant pension mathematics",
    "section": "A general outcome of c_1 versus lawyer fees",
    "text": "A general outcome of c_1 versus lawyer fees\nThere’s nothing special about pension contribution being at 6%. To look at the general case, we plot a heatmap of c_1 versus \\ell as a percent of annual income. We consider c_1 to range from 0 to 20 percent and lawyer fees to range from 0 to 10 percent and see how these values affect the c_2 criterion. In the heatmap, we mark high values of c_2 up to 20% by red to acknowledge that this is the boundary that corporations can go up to, and anything beyond by gray.\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nX = 100_000\n\ne = 0.55\nm = 0.4\n\nc1s = np.arange(0, 21)/ 100  # c1\n\nys_perc = np.arange(0, 11)/100\nys = ys_perc*X # lawyer fees\n\nXS, YS = np.meshgrid(c1s, ys)\n\ndef c2_crit(c1, m, e, t, l, X):\n    return c1*((1-m)/((1-e)*t)-1) + l/(1-e)/t/X\nZ = c2_crit(c1=XS, m=m, e=e, t=1-m, l=YS , X=X)\n\n# Create the heatmap\nplt.figure(figsize=(10, 8))\n\n# Create a custom colormap\n# Values &lt;= 0.20 will use bwr colormap\n# Values &gt; 0.20 will be slategray\ncustom_cmap = plt.cm.bwr.copy()\ncustom_cmap.set_over('slategray')\nvmax = 20\n\nsns.heatmap(Z*100, \n            xticklabels=[f\"{val*100:.0f}\" for val in c1s],\n            yticklabels=[f\"{val*100:.0f}\" for val in ys_perc],\n            cmap=custom_cmap,\n            annot=True, # Show values in cells\n            fmt='.0f',\n            vmax=vmax)\n\nplt.title('Company matching $c_2$ (%) bound that makes Scenario 2 more worth it')\nplt.xlabel('Your pension contribution $c_1$ (%)')\nplt.ylabel('Lawyer fees (as % of annual income)')\nplt.show()\n\n\n\n\n\n\n\n\nIn the heatmap, we can see the outcomes of 7 and 11 percent in c_2 that we have computed before with c_1 = 0.06 and \\ell being 0 and 1 percent of your annual income respectively. In general, for low lawyer fees &lt; 2%, we see that the criterion is obeyed with relatively sensible c_1 versus c_2 values. For example, at \\ell = 0.01X and c_1 = 10\\%, Scenario 2 is only worth it iff c_2 \\leqslant 16\\%.\nHowever, the tradeoff becomes worse quite fast. At 5% (of annual income) lawyer fees, the only sensible company matching is very high and it is unlikely you would find companies that are willing to match these values. In these cases, it is much worth it to grab that quick cash and pay income tax.\nSo what’s the verdict? Well it’s up to you. We’ve done the maths and crunched the numbers. If you find that your company is matching well outside the range of the c_2 criterion with your pension contribution of c_1, then maybe it’s more sensible to just put your money in pension and get that extra tax-free booster. Sure, you’ll get the money at pension age, and you might not even live to see that money, but the hedging still makes sense especially if you can work out a will for your future generation. Plus, do you really need that excess money today anyways? I would personally rather focus on reworking my personal finance to fit the post-taxed post-pension-sacrificed money."
  }
]